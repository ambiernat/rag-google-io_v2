[
  {
    "schema_version": "chunk_v1",
    "chunk_id": 0,
    "video_id": "4TE-KFXvhAk",
    "start": 0.0,
    "end": 139.04,
    "text": "[MUSIC PLAYING] JOANA CARRASQUEIRA:\nHello, everyone. My name is Joana Carrasqueira,\nand I lead Developer Relations at Google DeepMind. JOSH GORDON: Hi, everyone. I'm Josh. JOANA CARRASQUEIRA:\nAnd we're very excited to welcome you to\nour session, Google's AI Stack for Developers. We'll start by giving you a\nquick overview of Google's AI stack. Who's at I/O for the first time? Can I see some hands up? Oh, OK. Welcome to Google\nI/O. It's a pleasure to have you with us today. So we'll start by giving\nyou an overview of Google's end-to-end ecosystem of AI. And as you know, we've\nbeen leading the way in AI for decades, since we\nopen-sourced TensorFlow in 2015, from when we published our\nfield-defining research with transformers\nin 2017, to Gemini. And we are now in\nthe Gemini era. So we've been releasing a lot. Relentlessly, as it's\nbeen called today, we've been shipping many\nfeatures, many new products. And in our talk,\nwe're actually going to give you an overview\nof everything that's new for developers\nthroughout the AI stack. Our mission is to empower every\ndeveloper and organization to harness the power of AI. And Google's stack is so good\nand flexible because it combines very robust infrastructure\nwith state-of-the-art research. And all of this enables\nreal-world applications come to life that change\nentire fields, industries, and companies. We'll start by discussing\nfoundation models, touching upon our\nGemini, Gemma, and some of our domain-specific models. JOSH GORDON: After\nfoundation models, we'll take a look\nat AI frameworks that we use to build them. So we'll talk about JAX, which\nis really great for researchers. We'll talk about Keras, which is\nreally amazing for applied AI. Later on, we'll even\ntalk a little bit about the work we're\ndoing with PyTorch. JOANA CARRASQUEIRA: We'll\nalso touch upon some developer tools for all\ntypes of experience from beginners to advanced. JOSH GORDON: Then we'll talk a\nlittle bit about infrastructure. And this talk is about\nsoftware not hardware.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 1,
    "video_id": "4TE-KFXvhAk",
    "start": 127.49,
    "end": 248.54,
    "text": "JOANA CARRASQUEIRA: We'll\nalso touch upon some developer tools for all\ntypes of experience from beginners to advanced. JOSH GORDON: Then we'll talk a\nlittle bit about infrastructure. And this talk is about\nsoftware not hardware. Our hardware infrastructure\nis TPUs, which you've probably heard a lot of. But in this talk,\nI'll briefly talk about XLA, which is a\nmachine learning compiler. And I'll talk about\nsome of the work we're doing for\ninference-- so making it possible to serve\nmodels at scale super efficiently with really\ncool new things with XLA for JAX and PyTorch. JOANA CARRASQUEIRA: OK. JOSH GORDON: Oh, and then\none more thing to mention. I went too fast. So sorry about that. So a lot of this talk is about\nthese huge foundation models. Towards the end of the talk,\nI'll talk about Google AI Edge. And we'll talk about\ndeploying small models on device, which is also super\nimportant for many reasons. JOANA CARRASQUEIRA: Awesome. OK. Let's start by exploring\nour core intelligence within our stack. We'll start with\nour Gemini models, which are our most capable\nand versatile model family. And our core philosophy here at\nGoogle is to provide developers with state-of-the-art models and\ntools that you can use to build powerful applications\nall throughout. And our Gemini models, they\nare known for being multimodal, have a long context window, and\nhaving very powerful reasoning, but we've built a variety of\nmodels for different use cases. So depending on what\nyou're trying to build, Google will have a model that\nis tailored for your use case. And I would like to just\ngive you a quick walkthrough of these models. I know you've heard\nit during the keynote, but, just very quickly,\nGemini 2.5 Pro, which is our most advanced\nmodel yet, especially for high complex tasks that\nbenefit from deep reasoning, it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 2,
    "video_id": "4TE-KFXvhAk",
    "start": 230.74,
    "end": 380.44,
    "text": "it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it because of its\nefficiency and speed, and it's now even better at\nalmost every single dimension. So we improved\nall the benchmarks across reasoning, coding,\nmultimodality, and also long context. Then we have our Gemini 2.0\nFlash, which is fast and cheap, works fine, and our\nGemini Nano, which is optimized for on-device tasks. And as you've heard, we've\nbeen shipping relentlessly. And I would like to give\nyou just a quick highlight of everything that we've\nbeen shipping in AI Studio and the Gemini API. There's a talk\ntomorrow that I would like to invite you to attend,\nwhich is by Shrestha Basu Mallick, a group product\nmanager on Gemini API, and Luciano Martins, our\ntechnical lead for Gemini API from DevRel. And they're going to do a deep\ndive into everything that's new within the Gemini\nAPI, so you definitely don't want to miss\nthat session tomorrow. But for now, just a glimpse to\nget you excited about what's new in AI Studio. We've built a new tab\nthat is called Build that instantly generates web apps. And it's really cool, because it\nenables developers and builders alike to prototype very\nquickly with natural language. We have a new generative media\nexperience in AI Studio as well, and I'm going to\ndemo all of this so you can see how\nit actually works. And we are always\nlistening to the community. We listen to your\nfeedback, and we always build with developers in mind. And that's why that\nsome of these features were actually\nrequested by community, and that's what happened with\nthe built-in new dashboard. You requested. We built it. And we also have some\nnew Native Audio and TTS support in AI Studio. On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 3,
    "video_id": "4TE-KFXvhAk",
    "start": 363.97,
    "end": 521.659,
    "text": "On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API and also on [INAUDIBLE] API\nfor real generating audio. And some of the use\ncases here that we had in mind when\nwe built this were more dynamic audiobooks,\nsome more engaging podcasts, or, even for those of\nyou in customer support, producing more natural\nvoices into your workflows. We also have enhanced\ntooling, which is really cool, because now you\ncan use Grounding with Google Search, also with Code Execution\ntogether in just one API call, and URL context, which you\nheard during the keynote, which provides the model with\ndepth content from web pages. And since you can chain\nit to other tools, it's actually really powerful\nto build search agents. So really cool stuff. Lastly, just to call out that\nwe now offer Gemini SDK support for MCP, which reduces a\nlot of developer friction and simplifies building\nagentic capabilities. So you don't want to\nmiss tomorrow's talk to learn more about this. And Google AI Studio. Who here uses Google AI Studio? Can I see some hands? OK, awesome. Awesome. We have a lot to chat\nafter this session then. Google AI Studio is the\nperfect place for anyone to start developing with AI. It's the simplest way to test\nthe latest DeepMind models. We typically bring them\nto Google AI Studio so you can start prototyping\nand playing with the models. And you don't need Google\nCloud knowledge in order to set up your environment. It is free of charge, and you\ncan create, and test, and save your prompts. And there's also starter\napps that will inspire you. And that's exactly what I'm\ngoing to demo here today for you. I am going to show\nyou some of the work that the team has been doing. So if we go to Google AI Studio\nhere, if we go into Build, and Mumble Jumble is\nsomething that we've literally just created, and\nit's one of my favorite apps. So Mumble Jumble is one\nof those applications that",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 4,
    "video_id": "4TE-KFXvhAk",
    "start": 502.44,
    "end": 656.4300000000001,
    "text": "So if we go to Google AI Studio\nhere, if we go into Build, and Mumble Jumble is\nsomething that we've literally just created, and\nit's one of my favorite apps. So Mumble Jumble is one\nof those applications that with just natural\nlanguage allows you to interact with\nthe model so you can build those more dynamic and\ninteractive audio experiences. So let's have a look. So, first of all, it uses 2.5\npreview native audio dialog. You can choose if it's easy to\ninterrupt or hard to interrupt. And then you can come and\npick your voice, hi pitch. So there's multiple things\nthat you can play with and then you can customize. I really like the fox, so let's\ngo ahead and pick the fox. Let's get the cowboy. Happy because we're\nat Google I/O. We're all super\nhappy to be here. And let's get a microphone,\nbecause I have one. AI FOX: Howdy, partner. Finley Flicker Fox here ready\nto charm the boots off you. JOANA CARRASQUEIRA: Hello. What's the weather like\ntoday in Mountain View? AI FOX: Well now, it's a fine\nday in Mountain View, sunny and quite pleasant. Perfect for moseying\non out and about. JOANA CARRASQUEIRA: I agree. Are there any hiking trails\nnear the Google campus? AI FOX: Why sure as shooting. There are plenty of trails\nnear the Google campus, perfect for a grand\nadventure and some fresh air. JOANA CARRASQUEIRA:\n[SPEAKING PORTUGUESE] AI FOX: Well now, the best\nway to get to those trails is to head towards the foothills\nfollowing the marked paths. You'll find them\nright along the way. JOANA CARRASQUEIRA:\n[SPEAKING PORTUGUESE] AI FOX: You betcha. There are a few\ncozy cafes nearby, perfect for a spell\nof rest and a good cup of coffee after your hike. JOANA CARRASQUEIRA:\nThank you so much. AI FOX: It was my\npleasure, partner. Happy trails, and\nenjoy your coffee. JOANA CARRASQUEIRA: Thank you. So, as you could see, there's\nsome really cool experiences that we're bringing\ninto AI Studio. Audio is getting better; more\nnatural experiences with voice. And in case you\ndidn't notice, I even",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 5,
    "video_id": "4TE-KFXvhAk",
    "start": 641.14,
    "end": 791.74,
    "text": "JOANA CARRASQUEIRA: Thank you. So, as you could see, there's\nsome really cool experiences that we're bringing\ninto AI Studio. Audio is getting better; more\nnatural experiences with voice. And in case you\ndidn't notice, I even changed the language in how\nI interacted with the model. I spoke in Portuguese,\nmy mother tongue, and it actually replied\nwith very good information. So what I did\nhere, Josh is going to show you exactly\nwhat is happening on the API side of things\nin just one second. I have one prompt that I just\nwant to very quickly show you. Sorry. Roll a dice twice, and\nwhat's the probability of the result being seven? OK, let's just run\nthis very quickly, because I just want to show\nyou one thing before I hand it over to Josh. So as you can see,\nthought summaries. The model is actually\nshowcasing how it thinks, and you can see\nthe summaries here. We have the result. And\nthen, basically, what is available in the UI\nin AI Studio is also available in the API. And Josh is going to\nshow you that right now. JOSH GORDON: Thanks. OK, great. So very briefly,\nwe have something called the Gemini Developer\nAPI, which is really great. It's the easiest possible way to\ndevelop with Google's foundation models. The best place to get\nstarted is ai.google.dev. There is a whole lot of\ncapabilities in the API. It's got code execution. It's got function calling. I remember sitting down\nwith the team to build this from a blank piece of paper. Starting about two years\nago, we had basically you could prompt it with\ntext, and now we have image understanding,\nvideo understanding, but now we can also\ngenerate images and videos that Joana will show you later. Very, very briefly,\nai.google.dev has all of our\ndeveloper documentation. There's lots of\nreally great guides, there's information about\nthe models, everything you need to get started. We also have the\nGemini API Cookbook. We have a link to\nthis at the end. It's basically goo.gle/cookbook. G-O-O.G-L-E/cookbook. And this will take you to\na whole slew of notebooks",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50,
      51
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 6,
    "video_id": "4TE-KFXvhAk",
    "start": 777.32,
    "end": 892.14,
    "text": "there's information about\nthe models, everything you need to get started. We also have the\nGemini API Cookbook. We have a link to\nthis at the end. It's basically goo.gle/cookbook. G-O-O.G-L-E/cookbook. And this will take you to\na whole slew of notebooks that the team has put together. And basically, all\nof these notebooks are end-to-end examples\nthat show you one thing that you might be interested\nin, like what's the best way to do code execution? What's the best way to\ndo function calling? You'll find that\nin the cookbook. I also very, very quickly\nwant to show you how easy it is to get started with the API. So basically, in\nGoogle AI Studio, you don't need a credit\ncard or anything like that, in about a minute you can\njust click Get API key. Create your key. Now, if you're doing\nthis for the first time, behind the scenes,\nthis will automatically create a Cloud project\nfor you, but that detail is not important. Basically, now I have an API key\nand I'm ready to install the SDK and call the model. If you open up any of the\nnotebooks in the cookbook, well, let's just say-- it's in a different\ndirectory here, but let's just say\nwe've opened up-- eh, we'll just say we\nopened up this one, which is in the quickstarts directory. And this shows you\nexactly what Joana showed, how to get the\nthinking summaries. You can add your API\nkey in Google Colab. If you zoom in, you\ncan hit Add new secret. And in this particular notebook,\nit's called Google API key. But you could call\nit whatever you like. So you would add\nGoogle API key there, you would paste your\nkey there, and now you're ready to run this. So if you do Runtime and Run\nall, you're calling the API and you're running\nall the examples. You can also, directly\nin Google Colab, we have this thing where you\ncan grab an API key straight inside Google Colab. So it's just really\nquick and easy to do. OK, we can go back\nto the slides. So very, very\nquickly, as a recap, Gemini Developer API is the\neasiest way to get started. It's super lightweight,\nit's fast to install,",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 7,
    "video_id": "4TE-KFXvhAk",
    "start": 880.26,
    "end": 979.77,
    "text": "So it's just really\nquick and easy to do. OK, we can go back\nto the slides. So very, very\nquickly, as a recap, Gemini Developer API is the\neasiest way to get started. It's super lightweight,\nit's fast to install, and you can get up and running\nhonestly in about a minute. OK? I will use the clicker. This is the flow to get\nstarted with Google AI Studio. Go to Google AI Studio. Get your key. Try one of the code\nexamples on ai.google.dev or in the cookbook. I see people taking pictures. That makes me happy. Please try this. We spent so much time\non making this easy, and I hope it works for you. If not, please file an\nissue and we'll get on it. This is the Gen AI SDK\nfor the Gemini API, and this is something\nwe've been rolling out. It's our latest SDK. We've been rolling\nit out gradually over the course of\nthe last six months. It's super user friendly. It's really easy to use. Really, the only point\nI want to make here, because I don't want\nto read the code examples or the\ndocumentation to you, you can call the API\nin a few lines of code. Basically add your key, select\na model, write a prompt, you can go ahead and call it. You can also get access\nto advanced functionality in one line of code. So if you'd like to get the\nthinking summaries that Joana showed you, you can just\nadd a ThinkingConfig, say include the thoughts. Now, you've got the\nthinking summaries. And a good use\ncase for this could be any time you need to\nexplain the model's reasoning. Particularly, you can\nimagine, if you're building an education\napp or a tutoring app, you can get the\nthinking summaries. In addition to\nreally cool things that you can do with\na single line of code, there's some more advanced stuff\nthat you can do with the SDK as well. So I know there's a lot\nof code on this slide, but we've talked a lot\nabout building agents and agentic experiences. In this example,\nyou could imagine that you have a Python\nfunction on your laptop called weather_function. And maybe that calls\nyour own weather server to get the weather.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50,
      51,
      52,
      53,
      54
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 8,
    "video_id": "4TE-KFXvhAk",
    "start": 967.05,
    "end": 1087.58,
    "text": "but we've talked a lot\nabout building agents and agentic experiences. In this example,\nyou could imagine that you have a Python\nfunction on your laptop called weather_function. And maybe that calls\nyour own weather server to get the weather. What you can do is you can pass\nthe definition of that function to the Gemini API\nin JSON, including the function name and the\nparameters that it takes. Then what you can do is\nyou can write a prompt. So here, the prompt happens\nto be \"What's the temperature in London?\" When you send the\nprompt and the function to the model, what\nthe model will do is assess whether it makes sense\nto call that function based on your prompt. If so, it won't\nactually call it. But you can see in the\nfunction_call.name that it returns and the\nfunction_call.args, it returns the name of the\nfunction and the arguments to pass to it. So if you want, you're\nready to call this function on your laptop. And we have code that you can\ncopy and paste to do that. What's really cool too is this\nworks with multiple functions at the same time. So you can imagine you have a\nfunction like schedule a meeting or something like that, and\nyou can very easily-- well, with some work, you could build\nan agent to actually do that. So function calling\nis super important, and it works extremely well. So now, Joana's going\nto talk about GenMedia. JOANA CARRASQUEIRA: Awesome. So as you can see, what\nyou can build in the UI within AI Studio, also\navailable in the API. And also, just building on the\ncapabilities of our foundation models, our core\nintelligence also encompasses a powerful suite\nof generative media models. And they are designed to\ntransform creative experiences across content generation\nacross different modalities like images, video, and audio. And I would also like to\ndemo one of these new apps that we have in AI Studio. So I'm going back to\nthe laptop, and I'm going to show you something\nthat the team also just created. So AI Studio also got a facelift\nand has some new features.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 9,
    "video_id": "4TE-KFXvhAk",
    "start": 1066.46,
    "end": 1237.45,
    "text": "And I would also like to\ndemo one of these new apps that we have in AI Studio. So I'm going back to\nthe laptop, and I'm going to show you something\nthat the team also just created. So AI Studio also got a facelift\nand has some new features. And the chat interface\nis still the same, but you've seen the talk to\nGemini live during the keynote. We have the new\nGenerative Media Console, which allows you to\ncreate and interact with our most creative models. And then we have\nthe Build, which is where all these new\napps are coming to. So I just wanted to show\nyou very quickly this one. There we go. And then we basically\ncan choose here what are the sounds\nthat we want. And this is all powered by\nLyria, our music generation model. [GENERATED MUSIC PLAYING] And just for the\ninterest of time, I'm not going to\nkeep playing it. But you can see some of the\ncapabilities of these models that we're bringing\nto AI Studio. We'll continue to the slides. In the Console,\nas you could see, you have access to\nour image generation, our video generation, and music\ngeneration models with applets to get you started. And so that's a really\ncool thing for you to play with after this session. Some of our videos,\nvery realistic images with a really good understanding\nof real-world physics and dynamics, improved quality,\nand more and more capabilities coming to these models. And this is the example\nthat I just showed. We've made Lyria RealTime our\ninteractive music generation model, which powers MusicFX DJ. It's available in the\nAPI and AI Studio, and you can check\nour API documentation for more information. This also allows\neveryone to interact, to create, and to perform\ngenerative music in real time. It's really cool. You might remember that in the\nshow before the first keynote, you might have\nseen this console. That's exactly why I wanted to\nshow you this particular app in this session. But there's a lot more that\nyou can try afterwards. And shifting the\ngears towards Gemma, earlier this year we\nreleased Gemma 3, which",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 10,
    "video_id": "4TE-KFXvhAk",
    "start": 1223.76,
    "end": 1374.39,
    "text": "That's exactly why I wanted to\nshow you this particular app in this session. But there's a lot more that\nyou can try afterwards. And shifting the\ngears towards Gemma, earlier this year we\nreleased Gemma 3, which is our most advanced model. And it comes in four sizes,\n1B, 4B, 12B, and 27B, and offers developers the\nflexibility to optimize performance for\ndiverse applications, from efficient\non-device inference, to also scalable\nCloud deployment. And in particular, 4B, 12B, and\n27B is multimodal, multilingual, and has a long context\nwindow up to 128,000 tokens. And the fact that it's available\nin more than 140 languages is really cool, because\n80% of our users are actually outside\nthe United States. And you heard during\nthe keynote as well that MedGemma is our\nmost capable collection of open models for multimodal\nmedical text and image comprehension. It's a really good starting\npoint for building medical application, and it's\navailable in 4B and 27B. You can download the\nmodel and adapt it to your use case via\nprompting, fine-tuning, or agentic workflows. And we also announced Gemma 3n. It's optimized for on-device\noperation on phones, tablets, and laptops. And as you can\nsee, the Gemmaverse is booming with all these\nnew variants coming and being developed all the time. Chill Gemma DolphinGemma,\nnow MedGemma, SignGemma-- so many different\ncapabilities and options that it's truly exciting to see. And one last thing that we\nare really excited about is the fact that now\nwe brought to AI Studio the possibility to deploy\nthe Gemma models directly from AI Studio into\nCloud Run with one click. So you can use the\nGen AI SDK to call it and just requires\na two-line change. Change API key, change\nbase URL, and you're set. That's the easiest deployment. And now, Josh is going to\ntell you all about frameworks. JOSH GORDON: Thanks. OK, so we've talked a lot\nabout foundation models, Gemini and Gemma. Now, let's talk a little\nbit about the frameworks that Google and the\ncommunity use to build them. So a lot of cool stuff to cover.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 11,
    "video_id": "4TE-KFXvhAk",
    "start": 1363.027,
    "end": 1474.6200000000001,
    "text": "JOSH GORDON: Thanks. OK, so we've talked a lot\nabout foundation models, Gemini and Gemma. Now, let's talk a little\nbit about the frameworks that Google and the\ncommunity use to build them. So a lot of cool stuff to cover. Let's start with the\neasiest possible way to get started to fine tune a model. So in the developer keynote,\nGus showed a version of Gemma that speaks emoji. And this is a language that\nhe came up with his daughter. One way to do that is you\ncould just prompt the model to speak emoji. And in a lot of cases, you\ncan get away with a prompt. But if you have a very\nlarge amount of data, or maybe you're building a\nreally serious application like something in health or\nmedicine, what you can do is you can fine tune\nthe model to work even better with your data. And a really, really\ngreat thing about this is the truth is it\nsounds complicated, but it's not in practice. All you really need is\na two-column CSV file. And here, what you're\nlooking at is something with a prompt and a response. And if you've got\na couple thousand rows using our framework Keras-- and Keras is my favorite way by\nfar of doing just applied AI. That means using AI in practice. You can tell I care a\nlot about-- both of us care a lot about\nhealth and medicine, so there's a lot of wonderful,\nmore than you could ever count, opportunities to do\ngood in the world in those fields using\ntechnologies like this. You can train the model to\ndo something really useful. So we have a really great\ntutorial about this. It's honestly about\nfive key lines of code. You import a model of\nGemma from Keras hub. This model is already\ninstruction tuned. You can prompt it\nin a line of code, and you can also\ndo LoRA fine-tuning in about a line of code, which\nalso sounds fancy, but it's not. So Keras is great\nfor Applied AI. If you're doing research,\nwe have a really wonderful framework called JAX. JAX is a Python machine\nlearning library, and I guess I have two\nthings to say about it. One is that at the\nhighest scales, JAX is the best place to go.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 12,
    "video_id": "4TE-KFXvhAk",
    "start": 1459.79,
    "end": 1567.51,
    "text": "If you're doing research,\nwe have a really wonderful framework called JAX. JAX is a Python machine\nlearning library, and I guess I have two\nthings to say about it. One is that at the\nhighest scales, JAX is the best place to go. So it scales really easily\nto tens of thousands of accelerators. It's super powerful. We use it to build\nGemini and Gemma. The community uses it to build\na bunch of really large, awesome foundation models as well. But one thing I like about JAX-- because I'm operating at\na much simpler level-- at its core, it's a Python\nmachine learning library with a NumPy API. And when a new model\ncomes out or a new paper, it takes me a long\ntime to understand it. What I like to do is\nbasically implement it line by line in NumPy, and I\nvery carefully just understand the input, the\noutput, the shapes, and debug it just in NumPy. And what's really\nwonderful, if you use JAX, you can do that in NumPy. There's transforms that\nyou can read about. You can add a line of code\nlike grad to get the gradients. You can add a line of code\njit to jit compile your model. And now, without\nchanging anything else, you can run it on GPUs and TPUs. So JAX core gives you\nthis really good way to think very carefully through\ndifferent techniques in machine learning. And then, when you're\nready, you can scale them up without really\nchanging your code. And that's really,\nreally awesome. On top of JAX, which is\nout of scope for this talk, there's a huge\necosystem of libraries. So there's great\nlibraries for Google in the community for things like\noptimizers, and checkpoints, and implementing\nneural networks. So you don't have to do that\nfrom scratch if you want. But just as I'm\nlearning things, if you do it totally from\nscratch once, you really can-- at least it helps\nme get my head around it, even though it takes\na little while. If you want to skip\nthat part and you want to go straight\nto just show me a super optimized large language\nmodel implemented in JAX that's ready to scale\nto hundreds or even",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 13,
    "video_id": "4TE-KFXvhAk",
    "start": 1555.152,
    "end": 1662.2379999999998,
    "text": "even though it takes\na little while. If you want to skip\nthat part and you want to go straight\nto just show me a super optimized large language\nmodel implemented in JAX that's ready to scale\nto hundreds or even thousands of\naccelerators, then there's two really cool GitHub\nlibraries that I'd point you to. MaxText, as you might guess,\nhas reference implementations of large language models. And MaxDiffusion has,\nas you might guess, reference implementations\nand models that you can use to\ngenerate beautiful images and stuff like that. Those can take some\nwork, but we're working on making them\nsuper user friendly. But right now,\nthey're designed for-- I think the way I\nthink about it is-- well, anyway, they take\nsome work to scale, but they're great. Using JAX-- this just\ncame out yesterday. I wanted to point you to\nnew, really amazing work from the community. And so we've been talking about\nGoogle's foundation models. This is a new foundation model\nthat Stanford University just released. This is called Marin. It happens to be built with\nJAX and TPUs, which is great. But what's really\nspecial about it is that Marin is a\nfully open model. And so, in addition to sharing\nthe weights in the architecture, they've shared the data sets\nthat they use to train it, the code they use to\nfilter the data sets, the experiments that worked, the\nexperiments that didn't work. So this is a really great\nfoundation for open science and building these really\ncool models in the open. And they trained this model\nusing Google's TPU Research Cloud. And this is a collection of TPUs\nthat if you're a researcher, you can apply for access to. And it's basically a\nfree-of-charge cluster of TPUs that you can use to do really\ncool research like this. Very briefly, we talked\nabout doing LoRA training-- or, excuse me, LoRA\npost-training in Keras. And now, I'll show you a\nlittle bit about what we're working on for tuning in JAX. So we're working on a\nnew library called Tunix. And the vision here-- it's\nvery, very early stage--",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 14,
    "video_id": "4TE-KFXvhAk",
    "start": 1651.95,
    "end": 1751.97,
    "text": "or, excuse me, LoRA\npost-training in Keras. And now, I'll show you a\nlittle bit about what we're working on for tuning in JAX. So we're working on a\nnew library called Tunix. And the vision here-- it's\nvery, very early stage-- we're building it\nwith the community. So we're working\nwith researchers from these great universities. And the vision is to\nmake it a really easy to use library for developers\nbut also a really good framework for researchers to implement the\nlatest post-training algorithms in JAX. And we're working\non a bunch now. I think it's going\nto be really good. And stay tuned. So that's Tunix. In addition to the libraries,\nvery briefly I just want to talk about\ninfrastructure. So TPUs, hardware, out of scope. But there's a really\ncool software package that I wanted to briefly\nmention called XLA. And XLA, it's\nbasically a compiler for your machine learning code. The way this works\nis that when you use a library like JAX or Keras\nor TensorFlow, or even PyTorch, what you're doing is you're\nwriting code in Python. And then somehow it gets\ncompiled, and optimized, and run on GPUs and TPUs. And XLA is the compiler that\nwe use at Google to do that. It powers our entire\nproduction stack. It's used by some of the\nlargest large language model builders in the world. And what it does is it\ntakes your Python code, does a whole bunch\nof optimizations, and gets it ready to\nrun on accelerators. One thing that's really cool\nabout XLA is it's portable. So if you run an XLA, you're\nnever locked into TPUs. You can use your\nexact same code to run on GPUs and other\ntypes of accelerators. So it's really great for that. We like it a lot. The important thing here is that\nPyTorch now also works with XLA. So if you're a\nPyTorch developer, it has a wonderful ecosystem,\nreally great libraries. If you want, you can use PyTorch\nXLA to train your models on TPUs and get all the really good\nprice performance benefits that come with that. In addition to\ntraining models, we've done great work with\nthe vLLM community.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 15,
    "video_id": "4TE-KFXvhAk",
    "start": 1739.61,
    "end": 1851.45,
    "text": "If you want, you can use PyTorch\nXLA to train your models on TPUs and get all the really good\nprice performance benefits that come with that. In addition to\ntraining models, we've done great work with\nthe vLLM community. So now, you can also\nserve your PyTorch models using vLLM on TPUs. And vLLM is a super\npopular inference engine. We've added TPU\nsupport, so that's available to PyTorch\ndevelopers now as well, and we're also working on\nadding JAX support to vLLM. Here's some more really\ngreat work that's happening with the community. So this is a new partnership\nbetween Red Hat, NVIDIA, and Google. And it's working on a\nproject called LLM-d. And the vision here, this\nis for distributed serving. The vision here is\nto bring the very best of serving into\nopen source and make it available to everybody and\nto have this work with both JAX and PyTorch. So really cool new project. There's some more sophisticated\nstuff which you can check out. And stay tuned for this,\nit's going to be really good. OK. So at warp speed, we have\ntalked about basically foundation models Google has,\ndifferent frameworks that we use to train them, different\nways that you can serve them on the Cloud. Now, let's briefly look\nat how you can deploy them on mobile devices. The way that you would do this\nis using Google AI Edge, which is basically a framework for\ndeploying machine learning models on things\nlike Android, iOS, get them running in the browser,\nand also on embedded devices. And I know it's Google\nI/O. A lot of you are mobile developers,\nso a lot of this is probably intuitive to you. But if you're coming from\nI'm a Python machine learning developer, I work\nin the back end, this is all really\nawesome points. There's many good\nreasons why you might want to deploy on mobile. One is latency. So you can imagine, if\nyou're doing something like sign language recognition\nand maybe the user is holding up their hand and they're signing,\nyou don't want to drop frames. And if you're sending those\nframes to a server on the cloud,",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 16,
    "video_id": "4TE-KFXvhAk",
    "start": 1840.035,
    "end": 1931.482,
    "text": "So you can imagine, if\nyou're doing something like sign language recognition\nand maybe the user is holding up their hand and they're signing,\nyou don't want to drop frames. And if you're sending those\nframes to a server on the cloud, unless you happen to have\nthe world's fastest internet connection, you're probably\ngoing to drop frames. But if you have that gesture\nrecognition model running locally, you're not going to. So that's one huge advantage. Others, of course, are privacy. Data doesn't need\nto leave the device. I mean, a lot of this offline. I know this is obvious\nto mobile folks, but if you're working\non an airplane, maybe you want to run your\nmachine learning model there. Cost savings is a really\nimportant one too. So if you're serving a model\nto lots of users on the cloud, you might be paying\nfor the compute that you need to serve it. But of course, if it's\nrunning on the phone, the compute's happening\nlocally, so you don't need to bother with\nserving infrastructure. There's a lot of really cool\nnew stuff in Google AI Edge. On our side, we've added support\nfor things like the latest Gemma models. And by the way, this is for both\nclassical machine learning-- well, \"deep learning,\" which is\nnow suddenly becoming classical. Things like gesture recognition,\nwhich were state-of-the-art four years ago, now that's classical\nML because we're talking about large language models\nand generative AI. But you can run small large\nlanguage models on device. We have a new really awesome\ncommunity with Hugging Face, and there's a lot of really\nsmart people putting together models that are ready to\nrun pre-optimized on device. And we have a private\npreview-- this is coming soon-- for AI Edge Portal, which is\nbasically a testing service. So you submit your model\nto a cloud service, and it runs it on a fleet of\nreal devices of different sizes just to verify that\nit works really well. So if you're interested\nin mobile development, check out Google AI Edge. Google AI Edge,\nit's really cool.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 17,
    "video_id": "4TE-KFXvhAk",
    "start": 1920.12,
    "end": 2074.09,
    "text": "and it runs it on a fleet of\nreal devices of different sizes just to verify that\nit works really well. So if you're interested\nin mobile development, check out Google AI Edge. Google AI Edge,\nit's really cool. And with that, I'll\nhand it over to Joana to talk about what's next. JOANA CARRASQUEIRA: Awesome. Thank you, Josh. And you've heard it in the\nkeynotes in the previous session with Demis and Sergey,\nwe're pushing the boundaries of what's possible to build with\nAI here at Google and Google DeepMind. And we're really excited to\nbring all this innovation and put it in the\nhands of developers, in the hands of the community. And it's never been a better\ntime to build and co-create together. So we really believe\nin a future where AI is changing various fields\nacross scientific discovery, health care, and so many more. And we're going to achieve\nthis radical abundance in a safe and\nresponsible way, and we want to get there with\nyou, with the community. So let's have a look\nat some of the domains that we believe that have a\nhuge potential for developers and humanity at scale. AlphaEvolve, a\nGemini-powered coding agent for designing\nadvanced algorithms. A self-improving coding agent. And we all know that\nlarge language models can summarize documents. They can generate code. You can even\nbrainstorm with them. But with AlphaEvolve,\nwe're really expanding these\ncapabilities, and we are targeting fundamental\nand highly complex problems on\nmathematics and coding. AlphaEvolve leverages\nGemini Flash and Pro, and it's one of the big\npromises for the future. Another one-- and I'm really\nexcited about AI co-scientist. It's another\nscientific breakthrough that we're seeing, especially\nin the medical, in medicine, and research fields. And our goal is to accelerate\nthe speed up discovery and drug development. And with AI\nco-scientist, a scientist can give a research goal to\nthe agent in natural language, and then the AI\nco-scientist is designed to give you an overview, and a\nhypothesis, and a methodology.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 18,
    "video_id": "4TE-KFXvhAk",
    "start": 2055.719,
    "end": 2210.57,
    "text": "development. And with AI\nco-scientist, a scientist can give a research goal to\nthe agent in natural language, and then the AI\nco-scientist is designed to give you an overview, and a\nhypothesis, and a methodology. So in order to do so, it uses\na coalition of different agents that work together. And we have the generation,\nagent review, ranking, evolution, proximity,\nand meta review that are all created\nwithin the inspiration and driven from the\nscientific method in itself. So it's another huge\nbreakthrough and another domain that we'll continue\nto see evolving here at Google DeepMind. And lastly, an area where we're\nseeing tremendous progress and we expect to continue\nhaving more future breakthroughs is in domain-specific models. And Gemini robotics\nmodels, which are currently in private early access, are\nadvanced vision, language, action models with the\naddition of physical actions as a new output\nmodality specifically for controlling robots. These models are robot agnostic,\nand it uses multi-embodiment which is a technique that can be\nused on anything from humanoids to large-scale\nindustrial machinery. So this is really,\nreally exciting. And Gemini robotics has been\nfine tuned to be dexterous. And that's why you can see so\nmany different cool use cases and applications here on stage,\nfrom folding an origami, which is something a bit more complex,\nand just holding a sandwich bag. So many new innovations\nare coming to you, are coming to life,\nand we'll continue pushing the boundaries of\nwhat's possible across all these different domains. And now, if you\nwant to learn more, there's many ways that you\ncan keep engaging with us, that you can keep\ngiving us feedback. We're also active\non social media, and we have a developer forum\nwhere you can interact directly with Googlers. So in order to learn\nmore, Josh, what do our developers have to do? JOSH GORDON: We have\njust a few links for you. So no problem. But we talked about a lot of\ndifferent tools in the stack. JOANA CARRASQUEIRA: We did.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 19,
    "video_id": "4TE-KFXvhAk",
    "start": 2201.15,
    "end": 2283.1,
    "text": "So in order to learn\nmore, Josh, what do our developers have to do? JOSH GORDON: We have\njust a few links for you. So no problem. But we talked about a lot of\ndifferent tools in the stack. JOANA CARRASQUEIRA: We did. JOSH GORDON: So I don't\nwant to read the slide, but let me just point you\nto a couple highlights. ai.google.dev is the\nbest place to go to get started with Gemini and Gemma. We have a cookbook for Gemini. We have a cookbook for Gemma. Google AI Studio is\naistudio.google.com. If you're interested in JAX\nand Keras, there are the links. If you happen to be\ninterested in XLA, please check it\nout-- openxla.org. Google AI Edge is\nat the very bottom if you're a mobile\ndeveloper and you're interested in mobile deployment. And just to be clear, there's\nso many amazing things in the Google AI stack we didn't\nhave time to talk about today. Vertex has really amazing tools\nfor enterprise developers. But please, start\nhere, have fun, and, yeah, we're\naround after the talk. JOANA CARRASQUEIRA:\nYes, absolutely. And the Developer Relations\nteam is just outside. We have some really\ncool demo stations that you can experience. Engage with the team. Check out the sessions\ntomorrow, especially on the Gemini API,\nGemmaverse, and robotics. We have a lot of\ncool stuff that we want to put in the\nhands of developers. Many early access\nprograms as well. Stay in touch, stay\nengaged, and let's co-create the future of AI together. Thank you so much. JOSH GORDON: Thanks a lot. Thanks [APPLAUSE] [MUSIC PLAYING]",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40
    ]
  }
]
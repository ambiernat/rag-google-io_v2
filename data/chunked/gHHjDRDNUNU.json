[
  {
    "schema_version": "chunk_v1",
    "chunk_id": 0,
    "video_id": "gHHjDRDNUNU",
    "start": 0.0,
    "end": 146.11,
    "text": "[MUSIC PLAYING] [CHEERING] [AUDIENCE SCREAMING] LUCIANO MARTINS: Hey, folks. Good morning. It's a pleasure to\nbe here with you all. I'm Luciano Martins. I'm Brazilian. [CHEERING] I'm an AI Developer\nAdvocate at Google DeepMind. And I'm here with\nmy friend Shrestha. SHRESTHA BASU\nMALLICK: Thank you. Luciano Hi, everyone. I am Shreshta Basu Mallick. I'm the Product Lead for\nthe Gemini Developer API. And it looks like I should have\nbrought an Indian contingent here. [LAUGHING] Thank you. LUCIANO MARTINS: Yay! OK, so the idea of\nthis conversation is we want to share with you\nsome of the new things you have available to develop your\nsolutions using Gemini models and the Gemini API. How many developers\nyou have here? Amazing. OK, so we can start talking\nabout the Gemini models universe. We started Gemini\nby the end of 2023. And since then, we\nhave done a lot of work together between many different\nGoogle DeepMind teams. And by now, we are very\nproud of the point we got in and all the stuff we\nlaunched at during I/O. Just doing a quick recap,\none of the key differentials of the Gemini is that it is\nmulti-modal from scratch. Since when we started\ndeveloping the model, it always handled and\nunderstood multi-modal data. It means that you\ncan work with Gemini with any format\nof information you want, from text, image, video,\naudio, code, anything, right Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. And just a quick\noverview of what are all the types\nof models, and what are all the families of\nmodels we have available. So what we're going\nto do today is Luciano and I split this\ntalk into two parts. So the first part\nof the talk will be talking about the models. And then the second\npart of the talk will be talking about the API. What are the capabilities\nand functionalities available through the API? And as part of that,\nwe have a section where we do a deep dive on\nagentic capabilities in the API. LUCIANO MARTINS:\nThanks for the add. SHRESTHA BASU MALLICK: Yeah. So starting with what are\nthe families of models we have available?",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50,
      51,
      52,
      53,
      54,
      55,
      56,
      57,
      58
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 1,
    "video_id": "gHHjDRDNUNU",
    "start": 132.94,
    "end": 265.63,
    "text": "And as part of that,\nwe have a section where we do a deep dive on\nagentic capabilities in the API. LUCIANO MARTINS:\nThanks for the add. SHRESTHA BASU MALLICK: Yeah. So starting with what are\nthe families of models we have available? So the most performant and\npowerful model that we have is the Gemini 2.5 Pro. It's in preview right now. A few weeks ago, we released\na updated version of 2.5 Pro. And it's suitable for highly\ncomplex tasks that require a lot of deep reasoning. And coding has been, of course,\na standout use case of that. We also have 2.5 Flash,\nwhich this Google I/O we released a new preview\nversion just yesterday for 2.5 Flash. And 2.5 Flash is\nour model size that has probably one of the\nbest price performance ratios in the market today. LUCIANO MARTINS: That's right. SHRESTHA BASU MALLICK: We\nalso have 2.0 Flash-Lite. So this is a small,\nfast, cheap model that you can use for high\nvolume tasks like summarization. And then Luciano,\ndo you want to talk about the Nano and\nthe Embedding models? LUCIANO MARTINS:\nYeah, absolutely. We heard a lot of\nfeedback from you folks. And many of you are\ndeveloping mobile applications or applications that\nmust run on devices. So then you have\nalso available what we call the Gemini\nNano, which is a smaller version of the Gemini, which is\nable to run locally on devices like on Android devices if you\nare developing with the Android Studio using the AI Corps. And also, many of you are\ntrying to create applications that do some kind\nof semantic ranking or to organize information\nin large scale. So you have also one model that\nwe call the Gemini Embedding, which the key objective of this\nmodel is to let you ingest text. And then the model delivers you\nhigh quality multidimensional embeddings. But how those\nmodels are behaving when we think about the key\nstandard benchmarks, Shrestha? SHRESTHA BASU\nMALLICK: Yeah, we'll go to the benchmarks\nin a second. But the message\nthat I want everyone to take away from this\nslide is whether it's",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 2,
    "video_id": "gHHjDRDNUNU",
    "start": 254.53,
    "end": 382.64,
    "text": "when we think about the key\nstandard benchmarks, Shrestha? SHRESTHA BASU\nMALLICK: Yeah, we'll go to the benchmarks\nin a second. But the message\nthat I want everyone to take away from this\nslide is whether it's a super complex task, or whether\nit's on-device processing, you now have a Gemini model\nthat you can use for it. And it's pretty powerful. And it's pretty price effective. LUCIANO MARTINS: Yeah,\nmaybe it is also worth mentioning, Shrestha,\nthat some folks here may be developing with\nGemini since it was released. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS: So using the\nAI Studio, using the Gemini API, the older versions\nof Gemini like 2.0, 1.5 are still available. But we really\nencourage everybody to start experimenting\nwith the newer ones with better capabilities\nand better performance. SHRESTHA BASU MALLICK:\nThat's correct. So 2.0 Flash continues to be one\nof our most widely-used models, just again, because of how good\nit is for the price it's at. And we have a lot of people\nwho are still using it. And then we also have some\npeople on the 1.5 models. But we're encouraging\npeople now, we're two generations\nahead, to start using the 2.5 series of\nmodels and give us feedback. LUCIANO MARTINS: Nice. So please tell us a little bit\nabout the benchmarks, Shrestha. SHRESTHA BASU MALLICK:\nYeah, next slide. Yeah, so we're not going\nto spend a lot of time on the benchmarks. But the two key points\nthat I'd like to hit here is number one, is one\nway in which we measure how good these models are is\nby putting them on LMArena, which is where developers\nand builders like yourself test out\nall these models and give it an ELO rating. And we're, of course, very\nproud that right now you have three Gemini models in\nthe top 10 in the LMArena. LUCIANO MARTINS:\nIncluding the top one. SHRESTHA BASU\nMALLICK: Including, how did I forget that? We are number one guys,\nand we're very proud of it. [CHEERING] Thank you. One of the things,\none of the use cases that has been really emerging\nin the last couple of months",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 3,
    "video_id": "gHHjDRDNUNU",
    "start": 364.16,
    "end": 515.85,
    "text": "SHRESTHA BASU\nMALLICK: Including, how did I forget that? We are number one guys,\nand we're very proud of it. [CHEERING] Thank you. One of the things,\none of the use cases that has been really emerging\nin the last couple of months is, of course, app creation\nand live coding and going from 0 to 1. The best leaderboard right now\nfor that is the WebDev Arena. And we again, are number one\nwith 2.5 Pro on WebDev Arena. So if you are trying to build\n0 to 1 or few shot apps, Gemini 2.5 Pro is a\ngreat model for that. But in addition to\nuser preferences, we also like to measure\nhow well our models are doing based on rigorous\nacademic benchmarks. And so I'm not going to\ngo into a lot of details on these slides. These are available\nin blog posts. Reach out to me\nand Luciano if you want to know how exactly\nthese models are doing. But the TL;DR here is whether\nit's in highly complex domain-specific questions,\nwhether it's in coding, whether it's in\nmultimodal understanding, the Gemini 2.5 Pro is leading\non a lot of these academic benchmarks as well. Finally, performance is\none side of the coin. The other side of the\ncoin is, of course, price. This is a chart from Swyx that's\nbeen very popular on Twitter for the last few years-- on X, I'm sorry for\nthe last few months. But this really shows that\neven for price performance, the Gemini 2.5 models are\nreally at the frontier. Luciano? LUCIANO MARTINS:\nYeah, and I think it is worth\nmentioning, Shrestha, especially for those\nof you who watched Sundar's keynote yesterday and\nalso the developer's keynotes. Inside the DeepMind, we\nare doing this huge effort of trying to cover the\ndifferent aspects of experiences you may have. So we will talk a lot\nabout Gemini here. But also, we have\nthe model family that we are calling\nGenMedia, where you are able to generate high\nquality images, high quality videos. You also have-- and there is a\ncolleague from our team, Paul Ruiz, who is delivering\na talk this afternoon related to the Gemini Robotics,\nthe Gemini family of models",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 4,
    "video_id": "gHHjDRDNUNU",
    "start": 501.0,
    "end": 641.8,
    "text": "you are able to generate high\nquality images, high quality videos. You also have-- and there is a\ncolleague from our team, Paul Ruiz, who is delivering\na talk this afternoon related to the Gemini Robotics,\nthe Gemini family of models related to people\napplying robotics, developing robotic\nsolution with applied AI. And also, just\nafter the session, we have one another one\nwith Omar and Gus Martins to talk about Gemma 3,\nwhich is the open models family for from\nGoogle DeepMind, also developed with all\nthe technologies we use to develop Gemini. SHRESTHA BASU MALLICK:\nAnd some really good benchmark performance-- LUCIANO MARTINS: Absolutely. SHRESTHA BASU\nMALLICK: --in Gemma 3 as well, as Omar and\nGus will tell you. LUCIANO MARTINS:\nSo still talking about the different\nexperiences you may have, we heard a lot of feedbacks\nfrom you folks around the world. And one of the most asked\nfeatures we always heard were related to letting Gemini\nto create high quality audio. So yesterday we launched what\nwe are calling the Gemini TTS model, where you can from text\ngenerate high quality audio. But the coolest\nthing about the TTS is that you are not\njust creating audio. You have abilities to customize\nemotions on the audio. You may have multiple voices. You can create the audios\nwith different languages, like Brazilian Portuguese. And you can even create\nmulti-speaker interactions. So if, for example, you want\nto deliver an experience where your users will be thoughts\nabout something on a podcast, like formats, like the one\nyou have on the NotebookLM that people love. Who here is using\nNotebookLM for anything? Amazing. So you can have something\nlike the AI Overviews that are developed and delivered\nby the Gemini TTS model using the TTS, right Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: And I think it\nis not the only thing related to audio. We brought that to the live\ninteractions as well, right? SHRESTHA BASU\nMALLICK: That's right. So the TTS models,\nlike Luciano said, are available via chat endpoint.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 5,
    "video_id": "gHHjDRDNUNU",
    "start": 628.55,
    "end": 769.3299999999999,
    "text": "LUCIANO MARTINS: And I think it\nis not the only thing related to audio. We brought that to the live\ninteractions as well, right? SHRESTHA BASU\nMALLICK: That's right. So the TTS models,\nlike Luciano said, are available via chat endpoint. And then on our Live API,\nyesterday, we also rolled out the native audio output models. So this is now\nGoogle's first release of an audio-to-audio\narchitecture. And there are two variants of\nthis model that you have access through the Gemini Live API,\nwhich is our real time API. So you have the Native\nAudio Dialogue model, which is the real benefit of\nthe native audio output models are that the voices\nthat come out of it are much more natural\nand much more compelling. The native audio-to-audio\narchitecture has better contextual\nunderstanding of what humans say to the AI. It can seamlessly transition\nbetween different languages, so maybe from Brazilian\nPortuguese to Bengali, if you want to switch\nin the same language, in the same sentence. And it's available on\nour Gemini real time API. We also have a version\nof this model available, again on the real time\nAPI with thinking enabled. So for more complex use\ncases, so one use case that comes to mind is let's say\nyou're building a gaming agent to play a strategy game. And you want that agent to be\nable to think, but also to talk to you with relatively\nlow latency. We have a thinking version\nof the native audio model that's also now available\nfrom the Live API. So please try it out\nand give us feedback. LUCIANO MARTINS: Perfect. So how people can start\nusing those models, Shrestha? SHRESTHA BASU MALLICK:\nThrough the API. Of course. But before we go there,\nthere are two other things that we should mention. It's hot off the news. You've probably\nall heard about it. We've also enabled\nan advanced reasoning mode called Deep Think. And the idea behind Deep\nThink is that the model 2.5 Pro can think through various\npossible answers to a problem before giving you\nthe best answer. So that is available\nin trusted testers,",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 6,
    "video_id": "gHHjDRDNUNU",
    "start": 751.58,
    "end": 904.85,
    "text": "mode called Deep Think. And the idea behind Deep\nThink is that the model 2.5 Pro can think through various\npossible answers to a problem before giving you\nthe best answer. So that is available\nin trusted testers, but will be rolled\nout more widely soon. And then we also have\nGemini Diffusion, which is the first time\nwe are testing a diffusion architecture as opposed to an\nautoregressive architecture. And these models are faster\nthan the fastest model out today and for almost\nsimilar, the performance. So try out both when you have\naccess to both Gemini Deep Diffusion and Deep Think. LUCIANO MARTINS: Perfect. SHRESTHA BASU MALLICK: All\nright, so the Gemini API. LUCIANO MARTINS: So yeah, if\nyou start thinking, especially after the keynote\nspeakers today, there are many surfaces where\nyou can interact with the Gemini models. So you have the Gemini app. You have Gemini available\non Google Cloud as the Code Assist, the assistant-- the\nCloud Assist inside the Console. You have Gemini on Workspace. So when you are talking\nabout developer experiences, we have the one\nspecific Gemini API with similar experiences as you\nhave on the no-code UI that we call AI Studio, where you\ncan start experimenting and developing your solutions\nusing the Gemini models, right? SHRESTHA BASU\nMALLICK: That's right. LUCIANO MARTINS: So\nbasically, the Gemini API is a very low barrier\nentry for having Gemini programmatic experiences. You have access to\nall public models there from the Gen Media ones,\nGemma, all the Gemini models, variants. You have a very generous\nfree of charge tier, where you can start\nexperimenting just after the session\nwithout concerns about credit cards, costs,\nbilling, anything of that. We keep developing\nmore SDKs for this API. So for now, you have available\none SDK for Python, JavaScript, Go, and Java. We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 7,
    "video_id": "gHHjDRDNUNU",
    "start": 890.57,
    "end": 1021.23,
    "text": "We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there. If you are a Google\nColab user, you have ways to interact with\nthe Gemini models as well. So the key idea is to\nmake it easier for you. No matter where you are having\nyour development experience, We are trying to bring\nGemini API closer to you. SHRESTHA BASU MALLICK:\nThat's correct. And I think this is\nalso a good place to make a plug for Google\nAI Studio, which of course, everyone knows about and loves. It is the place where\na lot of our developers first test out the\ncapabilities of the API before committing to\nbuilding applications at scale with the API. And Google AI Studio now\nhas code generation also. LUCIANO MARTINS: Absolutely. SHRESTHA BASU MALLICK: So this\nis a quick, high level overview of all the components that are\navailable through the Gemini API. It's pretty standard. You send in your prompt, you get\na response back from the model. If there's a tool\ncall needed, we do support a set of\nfirst party Google tools, as well as function calling. So in terms of the first party\nGoogle tools that we support, we, of course, support\nGoogle Search as a tool. The best search engine\nout there is now available as a tool for the\nmodel to call when it needs to. Now in addition to the\ninformation that you get out of Search, if you want to\nretrieve more in-depth content from web pages for\napplications such as you're building your own version\nof a research agent, so you pull some\ninformation from Search. You pull a set of\nURLs, but if you want to extract a little\nmore in depth content, we just released a new tool\nat I/O called URL Context. And then, of course, you can\ntake all that information. And if you want to create\nbeautiful charts out of it or run some analysis,\nwe make code execution available as a tool to\nyou via the API as well. Then, of course, you\nhave function calling.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 8,
    "video_id": "gHHjDRDNUNU",
    "start": 1006.14,
    "end": 1144.6,
    "text": "And then, of course, you can\ntake all that information. And if you want to create\nbeautiful charts out of it or run some analysis,\nwe make code execution available as a tool to\nyou via the API as well. Then, of course, you\nhave function calling. And as part of all\nthe improvements that we've released\nat Google I/O, we've also put a lot\nof effort into making sure our structured outputs\nfunctionality, which lets you get outputs\nin JSON schema, is much more robust\nand comprehensive. So again, try it out\nand give us feedback. And then finally, we do have\na set of safety and copyright filters. These are configurable\nby developers. The idea is in order-- you have the tools you need\nto make the applications you build safer. And you have control about\nhow-- for most of the filters, you have control about how where\nyou want to set that threshold. LUCIANO MARTINS: Perfect. So as a TL;DR, if we could\nexplain on a tweet how that works, with the Gemini\nAPI and using the SDK, you can work with all\nthe information you need. Doesn't matter on which format\nit is, if you have spreadsheets, docs, PDF files,\nvideos, audio, live interactions via voice\nand video sort of stuff. If you need to give specific\ninstructions to the model, you have the ability to bring\nthese instructions to the model follow on every interaction you\nhave during your application usage. And you have, as results\nor as outputs, the ability to have generated text or\ngenerated images or audio or video, or even keep\ntraining with more API calls using function calling. SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: That\nwas a huge tweet. SHRESTHA BASU MALLICK: That\nwas a huge tweet, maybe threaded tweet. All right, so let's talk about\nsome key Gemini API features. We don't have enough\ntime in this talk to go through everything. But Luciano and I thought we\nwould hit some of the highlights that we really want people\nto know more about and use. So one of the areas of feedback\nthat we used to get a lot is you can, of course, upload\nfiles to the Gemini API.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 9,
    "video_id": "gHHjDRDNUNU",
    "start": 1130.2,
    "end": 1280.26,
    "text": "But Luciano and I thought we\nwould hit some of the highlights that we really want people\nto know more about and use. So one of the areas of feedback\nthat we used to get a lot is you can, of course, upload\nfiles to the Gemini API. You can also, if you're\nless than 20 megabytes, you can also pass some of\nthese media information inline. But now, you can also-- and we've had this feature\nout for a few weeks. You can also pass\na YouTube link. And the Gemini API can\nanalyze that information. When you pass media files\nlike videos, depending on how much you want to fit\ninto the context window, you have now, a choice between\nthree resolution settings. At the lowest setting,\nyou can process up to six hours of video. But then you also have more\nhigh resolution settings that you can use. We support dynamic frame rate\nper second in the Gemini API. You can read that more about\nthat in our documentation, or come talk to me and Luciano. We support video clipping. That's a new feature. And we support\nimage segmentation. One other point I want to make\non multimodal understanding is even in the days\nof 1.5 and 2.0, Gemini models were\nsome of the best models out there for multimodal\nunderstanding. The example I like to give\nhere is I was in Costa Rica, and my guide showed\nme-- this is night. And my guide showed me\noh, there's a glass frog somewhere there on a branch. I took a photo, but I did not\nsee the glass frog in real life or in the photo. But I passed it to Gemini. And Gemini not\nonly saw the frog, it identified the\nspecies correctly. So that's how good multimodal\nunderstanding is on the Gemini. Please try it out. We also support long context. And then we, along with long-- long context is we have some\nof the largest context windows out there. So like the equivalent\nof depending on whether you're using a model\nwith 1 million context or 2 million context, you can read\nthe equivalent of eight novels, have the model read it\nand/or entire code bases. But what can sometimes\nhappen with long context",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 10,
    "video_id": "gHHjDRDNUNU",
    "start": 1264.27,
    "end": 1404.54,
    "text": "whether you're using a model\nwith 1 million context or 2 million context, you can read\nthe equivalent of eight novels, have the model read it\nand/or entire code bases. But what can sometimes\nhappen with long context is that your input\ntoken pricing goes up. But for that, we now\nhave context caching. We have been supporting what\nwe call explicit context caching for a while. Explicit context caching\nis when you tell the API, cache this context, and reuse\nit for the next few turns. And you were going to get a\n75% discount on the pricing. But now what we support is\nimplicit context caching. So if we feel that you\nhave a context that is getting repeatedly\nused, we will automatically cache that for you and pass\nthe price savings to you. So again, hopefully this\nis a huge price benefit to our developers. And then we, of course,\nprovide you with transparency to see how many\ntokens you are using. LUCIANO MARTINS: That's amazing. SHRESTHA BASU MALLICK: Yeah. Do you want to talk a little\nbit about text generation? LUCIANO MARTINS: Yeah, I think\none of the greatest things that we are building step by\nstep with the Gemini models and the Gemini API is, you may\nhave different experiences, as Shrestha explained,\nwith videos and clipping, specific time offsets, or\nchanging the frames per second. And then the first\nout-of-the-box output of Gemini when we launched it in December\nof '23 was text generation. So we keep having this. As one of the\nchoices you have, you can have an experience where\nGemini will just create text. And by text, it may\nbe like anything. It may be one\nstructured JSON output. Or it may be some\ncoding in Python, C, or any language of\nyour preference. But then we keep increasing\nthe semantic understanding capabilities of the models. So we are not just uploading\none PDF file with a lot of text and some charts and\nsome conclusions. You are also counting\non the Gemini ability to understand how the\ninformation on those documents or on those spreadsheets\nconnect to each other,",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 11,
    "video_id": "gHHjDRDNUNU",
    "start": 1387.2,
    "end": 1519.6,
    "text": "So we are not just uploading\none PDF file with a lot of text and some charts and\nsome conclusions. You are also counting\non the Gemini ability to understand how the\ninformation on those documents or on those spreadsheets\nconnect to each other, especially if you have a more\ncomplex situation, where you are sending multiple PDF files or\nPDF files with spreadsheets and videos and everything. Without a huge, heavy\nlift from your side, you can count on Gemini\nto understand what's the message, what's\nthe information, what's the reasoning behind all that\ninformation and do the math or do the understanding\nfor you, right Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. I just want to give a special\ncall-out to two features here, which again, developers\nhave been asking us for. One is you can get bounding\nboxes through the API. And secondly, in addition\nto bounding boxes, you have what's called\nimage segmentation, which is get the bounding\nbox information for an object in the image. You get a classification of\nwhat that image might be, so that's sent to\nyou as metadata. And then you get this\nmasked segmentation of that specific area\nand that object as well. LUCIANO MARTINS: Nice. SHRESTHA BASU MALLICK: We\nalso support streaming. And you can set\nsystem instructions on top of whatever\nsystem instructions we already have in place. LUCIANO MARTINS: Yep. And I think that's the\nsame for the media models right, Shrestha? So now you have basically,\nthree main doors to use the media models. You have the Imagen 3,\nthe, Google DeepMind model with the best high\nquality image generated. You have one variant of Gemini\nthat we call Gemini Image Out that lets you\nalso generate images, but with two key\ndifferences from Imagen 3. First, you can have interleaved\noutputs, including text and images together, like\nhaving one explanation on step-by-step guide to do\nsome action, including visuals. And also, you can edit\nimages, right, Shrestha? So you can have one first\nversion of the image generated.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 12,
    "video_id": "gHHjDRDNUNU",
    "start": 1506.06,
    "end": 1650.34,
    "text": "and images together, like\nhaving one explanation on step-by-step guide to do\nsome action, including visuals. And also, you can edit\nimages, right, Shrestha? So you can have one first\nversion of the image generated. You want to change the shirt\ncolor or the background or add glasses to the\ncharacter on the image, you can keep chatting\nwith the model, asking to modify the image. And you keep enhancing\nand optimizing your result, right, Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. And through Veo 2, you all\nsaw the release of Veo 3. That'll probably\ncome soon to the API. But with Veo 2, we support text\nto video and image to video. This is again from\na recent blog post that our researchers put\nout about a week ago. You can see the 2.5 Pro\nalso, as I mentioned earlier, leads on key video understanding\nbenchmarks like MMMU. So that's also something that\nwe're very proud of that we've been pushing the boundaries on. OK. So now let's talk\nabout the Live API. So a lot of our\nfeatures, as I mentioned, are available\nthrough the Chat API. But we also provide\nthe Gemini Live API, which is our real time, low\nlatency API for use cases that require more of these\ninteractive real time kind of experiences. There are two architectures now\navailable through this Live API. There is the cascaded\narchitecture, which has native audio\ninput, but the output is done using text\nto speech, and we were using the same text\nto speech models that was used by NotebookLM. A lot of people preferred\nthis architecture because it's more reliable. We've had it out since\nDecember, and we're aiming to bring the\nlatest 2.5 Flash model to this architecture. We also, starting I/O, now have\nthe audio-to-audio architecture that I mentioned, where you\nhave native audio input as well as native audio output. And this, of course, native\naudio output, as I already mentioned, gives you\nmuch more natural sounding voices\nand the ability-- you don't now have to\nspecify your language. You can seamlessly switch\nbetween languages and flow in and out.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 13,
    "video_id": "gHHjDRDNUNU",
    "start": 1635.49,
    "end": 1785.6299999999999,
    "text": "And this, of course, native\naudio output, as I already mentioned, gives you\nmuch more natural sounding voices\nand the ability-- you don't now have to\nspecify your language. You can seamlessly switch\nbetween languages and flow in and out. We support tool chaining\nin the Live API. We have supported\nthis since December. So all of the tools that\nI mentioned, Search, Code Execution, URL\ncontext, function calling, you can layer these\ntools in the same prompt and get much more\ncompelling results. Get data from Search, do some\nanalysis, get the output. We have voice\nactivity detection. Of course, we need\nit in the Live API. What we now provide for\nyou, though, is the ability to configure the\nthresholds, how much of time do you want after\nthe end of speech to decide that the\nuser's speech has ended. That's one of four\nthresholds that you can now set with the Live API. You can also disable our\nvoice activity detection model and bring your own. Session management. We have a lot of\nparameters out there. So in its most basic state,\nthe Live API currently supports about 20\nminutes of audio and about a few\nminutes of video. But we now have\nvarious techniques for you to increase\nyour session length, including sliding window, the\nability to change resolution on what video is\npassed, the ability to decide, do you want audio\nto be streamed only when-- do you want video to be streamed\nonly when audio is being spoken, or even when audio\nis not being spoken, and other parameters that you\ncan use through the Live API. Ephemeral tokens\nare coming soon, but that's one way to do\nauthorization into the Live API. And finally, with the native\naudio output, specifically the audio-to-audio\narchitecture, we are also releasing a couple\nof modes for you to try out. One of them is proactive audio. What this feature\ndoes today is it lets the AI decide\nwhen to respond to you and when whatever the human\nis saying is irrelevant. So imagine if I'm having a\nconversation with the AI, and Luciano comes to me and\nsays something unrelated.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 14,
    "video_id": "gHHjDRDNUNU",
    "start": 1767.92,
    "end": 1905.9299999999998,
    "text": "What this feature\ndoes today is it lets the AI decide\nwhen to respond to you and when whatever the human\nis saying is irrelevant. So imagine if I'm having a\nconversation with the AI, and Luciano comes to me and\nsays something unrelated. The AI will know not to\nrespond to that audio output, so the AI proactively\ndecides not to respond. So we are calling\nit proactive audio, because we aim to\nbring much more proactivity to this feature. And then effective\ndialogue lets you pick up on the user's tone\nand sentiment and lets the AI respond appropriately. As I mentioned, you also\nhave thinking available with the Live API. All right. Time for agents. LUCIANO MARTINS: Excellent. So how many of you are\ntrying to experiment solving your computational\nproblems using agents or multi-agent solutions? Yay, everybody. SHRESTHA BASU MALLICK: All\nthe Brazilian contingent. LUCIANO MARTINS: OK. So with that in\nmind, we always try to develop the new tools and the\nnew capabilities of the model thinking how you\nfolks can use them in your projects to make agents\nbetter and more trustful. Right, Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: So\nif you start thinking about how our regular\nagents architecture work, what do you have\nthere, Shrestha? So basically, we have\nthree main blocks. SHRESTHA BASU MALLICK: Yes. LUCIANO MARTINS: We normally\nsee one orchestration layer, one models layer,\nand one tools layer. Right? SHRESTHA BASU MALLICK: Yeah. May I build upon that? I think as we've been mentioning\nwith the 2.5 series models, these models are predominantly\ntrained to be really good at planning and reasoning,\nwhich, when you think about it, is a key part of what\nmakes an agent work. So the model layer is\nwhere a lot of the planning and reasoning happens. And then, of course,\nthere's the tools layer. We've already talked\na lot about we have a set of first party\nhosted tools from Google, Google Search, Code Execution,\nand URL context, as well as a few other tools. Some of you may have heard\nSundar mention the computer use",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50,
      51
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 15,
    "video_id": "gHHjDRDNUNU",
    "start": 1889.49,
    "end": 2019.5900000000001,
    "text": "We've already talked\na lot about we have a set of first party\nhosted tools from Google, Google Search, Code Execution,\nand URL context, as well as a few other tools. Some of you may have heard\nSundar mention the computer use tool that's coming,\nso we're going to make it available\nthrough the API. We've already rolled it\nout into trusted testers, but that's going to be\npublicly available soon. And then we have a couple of\nother tools on the way as well. LUCIANO MARTINS: All right. SHRESTHA BASU MALLICK: And then\nthere's the orchestration layer, as you were saying, Luciano. LUCIANO MARTINS: Yeah,\nyeah, absolutely. So basically, when we\nare creating an agent, you want to give key\ndirections to these agents. So basically how\nit's going to behave, what's the profile of these\nagents, what's the goal of it. It's going to help\nwith researching, with coding, with learning. Any specific area you are\ntrying to address actions for your users,\nyou need to count with some memory\nfor these agents so you can keep\nthe previous user's interactions with the model. Or you can try to extend\nthis agent memory using any mechanism like RAG or adding\nPDFs or using the wrong context. And also you must\ncount on the model to do the key reasoning\npart of this agent. So how to bring all\nthose stuff together, how to give the best\nanswer to the users, and when and how to\nuse each of the tools that are available\nto this agent. SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: So yeah. So basically, if you\nput all that together, we are looking for\napplications that we are calling like\nmulti-agentic applications or multi-agent applications\nwhere we want some autonomous integration with our tools. We want those agents to take\nactions to help our users. We count on the models like the\nGemini 2.5 Pro, the best one on reasoning capabilities,\nto reason and plan all their actions to\nbe given to this user from helping with shopping\nto generate new codes for a new application. This model must be able to\nbe continuously learning.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 16,
    "video_id": "gHHjDRDNUNU",
    "start": 2003.26,
    "end": 2144.72,
    "text": "on reasoning capabilities,\nto reason and plan all their actions to\nbe given to this user from helping with shopping\nto generate new codes for a new application. This model must be able to\nbe continuously learning. So if you want to append\nmore pieces of information, fresher information, use\ntools like the Google Search grounding, right, Shrestha, to\nbring up-to-date information from Google Search. This model must be\nable to support that. And at last but\nnot least, we count on multi-agent collaboration. So we must be able to not\ncreate huge monolithic agents, but also to keep connecting\nto other specialized agents to bring a better experience. Right? SHRESTHA BASU\nMALLICK: That's right. You've probably heard us talk\nabout all of these components 50 times already in\nthis talk, so I'm going to go through this\nslide really quickly. But one thing we\nwanted to emphasize is when we started\nthinking about how to enable agentic\ncapabilities through our API, we made a conscious\ndecision on first focusing on providing\nhigh quality primitives. And once we had\nmade some progress along that, that's\nwhen we have now started to do things like\nrelease MCP through our SDK, and you'll see some more\nhigher abstractions rolling out in the next couple of months. But in terms of\nagentic primitives, again, we have the 2.5\nseries models, which are thinking first models. You now have Deep Think,\nwhich is an advanced thinking mode on top of 2.5 Pro. And with Flash today\nand Pro soon, you have the ability to set budgets. So you can tell the model when\nto think and how much to think, and that lets you both control\ncost, latency, and whatever is the amount of thinking that's\nsuitable for your application. In terms of API, depending on\nwhat agent you are building, you may want to use different\nversions of the API. If you're building\na research agent, maybe you want to\nuse the Chat API. If you're building\na gaming agent, maybe you want to use the real\ntime API or a customer support agent. Of course, you can build\nresearch agents also",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 17,
    "video_id": "gHHjDRDNUNU",
    "start": 2130.98,
    "end": 2271.49,
    "text": "If you're building\na research agent, maybe you want to\nuse the Chat API. If you're building\na gaming agent, maybe you want to use the real\ntime API or a customer support agent. Of course, you can build\nresearch agents also on the real time API. But in general, you now\nhave these two APIs. And then we've talked\nabout tools a lot. So all I'll say is\ngive us feedback in terms of what are some\nof the other tools that you would see Google make available\nthrough the Gemini API. LUCIANO MARTINS: Yeah. So as you just said,\nthe Gemini 2.5 Pro-- 2.5 models are thinking\ncapable, so they can do more advanced and\nmore complex reasoning. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS:\nAnd maybe we could highlight that now,\nspecifically for the 2.5 Flash, we have the ability of\nusing what we just mentioned, the thinking budgets, where\nyou can calibrate how deep and how much the model\nwill do reasoning cycles. And also for both models,\nwe can have access to what we are calling\nthought summaries. SHRESTHA BASU MALLICK: Yes. LUCIANO MARTINS: So\nbasically you are not only counting on the model\nability to reason, but also you can see on your\nresponse which key steps or which key reasoning\nthoughts the model had to get to our conclusion. So basically, that's a\npretty basic Gemini API code using the Python SDK. For those of you who\nnever used the SDK before, it's pretty straightforward. So three key blocks. You need to import the SDK,\nyou instantiate the client, and then you interact\nwith the model you want. Here we are interacting\nwith the Gemini 2.5 Flash. You send your prompts. And then you have\none specific bit called ThinkingConfig,\nwhere, in this code, you are asking the API\nto include the thought summaries on your\nresponse, and that's to include thoughts\nequals true, and also the amount of tokens you want\nto use during the reasoning process. On your response,\nyou're going to have part of the response will\nbe the thought summaries. So it's separated from the\nfinal answer by intention. If you want to keep\nthose on your backend,",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 18,
    "video_id": "gHHjDRDNUNU",
    "start": 2251.89,
    "end": 2375.4500000000003,
    "text": "the amount of tokens you want\nto use during the reasoning process. On your response,\nyou're going to have part of the response will\nbe the thought summaries. So it's separated from the\nfinal answer by intention. If you want to keep\nthose on your backend, on your data warehouse,\nyour BI environment, you are free to do that. And also you have a second part,\nwhich is the final model answer. That may be, for example, the\npart that goes to the end user. Right, Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: So here,\nwe added two quick GIFs showing how the\nthought summaries work. You have, on the left GIF,\nthe AI Studio experience, and on the right, the\nGoogle Colab experience. And pretty much we ask\nit the same question and you can see how\nyou can interact with the thoughts on\nboth environments. Right, Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: OK. So what about the enhanced\ntooling we have now, Shrestha? SHRESTHA BASU MALLICK: I\nthink in the interest of time, we can skip the slide because\nwe've talked about tools a lot. The one point I\ndo want to make is I mentioned when I was\ntalking about the Live API, that one of the things we\nstarted doing in December itself is we allowed\nyou to chain tool, so use multiple tools\ntogether in the Live API. We're now rolling that out\nto the Chat API as well, starting from Search and\nCode Execution together. But you will see\nmore the ability to do more combinations of\ntools in the Chat API as well. LUCIANO MARTINS: Amazing. So basically that's\nwhat you just said. That's a similar\nexperience with the SDK. Now you have two tools\nincluded on your interaction. You have, in this case, the Code\nExecution tool and the Google Search tool. And then on your answer you can\nsee also on separate structures the Code Execution results,\nincluding the Python code used by the Code Execution, and also\nthe output of this execution. The Google Search grounding\nresults, including the Google Search, real search\nthat was performed,",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 19,
    "video_id": "gHHjDRDNUNU",
    "start": 2361.37,
    "end": 2499.7400000000002,
    "text": "the Code Execution results,\nincluding the Python code used by the Code Execution, and also\nthe output of this execution. The Google Search grounding\nresults, including the Google Search, real search\nthat was performed, and the results of this,\nand your final answer from the model. Right? SHRESTHA BASU\nMALLICK: URL context. As we said, this is a new\ntool that we are rolling out. It's one of the tools\nthat powers Deep Research if you've used Google's research\nagent through the Gemini app. And the idea is\ngiven a set of URLs, we allow you to extract\nmore in-depth content. Of course, in a way\nthat's approved by and respectful to our\npublisher ecosystem. But you can get\nmore content out. And this is really helpful\nfor research and analysis type of use cases. You can use this tool by itself,\nor you can use this in tandem with the Search tool. LUCIANO MARTINS: Yeah. And again, a quick code snippet. You add the two\ncode URL contexts and you can send\ndirectly on the prompts, without further efforts, all\nthe links, up to 20 links, you want to consider\non the model, processing on the\nmodel execution. And you have all the reasoning,\nall the semantic extraction performed by the\nmodel really fast. SHRESTHA BASU MALLICK:\nLooked like some people were still taking pictures. But if you all missed\ntaking an image of a slide, all of this information is\navailable in our documentation. LUCIANO MARTINS: Absolutely. SHRESTHA BASU MALLICK: And\nLuciano and I are also here to answer questions. Function calling. I think we've had-- of\ncourse, function calling is bread and butter for all\nkinds of agentic applications. The Gemini API has supported\nsingle function calling, parallel function calling, and\ncompositional function calling for a while. Compositional meaning where you\ncan put a whole logic around if A happens, then call\nfunction 1, if B happens, then call function 2. As part of I/O, we are also\nreleasing asynchronous function calling through the Live API. So imagine some tasks like you\nstart having a conversation",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 20,
    "video_id": "gHHjDRDNUNU",
    "start": 2484.66,
    "end": 2631.966,
    "text": "if A happens, then call\nfunction 1, if B happens, then call function 2. As part of I/O, we are also\nreleasing asynchronous function calling through the Live API. So imagine some tasks like you\nstart having a conversation with the AI agent,\nbut in the background, you ask it to crank\nout on some other task. And that task may not\nbe a real time task, but it may be something that's\nrelevant to the conversation, like maybe analyzing\na lot of context about the user to\nprovide some answer. You can now turn on an\nasynchronous function. The function will do its\nthing in the background. And then when ready, it will\nnotify you with the results. LUCIANO MARTINS: Amazing. And maybe one of\nthe greatest news for the Gemini\nAPI SDK during I/O is that we brought--\nwe heard you. Many of you gave\nthis feedback for us, and we added to the Gemini\nAPI SDK support for MCP. So now you don't need to have\nseparated and siloed codes with MCP clients,\nMCP servers, and then your Gemini API interactions. You can have it all\ntogether, using the same code base of the Gemini API SDK. SHRESTHA BASU MALLICK:\nVery exciting. [APPLAUSE] LUCIANO MARTINS: Yeah. Thank you. And still talking about\nthose interactions, we know that we have many\nchoices of agent frameworks. You may have heard\nabout the Google ADK, the Agent Development Kits\nlaunched during the Google Cloud Next a few weeks ago. We may have heard about the\nagent-to-agent protocol as well. You have other choices like link\nchain, link graph, everything, and we keep open\ncollaborating with all those internal and external\nopen source efforts to bring the best\nexperience for you all. SHRESTHA BASU MALLICK:\nThat's correct. And especially thanks to folks\nlike Philip Schmidt and Patrick here on our developer\nrelations team. We have been working on building\ncloser relationships, closer interactions, better code\nsamples with some of the leading agent frameworks,\nlike LangChain crew. And so that is also\nsome of the areas where you'll see the\nGemini API show up. LUCIANO MARTINS: Amazing.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 21,
    "video_id": "gHHjDRDNUNU",
    "start": 2610.75,
    "end": 2759.62,
    "text": "We have been working on building\ncloser relationships, closer interactions, better code\nsamples with some of the leading agent frameworks,\nlike LangChain crew. And so that is also\nsome of the areas where you'll see the\nGemini API show up. LUCIANO MARTINS: Amazing. So if you folks are\nconcerned about a lot of-- the amount\nof stuff you just shared today, that's\na huge dump of things. Maybe the best suggestion or the\nbest guidance we could give you is first, as Shrestha\nmentioned, we have the Google\nDeepMind DevRel team distributed across the globe. We have people in Latin\nAmerica, in Europe, in Asia. We have a huge presence here in\nthe US, so please count on us. Connect with us online, on\non-site events like this. And think on those\ntop six actions when you are creating your\nagentic experience from having clear objectives on your mind. Try to laser focus on the\nproblem you need to solve. Do a lot of interactions\non your developments. Do live coding if you\nneed to part on those. Live coding if you need, if\nyou are working with something you are not that familiar. Count with the Gemini\n2.5 models to help you with your coding experience. And always, always focus\non your user experience. That's the best key for success\nyou may have with your tools. Right, Shrestha? SHRESTHA BASU MALLICK:\nThe OG rule of product. LUCIANO MARTINS: Absolutely. So yeah. And that's it. I hope you enjoyed that. Start building now. We brought you here-- [CHEERING] Thank you. We give you some links here. So some of you may\nget the reference. But basically the first link\nis the AI Studio, the UI and no code experience where you\ncan experiment all the Gemini models features really\nfast without writing a single line of code. The second link is\nthe Gemini API docs, where you may find all the\nthings we shared here, and way more of the other features and\npossibilities with the models. And the last one is the Gemini\nCookbook, a GitHub repository that our team curates and keep\nupdating with a lot of sample experience for you. Thank you so much, Shrestha.",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50
    ]
  },
  {
    "schema_version": "chunk_v1",
    "chunk_id": 22,
    "video_id": "gHHjDRDNUNU",
    "start": 2746.57,
    "end": 2767.29,
    "text": "more of the other features and\npossibilities with the models. And the last one is the Gemini\nCookbook, a GitHub repository that our team curates and keep\nupdating with a lot of sample experience for you. Thank you so much, Shrestha. Thank you so much you all. SHRESTHA BASU MALLICK: Thank you\nall for coming out to hear us. [CHEERING] [MUSIC PLAYING]",
    "source_segment_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  }
]
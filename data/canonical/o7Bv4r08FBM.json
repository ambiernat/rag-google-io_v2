[
  {
    "id": "o7Bv4r08FBM__chunk_000",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 0.0,
    "timestamp_end": 152.73999999999998,
    "text": "[MUSIC PLAYING] GUS MARTINS: Hi, everyone. Good morning. I'm very happy to be here. Welcome to my \"very dear\nto my heart\" session, What's New in the Gemmaverse? It's something we really, really\nwork very hard to bring to you. So one question that we get a\nlot is, what is Gemma, right? What's this new thing? We have Gemini,\nbut what's Gemma? Let me give you one step back\nand try to explain to you why we do all that we do. In Google DeepMind,\nwe, of course, try to push AI research\nas much as possible. But the important\nthing about that is-- and we really want\nto solve intelligence. How can we make\nsystems be intelligent? This is a very hard task. It's not something that\nyou can just pull it off. We need everyone to collaborate. And that's why we think\nthat's very important to make our research\navailable to everyone, to make it available\nto developers, to other researchers,\nto regular users, just so that it\nevolves with everyone, and everyone can collaborate. We've been publishing\ngreat work for a while. I guess you've seen AlphaFold,\nAlphaZero, AlphaStar, all of these Alpha-- AlphaEvolve, all of them. That's not the real name. But the idea is we\npush this technology because we want to make sure\nthat everyone benefits from it. And when we wanted to\ncreate an open model, the idea was, first,\nAI for everyone. Make sure that everyone\nhave access to it. And that's why\nbeing open matters. That's why we want everyone\nto be able to try the model, change the model, make it their\nversion, make it their own. That's very important to us. And another very important\nthing that we have in mind is making this as\nresponsible as possible. We know that these models,\nthey bring a lot of power. But we also would like to have\nthe same responsible perspective on it so that we go\nbeyond benefiting people, protecting people from the\nharms that they could cause. So that's the idea of--\nall these models, whenever we train them, we\nkeep responsibility as one of the main\nthings for all of them. And that's how we got to Gemma.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_001",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 137.68,
    "timestamp_end": 279.98,
    "text": "protecting people from the\nharms that they could cause. So that's the idea of--\nall these models, whenever we train them, we\nkeep responsibility as one of the main\nthings for all of them. And that's how we got to Gemma. Gemma is our family\nof open models. They are based on\nGemini technology. And they are available for\nyou in many flavors and sizes. I'm going to go into\nsome details soon. We launched them last year. And we've just completed\na one-year anniversary. It was a beautiful journey. Last year, we\npresented them here. So if you know me, you've seen\nme talk about them for a while now. When we released Gemma\n1, we had two sizes, which was a 2B and a 7B. But what does that mean? So when we say \"a model,\" a\nmodel is basically a very long math equation, right? Very long. And for each a\ntimes x, b times x-- for each one of these a, b, c,\nwe would say that's a parameter. When we say 2B, we say that\nthere are 2 billion parameters. So it's like a\ngigantic math equation, with 2 billion parameters. That's like a layman\nversion of what it is. Don't tell this to a researcher. But the idea is that 2B\nis a very large model, but it's kind of small\ncompared to other models. And we release also a 7B. And people really,\nreally like them. And since then, Gemma\nhas been downloaded over 150 million times. I guess we're close to 160,\nsince we created this slide. So things are moving super fast. We are very, very happy\nwith the results so far. Since our launch, we released\nmany variants ourselves. We created multiple versions. So for example, there's\na bunch of names here. Let me go a little bit--\nfor example, PaliGemma. PaliGemma was a model we\nreleased last year at Google I/O, right here, which\nis the first Gemma model that has vision. You could give an image\nand ask a question. And it was the best VLM,\nVisual Language Model, so far. It was great. That's why we did a PaliGemma\n2 in the end of the year, so we kept pushing. We also had other\nvariants, like ShieldGemma, which I'll talk\na little bit more",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_002",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 264.95,
    "timestamp_end": 407.81,
    "text": "And it was the best VLM,\nVisual Language Model, so far. It was great. That's why we did a PaliGemma\n2 in the end of the year, so we kept pushing. We also had other\nvariants, like ShieldGemma, which I'll talk\na little bit more later, which the idea\nis a safety classifier. So you can do-- I'll explain the details\nlater, but the idea is to make your AI systems even\nmore responsible as a whole. Aside from the regular\nvariants, we also published more core\nmodels, as we say. When we say core models,\nwe release Gemma 1, and then we release\nGemma 2, and then we release Gemma 3,\nbeginning of this year. And yesterday, we released\nGemma 3n that I'm going to talk a little bit more,\nin a little bit. And more than tools, we\nalso released some tools to go around our models. For example, we have\nGemma Scope and Gemma.cpp. Gemma Scope is some\ntooling for you to see inside of all\nthe layers of the models and help you interpret,\nwhy some results? You give a prompt. Why you have this result? As I said, these\nmodels are quite large. So understanding how you go\nfrom \"Luna loves strawberries\" to some emojis. It might be very\nhard to explain that. So Gemma Scope helps with that. Gemma.cpp is our implementation\nof our Gemma models completely on C++. Then you would say, oh,\nyeah, why would you do that? When we do that, it enables\nyou to run these models on CPU only and super fast. So if you have a computer that\ndoesn't have a strong GPU, which I guess the majority\nof people have that, you can still use these\nmodels only on CPU, with a very good performance. And the fun thing\nis, in March, we released Gemma 3, our\nmost capable model so far. We had four sizes-- 1B, 4B, 12B, and 27B. And they were all state\nof the art, which I'll go into details in a little bit. But the idea was, we've learned\na lot with the community. We've received a\nlot of feedback. We've learned from our own--\nfrom researchers, from everyone, and we want to make the\nbest models possible. So what came with Gemma 3? First of all, community\nreally liked for the models",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_003",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 389.85,
    "timestamp_end": 525.89,
    "text": "We've received a\nlot of feedback. We've learned from our own--\nfrom researchers, from everyone, and we want to make the\nbest models possible. So what came with Gemma 3? First of all, community\nreally liked for the models to see beyond just text. So, of course, Gemma\n3 is multi-modal. There's 4, 12, and 27B. They can all receive an image,\nand they can do more than that. They can do a conversation. So you can ask a question\nabout a meme-- an image-- and you can continue with\na multi-turn conversation. You can add more images. You can do all of that. That's part of the\nbase model already. More than that, one thing for\nGemma was always it was English only. But we got feedback from the\ncommunity for two things-- the model could generate\ntexts in other languages, and we were very\nsurprised, which was-- they had very high\nquality in other languages. But we always said,\nit's English only. So what we did is we, OK, let's\nmake it really multilingual. So Gemma 3 now supports\nover 140 languages. And it's quite good in many\nof them in the benchmarks, in all the ones that\nwe could run at least. So this is a very\nstrong capability. I guess, I don't know-- let me tell you a\nlittle secret here. I don't know if you\nnoticed, English is not my first language. Yeah. Can you believe that? [LAUGHTER] So for me, having a\nmodel that can really help in multiple languages\nand multiple cultures, it's really, really\nimportant, dear to my heart. As you can see, there's\na lot of users of Gemma that are not from US,\nand they don't speak English as their main language. Maybe I'm part of\nthat 64% over there. So the idea of having a model\nthat embraces multiple languages and cultures, part\nof what we want to make in terms\nof accessibility, right, that we want\neveryone to be part of it. And the other thing that\nwe improved for Gemma 3 is we made the\ncontext window longer. What is a context window? When you write a prompt\nto these models, this text you're sending, let's say\nthat's the context window.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_004",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 510.55,
    "timestamp_end": 656.32,
    "text": "And the other thing that\nwe improved for Gemma 3 is we made the\ncontext window longer. What is a context window? When you write a prompt\nto these models, this text you're sending, let's say\nthat's the context window. So the larger the context window\nof a model, the more information you can give per prompt\nand the longer you can do a conversation. The other important thing\nis, when you add an image-- for example, you give an image\nand ask, describe this image-- all of that eats\nup-- let's say, like, that's your context window. So the longer or the larger\ncontext window you have, the more your model can process. And you can also have\nlonger conversations. I guess 128, you can put a book. You can put multiple-- I guess 500 images. You can do a lot of stuff. You can put all these--\nyou can put a video and ask questions\nabout the video. So it's very, very powerful. And before we were at 8k. So the community complained. We improved that,\nas you can see here. So we were from 8k to-- the\n1B has 32k context window. And the 4-- it's 12 there,\nit shouldn't be 14-- 27B have 128k context\nwindow, which is great. The 1B has a shorter context\nwindow because it's aimed for smaller devices. One other aspect\nof context windows, of course, there is no magic. If you give a larger, more\nprompt, it eats up memory. So if you're using a 1B model,\nwe expect you to be using a weaker machine so you will\nhave less memory anyway. Otherwise, you'll go for\nthe 4B, which is stronger. Now, here's a fun graph of how\nthe models keep the quality, while we increase\nthe context window. And we say that the context\nwindow for our models is 128k. You can see it\nover there, right? But if you look at the green\none, the green is the 27B, the lowest line over there. If you try a 512k, it will\nstill do good results. But, of course, we\nbenchmark until 128. That's what we say it's good. But it can go a\nlittle bit further. [CHUCKLES] Let's see some\nexamples of how the model works. Here, I'm giving an image. And I'm asking, how\nmuch is this ticket?",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_005",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 639.32,
    "timestamp_end": 776.84,
    "text": "But, of course, we\nbenchmark until 128. That's what we say it's good. But it can go a\nlittle bit further. [CHUCKLES] Let's see some\nexamples of how the model works. Here, I'm giving an image. And I'm asking, how\nmuch is this ticket? And the answer to\nthe right, you can see there that, first\nof all, the model can not understand what's\nthis and extract the price, which is in Czech Koruna-- which I've never\nused that currency. Sorry if I'm saying wrong. But the model can see and\nunderstand and parse that. And then the fun thing is we\ncan follow up with a question. How long is this ticket valid? Of course, you just read\nwhat's written there. But did you see that\nin the ticket before? Could you answer that yourself? Can I go back one slide, please? Can you go back\none slide, please? Yes. Can you see there, how\nlong is this ticket? Even for me, when I was looking\nfor the first time, I was like, where does this\ninformation come from? So on top left, the\ninformation is there, right? And the model knows that. And that's what it says there. So it lasts for 24 hours. So the model has all these, and\nyou can keep the conversation. Another cool thing\nthat the model can do is, of course, every\nfeature that's a capability, I told you, they\nall work together. It's not one or the other. They all work together. So on this example, can\nyou translate this picture? I'm using the vision capability,\nthe text understanding, of course, and the multilingual\nbecause that sign has at least four or five languages. Four, right? And it translates from all of\nthem, without any extra process. It just knows all of them,\nextracts the information, and gives you context\nof where the sign is. And this is an example, when\nI saw for the first time, I was like, well,\nthis is very strong. I didn't expect that to\nhappen to work so well. And one important aspect\nfor us is, as I said, the feedback you give. You give us a lot of feedback. Sometimes they're harsh, but\nI like the feedback anyway. Thank you very much. And the idea is, why\ndoes that matter?",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_006",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 764.11,
    "timestamp_end": 901.24,
    "text": "And one important aspect\nfor us is, as I said, the feedback you give. You give us a lot of feedback. Sometimes they're harsh, but\nI like the feedback anyway. Thank you very much. And the idea is, why\ndoes that matter? It's because we want you all\nto succeed with us so Gemma can really help you\non your applications, on your systems, whatever it is. So your feedback\nis very important. So my colleague here that\nwill present after me asked, what else-- what do\nyou want from Gemma? We asked openly. We got a lot of replies. Of course people said, we\nwant larger context window. We fixed that. We want smaller models. Remember, I told you that\nthe first model was 2 and 7. And then we released-- Gemma 2 was 2, 9, 27. People wanted smaller models. We did the 1B model, and all\nmodels are now quantized. We release the\nquantized versions that are smaller models, but\nthey still keep the quality. So the 27B, it's like-- I'll be very honest. We have the 27B. It's great, super\npowerful, blah, blah. But the 12B, which is like the\nmiddle son, let's say like that, it's amazing. The quality is so strong, and\nyou don't need too much hardware to run that. That's amazing. Can we have more modalities? Yes, we added vision so\nyou can do image and video. We also added more languages. As I told you, 140 languages. Community loved that. And we need system prompts. OK, we got there. Although, to be\nhonest, people want system prompts is\nlike system prompt is, before the\nconversation, you tell the model how it should behave\nand put some ground rules. We don't have a\nsystem prompt role, but if you add what you\nwant to the first message, the model will follow that. The Gemma models are very strong\nat following instructions. So whatever you say, do x, y,\nand z, it will do x, y, and z. So if you say,\ntalk like a pirate, the most basic system prompt,\nit will talk like a pirate during the conversation. So we don't have\na role for system prompts, but you can add to the\nfirst message, and it will work. You can add at any moment\nduring a conversation,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_007",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 888.38,
    "timestamp_end": 1038.17,
    "text": "the most basic system prompt,\nit will talk like a pirate during the conversation. So we don't have\na role for system prompts, but you can add to the\nfirst message, and it will work. You can add at any moment\nduring a conversation, and it will, from there,\ncontinue with your instructions. And people love also the\npersonality of Gemma. Personality is how\nthe model replies. When you ask a question,\nthe way it answers. People like its warm-- it has a strong personality. It's approachable. And that's why we have\na LMArena score so high. And as you can see here-- I guess those\nnumbers are updated. Those are the LM scores for our\nthree larger models, the 27 12, and 4B. And what's so great about this? First of all, this\nis from LMArena. This is a benchmark that people\ngo and blind vote on prompts. So you have a prompt. It gives you two answers, and\nyou say which one you prefer. So this is from the voice of\nthe people, let's say like that. There are, of course, criticism. But this is one of the\nbest benchmarks we have. It's people's choice. On this benchmark, you can see\nthe 27B is one of the top ones. And it's an open--\nremember, open model. Anyone can download\nand use right now. We are one of the best\nopen models, period. And the other thing is our\n27B needs 1 GPU to run, while the other open models that\nare large-- that are better, let's say like that, they\nneed, like, 30 GPUs, 10 GPUs. Do you have 10 GPUs to run now? Do you have access? So that's one of the\nimportant things. You can see the small\ndots over there. And you can see the\n12B, the middle kid. The 12B is really close. It's super strong. And you can see\nthe 4B over there. I'm not going to\nsay who is after. That doesn't matter. But the idea is that our 4B is\nreally, really strong for 4B, 4 billion. Can run basically\nmost of your devices. I'm not saying phones. And here, we see\nour capabilities in terms of specific languages. So if you look for\nGemma over there, it's right there at the bottom. You can see we were top\nin French, top in Spanish,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_008",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1021.19,
    "timestamp_end": 1181.07,
    "text": "I'm not saying phones. And here, we see\nour capabilities in terms of specific languages. So if you look for\nGemma over there, it's right there at the bottom. You can see we were top\nin French, top in Spanish, second in Japanese,\nthird in German. It's great all around\nthe multiple languages. Again, open model, right? This is very, very strong. And as of yesterday,\nwe released Gemma 3n, our newest model that can run on\nas little as 2 gigabytes of RAM. Gemma 3n has a lot\nof development. I'm very impressed by\nthe research behind it. And it's optimized\nfor on-device. So the architecture is\nfocused on making own device, or on small devices,\nit to run faster. It's the same model. It's the base model\nthat will be also used for Gemini Nano,\nwhich comes embedded in many, many Android\ndevices already. So it's the same architecture. So we're sharing that\nas open models now. We expanded the capabilities. Gemma 3 has vision,\ntext, and video, right? Gemma 3n, we added\naudio understanding. So now you can talk\ndirectly to the model. And this is very,\nvery interesting. This is very new. Oh, let me run because\nI have my friend. And we are releasing two sizes-- 4B and 2B. Today, you can test these models\nalready on Google AI Studio. Let me show you this video. [VIDEO PLAYBACK] - Can you tell me\nabout this replica? - Certainly. This is a replica of a\nVirgin Galactic SpaceShipOne, a suborbital spacecraft. It's a model of the vehicle used\nfor testing and demonstration flights. - Can you please save\nthis to my notes? - Absolutely. Creating a note. GUS MARTINS: This is\nrunning completely local. There's zero internet, and it\nwas just saved to Google Keep. - What can I add to the\nsystem to make it faster? - Interesting question. Adding caching layers can\nsignificantly improve speed. Consider implementing\na CDN or caching proxy to store\nfrequently-accessed content. - Write a super short poem\nthat highlights her skills. - Amazing Zoey, so bright. Counting to 100, what a sight. - What is the total square\nfootage of the largest room?",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_009",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1160.47,
    "timestamp_end": 1302.5800000000002,
    "text": "to store\nfrequently-accessed content. - Write a super short poem\nthat highlights her skills. - Amazing Zoey, so bright. Counting to 100, what a sight. - What is the total square\nfootage of the largest room? - The largest room\nis the living room. Its dimensions are\n18 feet by 12 feet. The area is 18 times 12,\nwhich is 216 square feet. - Can you guess which\nfamous figure is this? - This is a cartoon\ndrawing of Einstein. The wavy hair, the mustache,\nand the mischievous grin are all telltale signs. [END PLAYBACK] GUS MARTINS: Nice. Thank you. Thank you. [APPLAUSE] This is all running\ncompletely on-device, of course in an\ninternal app we have. It's not available\nto everyone to test. But this is the capability\nthat Gemma 3n offers. And all of it-- on-device,\nyou ask, can the model speak? No, that's using text to\nspeech from the Android phone, the native one\nthat everyone has. But that's cool. But how do you\nbuild using Gemma? How do you take these\nmodels and make your own? For that, I would like\nto call friend Omar to continue and explain\nto you all the details. Thank you. [APPLAUSE] Thank you. [CHEERING] OMAR SANSEVIERO:\nThank you, guys. So I'm very excited to be\nhere to talk about Gemma, the fantastic\nopensource ecosystem, and how to build products\nwith the Gemma models. As Gus mentioned, we have been\ntalking with the community. We have been talking with\nstartups, with researchers, enterprises, companies,\ndevelopers, and as part of that, we want to make Gemma as\neasy as possible to use. We want to make Gemma very\naccessible to the developer ecosystem. So the easiest way to get\nstarted is with AI Studio. So if you go to AI Studio\nat ai.dev, at the right, you can pick among\nmany different models. And within those, of course,\nyou can try out Gemma. So you can test your prompts. You can see if the Gemma models\nwork for your own use case. There are different model sizes. And depending on\nwhat you want to do, different model sizes may work. Just a few weeks\nago, we released also Image Input in AI Studio.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_010",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1288.3,
    "timestamp_end": 1409.02,
    "text": "You can see if the Gemma models\nwork for your own use case. There are different model sizes. And depending on\nwhat you want to do, different model sizes may work. Just a few weeks\nago, we released also Image Input in AI Studio. So you can try out an image. So for example,\nhere, we uploaded an image of a croissant. And you can ask,\ndescribe the image, and then the model will give\nyou a description of the image input. So let's say that you\ngo to your studio. You try out Gemma. The model works well\nfor you, and you want to deploy this in production. So in the same interface,\nthere is a new button, when you have Gemma selected,\nthat says Deploy in Google Cloud Run. Sorry, you have a button\nthat says Export the Code. And it will give\nyou a code snippet in Python, or in\nTypeScript, or in Rest, or in Apps Script, whichever is\nyour favorite language, which you can use to run and build\nyour own applications using Gemma. This is using the Google APIs. The code is actually\nvery simple. You just, first,\ninstantiate a client. You specify the token,\nthe Gemini API token. Then you pass the\ntext, the content. You can also pass images. And then on the right side,\nyou will create a text. You will generate a text. So you pass the name of the\nmodel, so in this case, Gemma 3, the 27-billion-parameter\nversion. You pass the text, and you\npass the content configuration. So a new feature in\nAI Studio is that when you have a Gemma model\nselected, there's this new button called\nDeploy in Cloud Run, that I was mentioning\nbefore, that allows you to create an endpoint\nthat will automatically scale. So if you wanted to\nscale to 100 GPUs, or if you wanted to scale down\nto 0 when there is no usage, Google Cloud Run will\ntake care of all of it. And the very nice thing\nof this is that you don't need to change the code. There are just two\nthings-- two lines of code, if you saw the details,\nthat you need to change. You need to change the URL, and\nyou need to change the token. So you just change\ntwo lines of code,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_011",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1398.0,
    "timestamp_end": 1517.53,
    "text": "don't need to change the code. There are just two\nthings-- two lines of code, if you saw the details,\nthat you need to change. You need to change the URL, and\nyou need to change the token. So you just change\ntwo lines of code, but the rest is exactly\nthe same code as before. But that's not the\nonly way to use Gemma. Gemma is designed to\nbe easily integrated into your favorite\nopensource tools. So of course, there\nis Google AI Studio, but we also support Gemma\nin Keras, or in vLLM, or in JAX, or Ollama, or Hugging\nFace Transformers, PyTorch, MediaPipe. So there are many\ndifferent opensource tools that you have probably used. And the idea is to make Gemma\nas easy as possible to user, to enable developers to\nrun the models locally. Because that's the cool\nthing of open models. You don't need an\nAPI necessarily. You can run these models in your\nown laptops, in your own phones. And we'll talk a\nbit more about that. But this is a good\nopportunity for me to talk a bit about Kaggle. Kaggle is the platform, where AI\nbuilders, learners, developers, researchers participate\nin AI challenges. There are over 24 million\npeople participating here. And here, you can\ndiscover models. You can share knowledge. And within those, of course,\nyou can find the Gemma models. You can find the\nGemma models, read the model card, which is like\na documentation of a model. You can download\nthe checkpoints. OK. Let's say that now you want\nto go and download and run Gemma in your computer. Who here knows Ollama? OK, many of you. That's great. So Ollama is a tool that is\ndesigned to very easily, very seamlessly run Gemma models-- well, open models locally\nin your own computer. So with a simple command,\nollama run gemma3, it will download and\ncreate a local API, or a local chat interface\nthat you can use to experiment with the Gemma models. The nice thing of Ollama is\nthat it's very well integrated into many opensource tools. So if you use LLaMA\nindex, for example, you can consume ollama-- you can use ollama to consume\ndifferent open models.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_012",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1503.94,
    "timestamp_end": 1627.45,
    "text": "The nice thing of Ollama is\nthat it's very well integrated into many opensource tools. So if you use LLaMA\nindex, for example, you can consume ollama-- you can use ollama to consume\ndifferent open models. Or same for langchain. So if you have\nused langchain, you can combine langchain and\nollama to very easily use-- to use open models,\nsuch as Gemma 3. Ollama, by the way, is also\nintegrated in Google Cloud Run. So you can very easily\ndeploy a model with Ollama through Google Cloud Run. Ollama is just one of\nmany different open tools to run open models. So another one very\npopular is Keras. Keras provides\nhigh-level APIs that are designed for very quickly\nprototyping and building with models. You can build models. You can experiment. You can train very easily. It gives you very nice,\nsimple abstractions that let you try out\nthe Gemma capabilities and build products with it. So here you can see the code. First, we load the Gemma 3\n4-billion-parameter model with a specific precision. So I'll talk a bit\nmore about precisions in a couple of minutes. But in this case, we\nare using bfloat16. The second part is parsing\nthe actual text to the model. And you may see there that\nthere are some weird words-- start_of_turn, end_of_turn. What's that? So when you go and\nyou train a model, you use a specific format. So when you train a\nmodel with conversations, you use a specific format. And the idea is that when you\nactually want to run a model that was trained-- so if you\ndownload Gemma 3, for example-- you want to use the same\nprompt structure, the same text structure as what was\nused to train the model. So some libraries,\nsuch as Ollama, take care of doing\nall of this for you, while other libraries,\nsuch as Keras, give you more fine-grained\ncontrol on the prompts and allow you to\ncontrol how things work. Just a couple of months ago,\nwith the Gemma 3 launch, we also announced\nGemma Python Library. It's powered by JAX. It also gives you\nhigh-level abstractions that let you run the Gemma models. And it has three key components.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_013",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1615.56,
    "timestamp_end": 1743.99,
    "text": "Just a couple of months ago,\nwith the Gemma 3 launch, we also announced\nGemma Python Library. It's powered by JAX. It also gives you\nhigh-level abstractions that let you run the Gemma models. And it has three key components. It has a tokenizer. It has the model, the model\nconfiguration, let's say. And it has the actual\nmodel parameters. And this JAX library gives\nyou this sampler class that lets you combine the\nmodel, the parameters, and the tokenizer\ninto a single class. And then when you do\n\"sampler.sample Roses are red,\" it will keep\ngenerating the text. Finally, if you\nknow Hugging Face, they have this library\ncalled Transformers, which gives you a\nvery high level, also a pipeline as a\nclass, that lets you say, I want to do text\ngeneration, with a Gemma 3 12-billion-parameter model. And just like that, you can\nspecify, OK, the user is saying, who are you? Answer in pirate speak. You pass that to the model, and\nthe model will keep generating. So as you can see, Gemma\nis deeply integrated into many of your\nfavorite opensource tools. And we are talking with\nall of these developers to make it as easy as\npossible to run them. Now, these models are\nrunning in your computer. And as Gus was\nmentioning, we have models that go from\n1 billion parameters to 27 billion parameters. And you may be\nwondering, can I run this model in my own computer? So each parameter is stored with\na number, a specific precision. So previously, I\nmentioned bfloat16. That means that there are 16\nbits for every single parameter. So kind of rough napkin\nmath-- so this is just an approximation-- is that for\nevery single billion parameters, you need 2 gigabytes of VRAM. So you need a GPU\nof 2 gigabytes. So the 27-billion-parameter\nmodel, it's 54 gigabytes. And probably most of you\ndon't have that at home. So now you may be wondering,\ncan I actually use these models? So there is a family\nof techniques called \"quantization.\" Quantization reduces the\nprecision of the parameters. So you can go down to 8 bits per\nparameter, 4 bits, even 2 bits.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_014",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1729.78,
    "timestamp_end": 1851.198,
    "text": "So now you may be wondering,\ncan I actually use these models? So there is a family\nof techniques called \"quantization.\" Quantization reduces the\nprecision of the parameters. So you can go down to 8 bits per\nparameter, 4 bits, even 2 bits. And as we reduce the\nprecision, the quality begins to degrade a bit. So the models may become\nworse than the bfloat16. So you may be\nwondering, OK, can I reduce the precision while\nmaintaining the same quality? And this is a good moment\njust to show, with int4, so if we quantize the model, now\nthe 27-billion parameter just needs 14 gigabytes of VRAM,\nwhich is quite good, actually. I personally really like the\n12-billion parameter model. It's a very nice size. Just 6.6 gigabytes\nof VRAM to load a model, which is a size for\nmost normal computer GPUs. So it's a very friendly size. A few weeks ago, we released\na QAT, Quantization-Aware Training, checkpoints of Gemma. QAT is a technique that allows\nyou to make the model more robust to quantization. It's a very small\nadditional training stage. And what that means is that\nonce you quantize the model down to int4, the model will\npreserve the same quality as a larger model. So it will be a bit worse, but\nyou don't have so much quality degradation. So now with the\nGemma 3 models, you can also try out\nthe QAT checkpoints that are specifically\naimed to make it very easy to run the models locally. So I've been talking\na lot about how to run and how to load\nthe Gemma models. And now I would love to talk a\nbit more about the Gemmaverse, which is composed of tens\nof thousands of variants, tools, libraries built by\nyou, the developer ecosystem, and also researchers. It's a vibrant ecosystem\nwith huge adoption. And just to understand\nit a bit more, it's good to talk about\npretraining and post-training. So in LLMs, there are\nusually two main stages. In the pretraining, you usually\ntrain a model from scratch. That needs lots of\ndata, lots of compute. And the outcome of that stage\nis that it's a model that will predict the next token.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_015",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1835.89,
    "timestamp_end": 1948.44,
    "text": "So in LLMs, there are\nusually two main stages. In the pretraining, you usually\ntrain a model from scratch. That needs lots of\ndata, lots of compute. And the outcome of that stage\nis that it's a model that will predict the next token. It's not the one that you\nwould put in a chat interface or in a product. There is also the\npost-training stage. So if you have heard about RLHF,\nQLoRA, PEFT for finetuning, that's post-training. And the idea is to make the\nmodel better conversational. So I'll talk a bit more\nabout that in a minute. But for Gemma, we released\nboth the pretrained and the post-trained\ncheckpoints. The pretrained checkpoints\nwork as a foundation for the community to\nbuild their own models with their favorite\nfinetuning tools. And the post-training checkpoint\nis a generalist conversational model. But there are many cases\nin which you will want to fine tune your own models. So when will you\nwant to do that? If you want to add\nnew capabilities; if you want to make the model\nbetter for Spanish, for example, so for your own\nlanguages; if you want to make the model\nbetter for your own domain, so if you have your\nown company data and you want the model to\nlearn about your company data, you can fine-tune. And these are\nmodels that you own, that you can host in your own\ncomputers, in your own servers. So you already saw\nthe LMArena score. So what you see here is\nactually the chat model. It's the post-train checkpoint,\nwhich is a generalist conversational model. But again, you can fine-tune\nGemma for your own use cases. So if have your own\ninternal leaderboards for your own use cases, this is\nwhere finetuning will come in. And there are many\ndifferent finetuning tools. So again, I mentioned already\nKeras and Hugging Face. There is Unsloth\nand Axolotl, which are two other very\npopular opensource libraries to fine-tune models. I will not dive too much into\nthe code for the sake of time. But with Transformers,\nfor example, there is trl, which is a\nhigh-level wrapper that",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_016",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1936.56,
    "timestamp_end": 2061.52,
    "text": "are two other very\npopular opensource libraries to fine-tune models. I will not dive too much into\nthe code for the sake of time. But with Transformers,\nfor example, there is trl, which is a\nhigh-level wrapper that allows you to very easily,\njust with a SFTTrainer, do the training of a model. Or Unsloth. Unsloth is a wrapper around\nHugging Face Transformers that makes it very, very, very\neasy to fine-tune models with very limited compute. So in a free Google Colab,\nin a Kaggle instance, for example, you can use\nUnsloth to train models. And this is extremely quickly,\nwithout losing any quality. So it's a very strong\ntechnique to do finetuning. And there are over 70,000\nGemma-based models built by the community. The top three are\nmultilingual examples. So we have seen people training\nGemma for Korean conversations. We have seen GemmaX, which\nis Gemma for translation. We have seen SILMA Kashif,\nwhich is a retrieval augmented generation for\nArabic and English. But we have also\nseen people using Gemma to change the way\nthey develop models, that they build models. So Lumina Image\nis a way in which they combine Gemma\nwith a few other models to do image generation. Or OmniAudio, which\nis a model that can also take audio as input. Or finally, SimPO, which is a\ndifferent post-train technique. It's a different RLHF\ntechnique based on Gemma. So we are seeing people\nbuilding research, also, on top of the open\nmodels that we release. It's been fantastic to see\nwhat the community built. Just a couple of months ago, we\ndid a Kaggle competition, focused on enabling\nthe community to train Gemma for their own languages. And we saw thousands of people\nparticipating, contributing, sharing knowledge,\ncollaborating. So this was very exciting. And now I'm happy\nto hand it over again to Gus, who will\ntalk a bit more about some of his favorite case studies. Thanks. [APPLAUSE] GUS MARTINS: Thank you. Nice. Yeah, you thought you didn't\nhave me anymore, right? So let's talk about how\npeople are leveraging Gemma even further.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_017",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 2041.86,
    "timestamp_end": 2186.4599999999996,
    "text": "again to Gus, who will\ntalk a bit more about some of his favorite case studies. Thanks. [APPLAUSE] GUS MARTINS: Thank you. Nice. Yeah, you thought you didn't\nhave me anymore, right? So let's talk about how\npeople are leveraging Gemma even further. We've seen lots of variants\nthat Omar just showed, but we have some\nvery cool use cases. For example, we have our\nfriends from AI Singapore. They built the SEA-LION model. And the idea is they\nwanted to create a Gemma version\nthat was very good on Southeast Asian languages. There's a bunch of those. And Gemma was good. There are many models\nthat try to do that. Our model was good. But what they did is\nthey finetuned Gemma, a Gemma 2 model, a 9B, to be the\nbest Southeast Asian language model, period. And that's their scores\nthere you can see. And we've been working with them\nto make even further updates to that and enable\na lot of people to have access, in\ntheir own language, to a large language model. Beyond that, we have our\ncolleagues on Insait. And they built the first\nnational Bulgarian LLM, let's say like that. And what they did is they\nfinetuned also a Gemma 2 model-- again, a model that\nwas English only, to be the best\nBulgarian LLM, period. And it's offered to\ncitizens in Bulgaria so every citizen can use. It's used by the government. So this is exactly what we want. We want to enable people to\nbring this to their communities, to their citizens, which\nis very, very cool, and giving this\npower to everyone, making it accessible\nto everyone. So we're very proud\nof these use cases. And in terms of\nvariants, there's, of course, the community ones. We are absurdly happy with them. But we also keep pushing our own\nvariants, going into new fields. I don't know if you saw. Last month, we released--\nwe announced DolphinGemma, which is our first large\nlanguage model for dolphin communication. The idea is, of\ncourse, to create-- with this model, we create a\nshared vocabulary with dolphins. And then we can, at\nsome point, start to understand how\nto talk to them.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_018",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 2170.59,
    "timestamp_end": 2315.26,
    "text": "which is our first large\nlanguage model for dolphin communication. The idea is, of\ncourse, to create-- with this model, we create a\nshared vocabulary with dolphins. And then we can, at\nsome point, start to understand how\nto talk to them. We are not there yet. I don't want to sell hype. That's not what I do. This model is helping\nresearchers get there. And we are using decades\nof field research data to train models together. This is very cool. It's coming soon. We are going to open that\nfor everyone to play. Maybe you do a version\nfor whales or for dogs. I don't know. Up to you. So this is great. It's coming soon. We also released,\nyesterday, MedGemma, which is a Gemma variant\nfor the health industry. The idea here is you can-- Gemma is good on visual and all. But when you start to use\nmedical vocabulary or images from health-related\nfield, of course Gemma is not great on that. It wasn't trained to do that. That's what MedGemma\nhelps you address. They fine-tune Gemma 3 to\nbe great for medical use cases, both image and text. The 4B and the 27B, these are\navailable on Hugging Face, Vertex, and we are bringing\nit to even more surfaces. So this is very good for you to\nstart playing on health apps. And why does this matter? When we talk about\nhealth-related applications, this is a field where\nyou cannot just, oh, let me send your data\nto any cloud, anywhere. Let me do that. You cannot do that. There's a lot of regulations. So running completely local on\nyour computer, on your server, that's very important. And MedGemma enables that. Gemma enables that. MedGemma enables that. We also mentioned\nyesterday, SignGemma, where we're expanding\nGemma to sign language. So you'll be able to do a\nsign-- this is not a real sign. But you'll be able to do\nsign language with the model. It will translate that to text. At first, it will be American\nSign language and English, but we are working\nwith the community to make this in as many\nsign languages as possible. This is coming soon-- hopefully really, really soon.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_019",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 2300.77,
    "timestamp_end": 2426.7999999999997,
    "text": "It will translate that to text. At first, it will be American\nSign language and English, but we are working\nwith the community to make this in as many\nsign languages as possible. This is coming soon-- hopefully really, really soon. And this is also\nanother field we're expanding with our variants\nto help, again, engage the community,\nreach more people, help more people benefit\nfrom these kind of tools. Another variant that I'm\nvery proud of is ShieldGemma. ShieldGemma, there's 1 and 2. ShieldGemma 2 specifically\nwas released in March. It's a model as a\nsafety classifier, but Gemma 2 is\nspecialized for image. So you give an\nimage and a prompt, and it can classify the\nsafety of that image based on your prompt. And it's based on\nthe Gemma 3 4B, which is already very strong. And how would you use that? You would put on a system that\nyou have an image input or text, and then you put your Gemma to-- you could use that\nto protect your LLM. That's after. You could put your Gemma after\nyour model generates an image, for example. And this is how you do a-- the safety classifier\nwas built, right? You have a policy,\nimages, and you train it. So Gemma works in\nmany, many places, helping your system become\nmore responsible, safe, and all like that. And this is all great and all,\nbut what I really want to know is what you are\ngoing to build next. Please reach out to us. We are all on social, or we will\nbe there on the Q&A after this. I would love to\nunderstand how you're using Gemma, how you\nwant to use Gemma, if Gemma can help\nyou on your business. We've been talking\nto the community. We know how important it is. Well, a lot of people know me\nfrom talking to the community because I really think that\nwe have to work all together. Your opinion is\nextremely important. So thank you very much. [MUSIC PLAYING] [APPLAUSE]",
    "source": "youtube",
    "speaker": "unknown"
  }
]
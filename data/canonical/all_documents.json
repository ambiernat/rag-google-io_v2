[
  {
    "id": "gHHjDRDNUNU__chunk_000",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 0.0,
    "timestamp_end": 146.11,
    "text": "[MUSIC PLAYING] [CHEERING] [AUDIENCE SCREAMING] LUCIANO MARTINS: Hey, folks. Good morning. It's a pleasure to\nbe here with you all. I'm Luciano Martins. I'm Brazilian. [CHEERING] I'm an AI Developer\nAdvocate at Google DeepMind. And I'm here with\nmy friend Shrestha. SHRESTHA BASU\nMALLICK: Thank you. Luciano Hi, everyone. I am Shreshta Basu Mallick. I'm the Product Lead for\nthe Gemini Developer API. And it looks like I should have\nbrought an Indian contingent here. [LAUGHING] Thank you. LUCIANO MARTINS: Yay! OK, so the idea of\nthis conversation is we want to share with you\nsome of the new things you have available to develop your\nsolutions using Gemini models and the Gemini API. How many developers\nyou have here? Amazing. OK, so we can start talking\nabout the Gemini models universe. We started Gemini\nby the end of 2023. And since then, we\nhave done a lot of work together between many different\nGoogle DeepMind teams. And by now, we are very\nproud of the point we got in and all the stuff we\nlaunched at during I/O. Just doing a quick recap,\none of the key differentials of the Gemini is that it is\nmulti-modal from scratch. Since when we started\ndeveloping the model, it always handled and\nunderstood multi-modal data. It means that you\ncan work with Gemini with any format\nof information you want, from text, image, video,\naudio, code, anything, right Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. And just a quick\noverview of what are all the types\nof models, and what are all the families of\nmodels we have available. So what we're going\nto do today is Luciano and I split this\ntalk into two parts. So the first part\nof the talk will be talking about the models. And then the second\npart of the talk will be talking about the API. What are the capabilities\nand functionalities available through the API? And as part of that,\nwe have a section where we do a deep dive on\nagentic capabilities in the API. LUCIANO MARTINS:\nThanks for the add. SHRESTHA BASU MALLICK: Yeah. So starting with what are\nthe families of models we have available?",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_001",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 132.94,
    "timestamp_end": 265.63,
    "text": "And as part of that,\nwe have a section where we do a deep dive on\nagentic capabilities in the API. LUCIANO MARTINS:\nThanks for the add. SHRESTHA BASU MALLICK: Yeah. So starting with what are\nthe families of models we have available? So the most performant and\npowerful model that we have is the Gemini 2.5 Pro. It's in preview right now. A few weeks ago, we released\na updated version of 2.5 Pro. And it's suitable for highly\ncomplex tasks that require a lot of deep reasoning. And coding has been, of course,\na standout use case of that. We also have 2.5 Flash,\nwhich this Google I/O we released a new preview\nversion just yesterday for 2.5 Flash. And 2.5 Flash is\nour model size that has probably one of the\nbest price performance ratios in the market today. LUCIANO MARTINS: That's right. SHRESTHA BASU MALLICK: We\nalso have 2.0 Flash-Lite. So this is a small,\nfast, cheap model that you can use for high\nvolume tasks like summarization. And then Luciano,\ndo you want to talk about the Nano and\nthe Embedding models? LUCIANO MARTINS:\nYeah, absolutely. We heard a lot of\nfeedback from you folks. And many of you are\ndeveloping mobile applications or applications that\nmust run on devices. So then you have\nalso available what we call the Gemini\nNano, which is a smaller version of the Gemini, which is\nable to run locally on devices like on Android devices if you\nare developing with the Android Studio using the AI Corps. And also, many of you are\ntrying to create applications that do some kind\nof semantic ranking or to organize information\nin large scale. So you have also one model that\nwe call the Gemini Embedding, which the key objective of this\nmodel is to let you ingest text. And then the model delivers you\nhigh quality multidimensional embeddings. But how those\nmodels are behaving when we think about the key\nstandard benchmarks, Shrestha? SHRESTHA BASU\nMALLICK: Yeah, we'll go to the benchmarks\nin a second. But the message\nthat I want everyone to take away from this\nslide is whether it's",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_002",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 254.53,
    "timestamp_end": 382.64,
    "text": "when we think about the key\nstandard benchmarks, Shrestha? SHRESTHA BASU\nMALLICK: Yeah, we'll go to the benchmarks\nin a second. But the message\nthat I want everyone to take away from this\nslide is whether it's a super complex task, or whether\nit's on-device processing, you now have a Gemini model\nthat you can use for it. And it's pretty powerful. And it's pretty price effective. LUCIANO MARTINS: Yeah,\nmaybe it is also worth mentioning, Shrestha,\nthat some folks here may be developing with\nGemini since it was released. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS: So using the\nAI Studio, using the Gemini API, the older versions\nof Gemini like 2.0, 1.5 are still available. But we really\nencourage everybody to start experimenting\nwith the newer ones with better capabilities\nand better performance. SHRESTHA BASU MALLICK:\nThat's correct. So 2.0 Flash continues to be one\nof our most widely-used models, just again, because of how good\nit is for the price it's at. And we have a lot of people\nwho are still using it. And then we also have some\npeople on the 1.5 models. But we're encouraging\npeople now, we're two generations\nahead, to start using the 2.5 series of\nmodels and give us feedback. LUCIANO MARTINS: Nice. So please tell us a little bit\nabout the benchmarks, Shrestha. SHRESTHA BASU MALLICK:\nYeah, next slide. Yeah, so we're not going\nto spend a lot of time on the benchmarks. But the two key points\nthat I'd like to hit here is number one, is one\nway in which we measure how good these models are is\nby putting them on LMArena, which is where developers\nand builders like yourself test out\nall these models and give it an ELO rating. And we're, of course, very\nproud that right now you have three Gemini models in\nthe top 10 in the LMArena. LUCIANO MARTINS:\nIncluding the top one. SHRESTHA BASU\nMALLICK: Including, how did I forget that? We are number one guys,\nand we're very proud of it. [CHEERING] Thank you. One of the things,\none of the use cases that has been really emerging\nin the last couple of months",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_003",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 364.16,
    "timestamp_end": 515.85,
    "text": "SHRESTHA BASU\nMALLICK: Including, how did I forget that? We are number one guys,\nand we're very proud of it. [CHEERING] Thank you. One of the things,\none of the use cases that has been really emerging\nin the last couple of months is, of course, app creation\nand live coding and going from 0 to 1. The best leaderboard right now\nfor that is the WebDev Arena. And we again, are number one\nwith 2.5 Pro on WebDev Arena. So if you are trying to build\n0 to 1 or few shot apps, Gemini 2.5 Pro is a\ngreat model for that. But in addition to\nuser preferences, we also like to measure\nhow well our models are doing based on rigorous\nacademic benchmarks. And so I'm not going to\ngo into a lot of details on these slides. These are available\nin blog posts. Reach out to me\nand Luciano if you want to know how exactly\nthese models are doing. But the TL;DR here is whether\nit's in highly complex domain-specific questions,\nwhether it's in coding, whether it's in\nmultimodal understanding, the Gemini 2.5 Pro is leading\non a lot of these academic benchmarks as well. Finally, performance is\none side of the coin. The other side of the\ncoin is, of course, price. This is a chart from Swyx that's\nbeen very popular on Twitter for the last few years-- on X, I'm sorry for\nthe last few months. But this really shows that\neven for price performance, the Gemini 2.5 models are\nreally at the frontier. Luciano? LUCIANO MARTINS:\nYeah, and I think it is worth\nmentioning, Shrestha, especially for those\nof you who watched Sundar's keynote yesterday and\nalso the developer's keynotes. Inside the DeepMind, we\nare doing this huge effort of trying to cover the\ndifferent aspects of experiences you may have. So we will talk a lot\nabout Gemini here. But also, we have\nthe model family that we are calling\nGenMedia, where you are able to generate high\nquality images, high quality videos. You also have-- and there is a\ncolleague from our team, Paul Ruiz, who is delivering\na talk this afternoon related to the Gemini Robotics,\nthe Gemini family of models",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_004",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 501.0,
    "timestamp_end": 641.8,
    "text": "you are able to generate high\nquality images, high quality videos. You also have-- and there is a\ncolleague from our team, Paul Ruiz, who is delivering\na talk this afternoon related to the Gemini Robotics,\nthe Gemini family of models related to people\napplying robotics, developing robotic\nsolution with applied AI. And also, just\nafter the session, we have one another one\nwith Omar and Gus Martins to talk about Gemma 3,\nwhich is the open models family for from\nGoogle DeepMind, also developed with all\nthe technologies we use to develop Gemini. SHRESTHA BASU MALLICK:\nAnd some really good benchmark performance-- LUCIANO MARTINS: Absolutely. SHRESTHA BASU\nMALLICK: --in Gemma 3 as well, as Omar and\nGus will tell you. LUCIANO MARTINS:\nSo still talking about the different\nexperiences you may have, we heard a lot of feedbacks\nfrom you folks around the world. And one of the most asked\nfeatures we always heard were related to letting Gemini\nto create high quality audio. So yesterday we launched what\nwe are calling the Gemini TTS model, where you can from text\ngenerate high quality audio. But the coolest\nthing about the TTS is that you are not\njust creating audio. You have abilities to customize\nemotions on the audio. You may have multiple voices. You can create the audios\nwith different languages, like Brazilian Portuguese. And you can even create\nmulti-speaker interactions. So if, for example, you want\nto deliver an experience where your users will be thoughts\nabout something on a podcast, like formats, like the one\nyou have on the NotebookLM that people love. Who here is using\nNotebookLM for anything? Amazing. So you can have something\nlike the AI Overviews that are developed and delivered\nby the Gemini TTS model using the TTS, right Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: And I think it\nis not the only thing related to audio. We brought that to the live\ninteractions as well, right? SHRESTHA BASU\nMALLICK: That's right. So the TTS models,\nlike Luciano said, are available via chat endpoint.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_005",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 628.55,
    "timestamp_end": 769.3299999999999,
    "text": "LUCIANO MARTINS: And I think it\nis not the only thing related to audio. We brought that to the live\ninteractions as well, right? SHRESTHA BASU\nMALLICK: That's right. So the TTS models,\nlike Luciano said, are available via chat endpoint. And then on our Live API,\nyesterday, we also rolled out the native audio output models. So this is now\nGoogle's first release of an audio-to-audio\narchitecture. And there are two variants of\nthis model that you have access through the Gemini Live API,\nwhich is our real time API. So you have the Native\nAudio Dialogue model, which is the real benefit of\nthe native audio output models are that the voices\nthat come out of it are much more natural\nand much more compelling. The native audio-to-audio\narchitecture has better contextual\nunderstanding of what humans say to the AI. It can seamlessly transition\nbetween different languages, so maybe from Brazilian\nPortuguese to Bengali, if you want to switch\nin the same language, in the same sentence. And it's available on\nour Gemini real time API. We also have a version\nof this model available, again on the real time\nAPI with thinking enabled. So for more complex use\ncases, so one use case that comes to mind is let's say\nyou're building a gaming agent to play a strategy game. And you want that agent to be\nable to think, but also to talk to you with relatively\nlow latency. We have a thinking version\nof the native audio model that's also now available\nfrom the Live API. So please try it out\nand give us feedback. LUCIANO MARTINS: Perfect. So how people can start\nusing those models, Shrestha? SHRESTHA BASU MALLICK:\nThrough the API. Of course. But before we go there,\nthere are two other things that we should mention. It's hot off the news. You've probably\nall heard about it. We've also enabled\nan advanced reasoning mode called Deep Think. And the idea behind Deep\nThink is that the model 2.5 Pro can think through various\npossible answers to a problem before giving you\nthe best answer. So that is available\nin trusted testers,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_006",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 751.58,
    "timestamp_end": 904.85,
    "text": "mode called Deep Think. And the idea behind Deep\nThink is that the model 2.5 Pro can think through various\npossible answers to a problem before giving you\nthe best answer. So that is available\nin trusted testers, but will be rolled\nout more widely soon. And then we also have\nGemini Diffusion, which is the first time\nwe are testing a diffusion architecture as opposed to an\nautoregressive architecture. And these models are faster\nthan the fastest model out today and for almost\nsimilar, the performance. So try out both when you have\naccess to both Gemini Deep Diffusion and Deep Think. LUCIANO MARTINS: Perfect. SHRESTHA BASU MALLICK: All\nright, so the Gemini API. LUCIANO MARTINS: So yeah, if\nyou start thinking, especially after the keynote\nspeakers today, there are many surfaces where\nyou can interact with the Gemini models. So you have the Gemini app. You have Gemini available\non Google Cloud as the Code Assist, the assistant-- the\nCloud Assist inside the Console. You have Gemini on Workspace. So when you are talking\nabout developer experiences, we have the one\nspecific Gemini API with similar experiences as you\nhave on the no-code UI that we call AI Studio, where you\ncan start experimenting and developing your solutions\nusing the Gemini models, right? SHRESTHA BASU\nMALLICK: That's right. LUCIANO MARTINS: So\nbasically, the Gemini API is a very low barrier\nentry for having Gemini programmatic experiences. You have access to\nall public models there from the Gen Media ones,\nGemma, all the Gemini models, variants. You have a very generous\nfree of charge tier, where you can start\nexperimenting just after the session\nwithout concerns about credit cards, costs,\nbilling, anything of that. We keep developing\nmore SDKs for this API. So for now, you have available\none SDK for Python, JavaScript, Go, and Java. We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_007",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 890.57,
    "timestamp_end": 1021.23,
    "text": "We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there. If you are a Google\nColab user, you have ways to interact with\nthe Gemini models as well. So the key idea is to\nmake it easier for you. No matter where you are having\nyour development experience, We are trying to bring\nGemini API closer to you. SHRESTHA BASU MALLICK:\nThat's correct. And I think this is\nalso a good place to make a plug for Google\nAI Studio, which of course, everyone knows about and loves. It is the place where\na lot of our developers first test out the\ncapabilities of the API before committing to\nbuilding applications at scale with the API. And Google AI Studio now\nhas code generation also. LUCIANO MARTINS: Absolutely. SHRESTHA BASU MALLICK: So this\nis a quick, high level overview of all the components that are\navailable through the Gemini API. It's pretty standard. You send in your prompt, you get\na response back from the model. If there's a tool\ncall needed, we do support a set of\nfirst party Google tools, as well as function calling. So in terms of the first party\nGoogle tools that we support, we, of course, support\nGoogle Search as a tool. The best search engine\nout there is now available as a tool for the\nmodel to call when it needs to. Now in addition to the\ninformation that you get out of Search, if you want to\nretrieve more in-depth content from web pages for\napplications such as you're building your own version\nof a research agent, so you pull some\ninformation from Search. You pull a set of\nURLs, but if you want to extract a little\nmore in depth content, we just released a new tool\nat I/O called URL Context. And then, of course, you can\ntake all that information. And if you want to create\nbeautiful charts out of it or run some analysis,\nwe make code execution available as a tool to\nyou via the API as well. Then, of course, you\nhave function calling.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_008",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 1006.14,
    "timestamp_end": 1144.6,
    "text": "And then, of course, you can\ntake all that information. And if you want to create\nbeautiful charts out of it or run some analysis,\nwe make code execution available as a tool to\nyou via the API as well. Then, of course, you\nhave function calling. And as part of all\nthe improvements that we've released\nat Google I/O, we've also put a lot\nof effort into making sure our structured outputs\nfunctionality, which lets you get outputs\nin JSON schema, is much more robust\nand comprehensive. So again, try it out\nand give us feedback. And then finally, we do have\na set of safety and copyright filters. These are configurable\nby developers. The idea is in order-- you have the tools you need\nto make the applications you build safer. And you have control about\nhow-- for most of the filters, you have control about how where\nyou want to set that threshold. LUCIANO MARTINS: Perfect. So as a TL;DR, if we could\nexplain on a tweet how that works, with the Gemini\nAPI and using the SDK, you can work with all\nthe information you need. Doesn't matter on which format\nit is, if you have spreadsheets, docs, PDF files,\nvideos, audio, live interactions via voice\nand video sort of stuff. If you need to give specific\ninstructions to the model, you have the ability to bring\nthese instructions to the model follow on every interaction you\nhave during your application usage. And you have, as results\nor as outputs, the ability to have generated text or\ngenerated images or audio or video, or even keep\ntraining with more API calls using function calling. SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: That\nwas a huge tweet. SHRESTHA BASU MALLICK: That\nwas a huge tweet, maybe threaded tweet. All right, so let's talk about\nsome key Gemini API features. We don't have enough\ntime in this talk to go through everything. But Luciano and I thought we\nwould hit some of the highlights that we really want people\nto know more about and use. So one of the areas of feedback\nthat we used to get a lot is you can, of course, upload\nfiles to the Gemini API.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_009",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 1130.2,
    "timestamp_end": 1280.26,
    "text": "But Luciano and I thought we\nwould hit some of the highlights that we really want people\nto know more about and use. So one of the areas of feedback\nthat we used to get a lot is you can, of course, upload\nfiles to the Gemini API. You can also, if you're\nless than 20 megabytes, you can also pass some of\nthese media information inline. But now, you can also-- and we've had this feature\nout for a few weeks. You can also pass\na YouTube link. And the Gemini API can\nanalyze that information. When you pass media files\nlike videos, depending on how much you want to fit\ninto the context window, you have now, a choice between\nthree resolution settings. At the lowest setting,\nyou can process up to six hours of video. But then you also have more\nhigh resolution settings that you can use. We support dynamic frame rate\nper second in the Gemini API. You can read that more about\nthat in our documentation, or come talk to me and Luciano. We support video clipping. That's a new feature. And we support\nimage segmentation. One other point I want to make\non multimodal understanding is even in the days\nof 1.5 and 2.0, Gemini models were\nsome of the best models out there for multimodal\nunderstanding. The example I like to give\nhere is I was in Costa Rica, and my guide showed\nme-- this is night. And my guide showed me\noh, there's a glass frog somewhere there on a branch. I took a photo, but I did not\nsee the glass frog in real life or in the photo. But I passed it to Gemini. And Gemini not\nonly saw the frog, it identified the\nspecies correctly. So that's how good multimodal\nunderstanding is on the Gemini. Please try it out. We also support long context. And then we, along with long-- long context is we have some\nof the largest context windows out there. So like the equivalent\nof depending on whether you're using a model\nwith 1 million context or 2 million context, you can read\nthe equivalent of eight novels, have the model read it\nand/or entire code bases. But what can sometimes\nhappen with long context",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_010",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 1264.27,
    "timestamp_end": 1404.54,
    "text": "whether you're using a model\nwith 1 million context or 2 million context, you can read\nthe equivalent of eight novels, have the model read it\nand/or entire code bases. But what can sometimes\nhappen with long context is that your input\ntoken pricing goes up. But for that, we now\nhave context caching. We have been supporting what\nwe call explicit context caching for a while. Explicit context caching\nis when you tell the API, cache this context, and reuse\nit for the next few turns. And you were going to get a\n75% discount on the pricing. But now what we support is\nimplicit context caching. So if we feel that you\nhave a context that is getting repeatedly\nused, we will automatically cache that for you and pass\nthe price savings to you. So again, hopefully this\nis a huge price benefit to our developers. And then we, of course,\nprovide you with transparency to see how many\ntokens you are using. LUCIANO MARTINS: That's amazing. SHRESTHA BASU MALLICK: Yeah. Do you want to talk a little\nbit about text generation? LUCIANO MARTINS: Yeah, I think\none of the greatest things that we are building step by\nstep with the Gemini models and the Gemini API is, you may\nhave different experiences, as Shrestha explained,\nwith videos and clipping, specific time offsets, or\nchanging the frames per second. And then the first\nout-of-the-box output of Gemini when we launched it in December\nof '23 was text generation. So we keep having this. As one of the\nchoices you have, you can have an experience where\nGemini will just create text. And by text, it may\nbe like anything. It may be one\nstructured JSON output. Or it may be some\ncoding in Python, C, or any language of\nyour preference. But then we keep increasing\nthe semantic understanding capabilities of the models. So we are not just uploading\none PDF file with a lot of text and some charts and\nsome conclusions. You are also counting\non the Gemini ability to understand how the\ninformation on those documents or on those spreadsheets\nconnect to each other,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_011",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 1387.2,
    "timestamp_end": 1519.6,
    "text": "So we are not just uploading\none PDF file with a lot of text and some charts and\nsome conclusions. You are also counting\non the Gemini ability to understand how the\ninformation on those documents or on those spreadsheets\nconnect to each other, especially if you have a more\ncomplex situation, where you are sending multiple PDF files or\nPDF files with spreadsheets and videos and everything. Without a huge, heavy\nlift from your side, you can count on Gemini\nto understand what's the message, what's\nthe information, what's the reasoning behind all that\ninformation and do the math or do the understanding\nfor you, right Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. I just want to give a special\ncall-out to two features here, which again, developers\nhave been asking us for. One is you can get bounding\nboxes through the API. And secondly, in addition\nto bounding boxes, you have what's called\nimage segmentation, which is get the bounding\nbox information for an object in the image. You get a classification of\nwhat that image might be, so that's sent to\nyou as metadata. And then you get this\nmasked segmentation of that specific area\nand that object as well. LUCIANO MARTINS: Nice. SHRESTHA BASU MALLICK: We\nalso support streaming. And you can set\nsystem instructions on top of whatever\nsystem instructions we already have in place. LUCIANO MARTINS: Yep. And I think that's the\nsame for the media models right, Shrestha? So now you have basically,\nthree main doors to use the media models. You have the Imagen 3,\nthe, Google DeepMind model with the best high\nquality image generated. You have one variant of Gemini\nthat we call Gemini Image Out that lets you\nalso generate images, but with two key\ndifferences from Imagen 3. First, you can have interleaved\noutputs, including text and images together, like\nhaving one explanation on step-by-step guide to do\nsome action, including visuals. And also, you can edit\nimages, right, Shrestha? So you can have one first\nversion of the image generated.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_012",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 1506.06,
    "timestamp_end": 1650.34,
    "text": "and images together, like\nhaving one explanation on step-by-step guide to do\nsome action, including visuals. And also, you can edit\nimages, right, Shrestha? So you can have one first\nversion of the image generated. You want to change the shirt\ncolor or the background or add glasses to the\ncharacter on the image, you can keep chatting\nwith the model, asking to modify the image. And you keep enhancing\nand optimizing your result, right, Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. And through Veo 2, you all\nsaw the release of Veo 3. That'll probably\ncome soon to the API. But with Veo 2, we support text\nto video and image to video. This is again from\na recent blog post that our researchers put\nout about a week ago. You can see the 2.5 Pro\nalso, as I mentioned earlier, leads on key video understanding\nbenchmarks like MMMU. So that's also something that\nwe're very proud of that we've been pushing the boundaries on. OK. So now let's talk\nabout the Live API. So a lot of our\nfeatures, as I mentioned, are available\nthrough the Chat API. But we also provide\nthe Gemini Live API, which is our real time, low\nlatency API for use cases that require more of these\ninteractive real time kind of experiences. There are two architectures now\navailable through this Live API. There is the cascaded\narchitecture, which has native audio\ninput, but the output is done using text\nto speech, and we were using the same text\nto speech models that was used by NotebookLM. A lot of people preferred\nthis architecture because it's more reliable. We've had it out since\nDecember, and we're aiming to bring the\nlatest 2.5 Flash model to this architecture. We also, starting I/O, now have\nthe audio-to-audio architecture that I mentioned, where you\nhave native audio input as well as native audio output. And this, of course, native\naudio output, as I already mentioned, gives you\nmuch more natural sounding voices\nand the ability-- you don't now have to\nspecify your language. You can seamlessly switch\nbetween languages and flow in and out.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_013",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 1635.49,
    "timestamp_end": 1785.6299999999999,
    "text": "And this, of course, native\naudio output, as I already mentioned, gives you\nmuch more natural sounding voices\nand the ability-- you don't now have to\nspecify your language. You can seamlessly switch\nbetween languages and flow in and out. We support tool chaining\nin the Live API. We have supported\nthis since December. So all of the tools that\nI mentioned, Search, Code Execution, URL\ncontext, function calling, you can layer these\ntools in the same prompt and get much more\ncompelling results. Get data from Search, do some\nanalysis, get the output. We have voice\nactivity detection. Of course, we need\nit in the Live API. What we now provide for\nyou, though, is the ability to configure the\nthresholds, how much of time do you want after\nthe end of speech to decide that the\nuser's speech has ended. That's one of four\nthresholds that you can now set with the Live API. You can also disable our\nvoice activity detection model and bring your own. Session management. We have a lot of\nparameters out there. So in its most basic state,\nthe Live API currently supports about 20\nminutes of audio and about a few\nminutes of video. But we now have\nvarious techniques for you to increase\nyour session length, including sliding window, the\nability to change resolution on what video is\npassed, the ability to decide, do you want audio\nto be streamed only when-- do you want video to be streamed\nonly when audio is being spoken, or even when audio\nis not being spoken, and other parameters that you\ncan use through the Live API. Ephemeral tokens\nare coming soon, but that's one way to do\nauthorization into the Live API. And finally, with the native\naudio output, specifically the audio-to-audio\narchitecture, we are also releasing a couple\nof modes for you to try out. One of them is proactive audio. What this feature\ndoes today is it lets the AI decide\nwhen to respond to you and when whatever the human\nis saying is irrelevant. So imagine if I'm having a\nconversation with the AI, and Luciano comes to me and\nsays something unrelated.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_014",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 1767.92,
    "timestamp_end": 1905.9299999999998,
    "text": "What this feature\ndoes today is it lets the AI decide\nwhen to respond to you and when whatever the human\nis saying is irrelevant. So imagine if I'm having a\nconversation with the AI, and Luciano comes to me and\nsays something unrelated. The AI will know not to\nrespond to that audio output, so the AI proactively\ndecides not to respond. So we are calling\nit proactive audio, because we aim to\nbring much more proactivity to this feature. And then effective\ndialogue lets you pick up on the user's tone\nand sentiment and lets the AI respond appropriately. As I mentioned, you also\nhave thinking available with the Live API. All right. Time for agents. LUCIANO MARTINS: Excellent. So how many of you are\ntrying to experiment solving your computational\nproblems using agents or multi-agent solutions? Yay, everybody. SHRESTHA BASU MALLICK: All\nthe Brazilian contingent. LUCIANO MARTINS: OK. So with that in\nmind, we always try to develop the new tools and the\nnew capabilities of the model thinking how you\nfolks can use them in your projects to make agents\nbetter and more trustful. Right, Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: So\nif you start thinking about how our regular\nagents architecture work, what do you have\nthere, Shrestha? So basically, we have\nthree main blocks. SHRESTHA BASU MALLICK: Yes. LUCIANO MARTINS: We normally\nsee one orchestration layer, one models layer,\nand one tools layer. Right? SHRESTHA BASU MALLICK: Yeah. May I build upon that? I think as we've been mentioning\nwith the 2.5 series models, these models are predominantly\ntrained to be really good at planning and reasoning,\nwhich, when you think about it, is a key part of what\nmakes an agent work. So the model layer is\nwhere a lot of the planning and reasoning happens. And then, of course,\nthere's the tools layer. We've already talked\na lot about we have a set of first party\nhosted tools from Google, Google Search, Code Execution,\nand URL context, as well as a few other tools. Some of you may have heard\nSundar mention the computer use",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_015",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 1889.49,
    "timestamp_end": 2019.5900000000001,
    "text": "We've already talked\na lot about we have a set of first party\nhosted tools from Google, Google Search, Code Execution,\nand URL context, as well as a few other tools. Some of you may have heard\nSundar mention the computer use tool that's coming,\nso we're going to make it available\nthrough the API. We've already rolled it\nout into trusted testers, but that's going to be\npublicly available soon. And then we have a couple of\nother tools on the way as well. LUCIANO MARTINS: All right. SHRESTHA BASU MALLICK: And then\nthere's the orchestration layer, as you were saying, Luciano. LUCIANO MARTINS: Yeah,\nyeah, absolutely. So basically, when we\nare creating an agent, you want to give key\ndirections to these agents. So basically how\nit's going to behave, what's the profile of these\nagents, what's the goal of it. It's going to help\nwith researching, with coding, with learning. Any specific area you are\ntrying to address actions for your users,\nyou need to count with some memory\nfor these agents so you can keep\nthe previous user's interactions with the model. Or you can try to extend\nthis agent memory using any mechanism like RAG or adding\nPDFs or using the wrong context. And also you must\ncount on the model to do the key reasoning\npart of this agent. So how to bring all\nthose stuff together, how to give the best\nanswer to the users, and when and how to\nuse each of the tools that are available\nto this agent. SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: So yeah. So basically, if you\nput all that together, we are looking for\napplications that we are calling like\nmulti-agentic applications or multi-agent applications\nwhere we want some autonomous integration with our tools. We want those agents to take\nactions to help our users. We count on the models like the\nGemini 2.5 Pro, the best one on reasoning capabilities,\nto reason and plan all their actions to\nbe given to this user from helping with shopping\nto generate new codes for a new application. This model must be able to\nbe continuously learning.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_016",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 2003.26,
    "timestamp_end": 2144.72,
    "text": "on reasoning capabilities,\nto reason and plan all their actions to\nbe given to this user from helping with shopping\nto generate new codes for a new application. This model must be able to\nbe continuously learning. So if you want to append\nmore pieces of information, fresher information, use\ntools like the Google Search grounding, right, Shrestha, to\nbring up-to-date information from Google Search. This model must be\nable to support that. And at last but\nnot least, we count on multi-agent collaboration. So we must be able to not\ncreate huge monolithic agents, but also to keep connecting\nto other specialized agents to bring a better experience. Right? SHRESTHA BASU\nMALLICK: That's right. You've probably heard us talk\nabout all of these components 50 times already in\nthis talk, so I'm going to go through this\nslide really quickly. But one thing we\nwanted to emphasize is when we started\nthinking about how to enable agentic\ncapabilities through our API, we made a conscious\ndecision on first focusing on providing\nhigh quality primitives. And once we had\nmade some progress along that, that's\nwhen we have now started to do things like\nrelease MCP through our SDK, and you'll see some more\nhigher abstractions rolling out in the next couple of months. But in terms of\nagentic primitives, again, we have the 2.5\nseries models, which are thinking first models. You now have Deep Think,\nwhich is an advanced thinking mode on top of 2.5 Pro. And with Flash today\nand Pro soon, you have the ability to set budgets. So you can tell the model when\nto think and how much to think, and that lets you both control\ncost, latency, and whatever is the amount of thinking that's\nsuitable for your application. In terms of API, depending on\nwhat agent you are building, you may want to use different\nversions of the API. If you're building\na research agent, maybe you want to\nuse the Chat API. If you're building\na gaming agent, maybe you want to use the real\ntime API or a customer support agent. Of course, you can build\nresearch agents also",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_017",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 2130.98,
    "timestamp_end": 2271.49,
    "text": "If you're building\na research agent, maybe you want to\nuse the Chat API. If you're building\na gaming agent, maybe you want to use the real\ntime API or a customer support agent. Of course, you can build\nresearch agents also on the real time API. But in general, you now\nhave these two APIs. And then we've talked\nabout tools a lot. So all I'll say is\ngive us feedback in terms of what are some\nof the other tools that you would see Google make available\nthrough the Gemini API. LUCIANO MARTINS: Yeah. So as you just said,\nthe Gemini 2.5 Pro-- 2.5 models are thinking\ncapable, so they can do more advanced and\nmore complex reasoning. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS:\nAnd maybe we could highlight that now,\nspecifically for the 2.5 Flash, we have the ability of\nusing what we just mentioned, the thinking budgets, where\nyou can calibrate how deep and how much the model\nwill do reasoning cycles. And also for both models,\nwe can have access to what we are calling\nthought summaries. SHRESTHA BASU MALLICK: Yes. LUCIANO MARTINS: So\nbasically you are not only counting on the model\nability to reason, but also you can see on your\nresponse which key steps or which key reasoning\nthoughts the model had to get to our conclusion. So basically, that's a\npretty basic Gemini API code using the Python SDK. For those of you who\nnever used the SDK before, it's pretty straightforward. So three key blocks. You need to import the SDK,\nyou instantiate the client, and then you interact\nwith the model you want. Here we are interacting\nwith the Gemini 2.5 Flash. You send your prompts. And then you have\none specific bit called ThinkingConfig,\nwhere, in this code, you are asking the API\nto include the thought summaries on your\nresponse, and that's to include thoughts\nequals true, and also the amount of tokens you want\nto use during the reasoning process. On your response,\nyou're going to have part of the response will\nbe the thought summaries. So it's separated from the\nfinal answer by intention. If you want to keep\nthose on your backend,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_018",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 2251.89,
    "timestamp_end": 2375.4500000000003,
    "text": "the amount of tokens you want\nto use during the reasoning process. On your response,\nyou're going to have part of the response will\nbe the thought summaries. So it's separated from the\nfinal answer by intention. If you want to keep\nthose on your backend, on your data warehouse,\nyour BI environment, you are free to do that. And also you have a second part,\nwhich is the final model answer. That may be, for example, the\npart that goes to the end user. Right, Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: So here,\nwe added two quick GIFs showing how the\nthought summaries work. You have, on the left GIF,\nthe AI Studio experience, and on the right, the\nGoogle Colab experience. And pretty much we ask\nit the same question and you can see how\nyou can interact with the thoughts on\nboth environments. Right, Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: OK. So what about the enhanced\ntooling we have now, Shrestha? SHRESTHA BASU MALLICK: I\nthink in the interest of time, we can skip the slide because\nwe've talked about tools a lot. The one point I\ndo want to make is I mentioned when I was\ntalking about the Live API, that one of the things we\nstarted doing in December itself is we allowed\nyou to chain tool, so use multiple tools\ntogether in the Live API. We're now rolling that out\nto the Chat API as well, starting from Search and\nCode Execution together. But you will see\nmore the ability to do more combinations of\ntools in the Chat API as well. LUCIANO MARTINS: Amazing. So basically that's\nwhat you just said. That's a similar\nexperience with the SDK. Now you have two tools\nincluded on your interaction. You have, in this case, the Code\nExecution tool and the Google Search tool. And then on your answer you can\nsee also on separate structures the Code Execution results,\nincluding the Python code used by the Code Execution, and also\nthe output of this execution. The Google Search grounding\nresults, including the Google Search, real search\nthat was performed,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_019",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 2361.37,
    "timestamp_end": 2499.7400000000002,
    "text": "the Code Execution results,\nincluding the Python code used by the Code Execution, and also\nthe output of this execution. The Google Search grounding\nresults, including the Google Search, real search\nthat was performed, and the results of this,\nand your final answer from the model. Right? SHRESTHA BASU\nMALLICK: URL context. As we said, this is a new\ntool that we are rolling out. It's one of the tools\nthat powers Deep Research if you've used Google's research\nagent through the Gemini app. And the idea is\ngiven a set of URLs, we allow you to extract\nmore in-depth content. Of course, in a way\nthat's approved by and respectful to our\npublisher ecosystem. But you can get\nmore content out. And this is really helpful\nfor research and analysis type of use cases. You can use this tool by itself,\nor you can use this in tandem with the Search tool. LUCIANO MARTINS: Yeah. And again, a quick code snippet. You add the two\ncode URL contexts and you can send\ndirectly on the prompts, without further efforts, all\nthe links, up to 20 links, you want to consider\non the model, processing on the\nmodel execution. And you have all the reasoning,\nall the semantic extraction performed by the\nmodel really fast. SHRESTHA BASU MALLICK:\nLooked like some people were still taking pictures. But if you all missed\ntaking an image of a slide, all of this information is\navailable in our documentation. LUCIANO MARTINS: Absolutely. SHRESTHA BASU MALLICK: And\nLuciano and I are also here to answer questions. Function calling. I think we've had-- of\ncourse, function calling is bread and butter for all\nkinds of agentic applications. The Gemini API has supported\nsingle function calling, parallel function calling, and\ncompositional function calling for a while. Compositional meaning where you\ncan put a whole logic around if A happens, then call\nfunction 1, if B happens, then call function 2. As part of I/O, we are also\nreleasing asynchronous function calling through the Live API. So imagine some tasks like you\nstart having a conversation",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_020",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 2484.66,
    "timestamp_end": 2631.966,
    "text": "if A happens, then call\nfunction 1, if B happens, then call function 2. As part of I/O, we are also\nreleasing asynchronous function calling through the Live API. So imagine some tasks like you\nstart having a conversation with the AI agent,\nbut in the background, you ask it to crank\nout on some other task. And that task may not\nbe a real time task, but it may be something that's\nrelevant to the conversation, like maybe analyzing\na lot of context about the user to\nprovide some answer. You can now turn on an\nasynchronous function. The function will do its\nthing in the background. And then when ready, it will\nnotify you with the results. LUCIANO MARTINS: Amazing. And maybe one of\nthe greatest news for the Gemini\nAPI SDK during I/O is that we brought--\nwe heard you. Many of you gave\nthis feedback for us, and we added to the Gemini\nAPI SDK support for MCP. So now you don't need to have\nseparated and siloed codes with MCP clients,\nMCP servers, and then your Gemini API interactions. You can have it all\ntogether, using the same code base of the Gemini API SDK. SHRESTHA BASU MALLICK:\nVery exciting. [APPLAUSE] LUCIANO MARTINS: Yeah. Thank you. And still talking about\nthose interactions, we know that we have many\nchoices of agent frameworks. You may have heard\nabout the Google ADK, the Agent Development Kits\nlaunched during the Google Cloud Next a few weeks ago. We may have heard about the\nagent-to-agent protocol as well. You have other choices like link\nchain, link graph, everything, and we keep open\ncollaborating with all those internal and external\nopen source efforts to bring the best\nexperience for you all. SHRESTHA BASU MALLICK:\nThat's correct. And especially thanks to folks\nlike Philip Schmidt and Patrick here on our developer\nrelations team. We have been working on building\ncloser relationships, closer interactions, better code\nsamples with some of the leading agent frameworks,\nlike LangChain crew. And so that is also\nsome of the areas where you'll see the\nGemini API show up. LUCIANO MARTINS: Amazing.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_021",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 2610.75,
    "timestamp_end": 2759.62,
    "text": "We have been working on building\ncloser relationships, closer interactions, better code\nsamples with some of the leading agent frameworks,\nlike LangChain crew. And so that is also\nsome of the areas where you'll see the\nGemini API show up. LUCIANO MARTINS: Amazing. So if you folks are\nconcerned about a lot of-- the amount\nof stuff you just shared today, that's\na huge dump of things. Maybe the best suggestion or the\nbest guidance we could give you is first, as Shrestha\nmentioned, we have the Google\nDeepMind DevRel team distributed across the globe. We have people in Latin\nAmerica, in Europe, in Asia. We have a huge presence here in\nthe US, so please count on us. Connect with us online, on\non-site events like this. And think on those\ntop six actions when you are creating your\nagentic experience from having clear objectives on your mind. Try to laser focus on the\nproblem you need to solve. Do a lot of interactions\non your developments. Do live coding if you\nneed to part on those. Live coding if you need, if\nyou are working with something you are not that familiar. Count with the Gemini\n2.5 models to help you with your coding experience. And always, always focus\non your user experience. That's the best key for success\nyou may have with your tools. Right, Shrestha? SHRESTHA BASU MALLICK:\nThe OG rule of product. LUCIANO MARTINS: Absolutely. So yeah. And that's it. I hope you enjoyed that. Start building now. We brought you here-- [CHEERING] Thank you. We give you some links here. So some of you may\nget the reference. But basically the first link\nis the AI Studio, the UI and no code experience where you\ncan experiment all the Gemini models features really\nfast without writing a single line of code. The second link is\nthe Gemini API docs, where you may find all the\nthings we shared here, and way more of the other features and\npossibilities with the models. And the last one is the Gemini\nCookbook, a GitHub repository that our team curates and keep\nupdating with a lot of sample experience for you. Thank you so much, Shrestha.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "gHHjDRDNUNU__chunk_022",
    "schema_version": "canonical_v1",
    "video_id": "gHHjDRDNUNU",
    "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
    "timestamp_start": 2746.57,
    "timestamp_end": 2767.29,
    "text": "more of the other features and\npossibilities with the models. And the last one is the Gemini\nCookbook, a GitHub repository that our team curates and keep\nupdating with a lot of sample experience for you. Thank you so much, Shrestha. Thank you so much you all. SHRESTHA BASU MALLICK: Thank you\nall for coming out to hear us. [CHEERING] [MUSIC PLAYING]",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_000",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 0.0,
    "timestamp_end": 139.04,
    "text": "[MUSIC PLAYING] JOANA CARRASQUEIRA:\nHello, everyone. My name is Joana Carrasqueira,\nand I lead Developer Relations at Google DeepMind. JOSH GORDON: Hi, everyone. I'm Josh. JOANA CARRASQUEIRA:\nAnd we're very excited to welcome you to\nour session, Google's AI Stack for Developers. We'll start by giving you a\nquick overview of Google's AI stack. Who's at I/O for the first time? Can I see some hands up? Oh, OK. Welcome to Google\nI/O. It's a pleasure to have you with us today. So we'll start by giving\nyou an overview of Google's end-to-end ecosystem of AI. And as you know, we've\nbeen leading the way in AI for decades, since we\nopen-sourced TensorFlow in 2015, from when we published our\nfield-defining research with transformers\nin 2017, to Gemini. And we are now in\nthe Gemini era. So we've been releasing a lot. Relentlessly, as it's\nbeen called today, we've been shipping many\nfeatures, many new products. And in our talk,\nwe're actually going to give you an overview\nof everything that's new for developers\nthroughout the AI stack. Our mission is to empower every\ndeveloper and organization to harness the power of AI. And Google's stack is so good\nand flexible because it combines very robust infrastructure\nwith state-of-the-art research. And all of this enables\nreal-world applications come to life that change\nentire fields, industries, and companies. We'll start by discussing\nfoundation models, touching upon our\nGemini, Gemma, and some of our domain-specific models. JOSH GORDON: After\nfoundation models, we'll take a look\nat AI frameworks that we use to build them. So we'll talk about JAX, which\nis really great for researchers. We'll talk about Keras, which is\nreally amazing for applied AI. Later on, we'll even\ntalk a little bit about the work we're\ndoing with PyTorch. JOANA CARRASQUEIRA: We'll\nalso touch upon some developer tools for all\ntypes of experience from beginners to advanced. JOSH GORDON: Then we'll talk a\nlittle bit about infrastructure. And this talk is about\nsoftware not hardware.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_001",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 127.49,
    "timestamp_end": 248.54,
    "text": "JOANA CARRASQUEIRA: We'll\nalso touch upon some developer tools for all\ntypes of experience from beginners to advanced. JOSH GORDON: Then we'll talk a\nlittle bit about infrastructure. And this talk is about\nsoftware not hardware. Our hardware infrastructure\nis TPUs, which you've probably heard a lot of. But in this talk,\nI'll briefly talk about XLA, which is a\nmachine learning compiler. And I'll talk about\nsome of the work we're doing for\ninference-- so making it possible to serve\nmodels at scale super efficiently with really\ncool new things with XLA for JAX and PyTorch. JOANA CARRASQUEIRA: OK. JOSH GORDON: Oh, and then\none more thing to mention. I went too fast. So sorry about that. So a lot of this talk is about\nthese huge foundation models. Towards the end of the talk,\nI'll talk about Google AI Edge. And we'll talk about\ndeploying small models on device, which is also super\nimportant for many reasons. JOANA CARRASQUEIRA: Awesome. OK. Let's start by exploring\nour core intelligence within our stack. We'll start with\nour Gemini models, which are our most capable\nand versatile model family. And our core philosophy here at\nGoogle is to provide developers with state-of-the-art models and\ntools that you can use to build powerful applications\nall throughout. And our Gemini models, they\nare known for being multimodal, have a long context window, and\nhaving very powerful reasoning, but we've built a variety of\nmodels for different use cases. So depending on what\nyou're trying to build, Google will have a model that\nis tailored for your use case. And I would like to just\ngive you a quick walkthrough of these models. I know you've heard\nit during the keynote, but, just very quickly,\nGemini 2.5 Pro, which is our most advanced\nmodel yet, especially for high complex tasks that\nbenefit from deep reasoning, it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_002",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 230.74,
    "timestamp_end": 380.44,
    "text": "it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it because of its\nefficiency and speed, and it's now even better at\nalmost every single dimension. So we improved\nall the benchmarks across reasoning, coding,\nmultimodality, and also long context. Then we have our Gemini 2.0\nFlash, which is fast and cheap, works fine, and our\nGemini Nano, which is optimized for on-device tasks. And as you've heard, we've\nbeen shipping relentlessly. And I would like to give\nyou just a quick highlight of everything that we've\nbeen shipping in AI Studio and the Gemini API. There's a talk\ntomorrow that I would like to invite you to attend,\nwhich is by Shrestha Basu Mallick, a group product\nmanager on Gemini API, and Luciano Martins, our\ntechnical lead for Gemini API from DevRel. And they're going to do a deep\ndive into everything that's new within the Gemini\nAPI, so you definitely don't want to miss\nthat session tomorrow. But for now, just a glimpse to\nget you excited about what's new in AI Studio. We've built a new tab\nthat is called Build that instantly generates web apps. And it's really cool, because it\nenables developers and builders alike to prototype very\nquickly with natural language. We have a new generative media\nexperience in AI Studio as well, and I'm going to\ndemo all of this so you can see how\nit actually works. And we are always\nlistening to the community. We listen to your\nfeedback, and we always build with developers in mind. And that's why that\nsome of these features were actually\nrequested by community, and that's what happened with\nthe built-in new dashboard. You requested. We built it. And we also have some\nnew Native Audio and TTS support in AI Studio. On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_003",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 363.97,
    "timestamp_end": 521.659,
    "text": "On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API and also on [INAUDIBLE] API\nfor real generating audio. And some of the use\ncases here that we had in mind when\nwe built this were more dynamic audiobooks,\nsome more engaging podcasts, or, even for those of\nyou in customer support, producing more natural\nvoices into your workflows. We also have enhanced\ntooling, which is really cool, because now you\ncan use Grounding with Google Search, also with Code Execution\ntogether in just one API call, and URL context, which you\nheard during the keynote, which provides the model with\ndepth content from web pages. And since you can chain\nit to other tools, it's actually really powerful\nto build search agents. So really cool stuff. Lastly, just to call out that\nwe now offer Gemini SDK support for MCP, which reduces a\nlot of developer friction and simplifies building\nagentic capabilities. So you don't want to\nmiss tomorrow's talk to learn more about this. And Google AI Studio. Who here uses Google AI Studio? Can I see some hands? OK, awesome. Awesome. We have a lot to chat\nafter this session then. Google AI Studio is the\nperfect place for anyone to start developing with AI. It's the simplest way to test\nthe latest DeepMind models. We typically bring them\nto Google AI Studio so you can start prototyping\nand playing with the models. And you don't need Google\nCloud knowledge in order to set up your environment. It is free of charge, and you\ncan create, and test, and save your prompts. And there's also starter\napps that will inspire you. And that's exactly what I'm\ngoing to demo here today for you. I am going to show\nyou some of the work that the team has been doing. So if we go to Google AI Studio\nhere, if we go into Build, and Mumble Jumble is\nsomething that we've literally just created, and\nit's one of my favorite apps. So Mumble Jumble is one\nof those applications that",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_004",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 502.44,
    "timestamp_end": 656.4300000000001,
    "text": "So if we go to Google AI Studio\nhere, if we go into Build, and Mumble Jumble is\nsomething that we've literally just created, and\nit's one of my favorite apps. So Mumble Jumble is one\nof those applications that with just natural\nlanguage allows you to interact with\nthe model so you can build those more dynamic and\ninteractive audio experiences. So let's have a look. So, first of all, it uses 2.5\npreview native audio dialog. You can choose if it's easy to\ninterrupt or hard to interrupt. And then you can come and\npick your voice, hi pitch. So there's multiple things\nthat you can play with and then you can customize. I really like the fox, so let's\ngo ahead and pick the fox. Let's get the cowboy. Happy because we're\nat Google I/O. We're all super\nhappy to be here. And let's get a microphone,\nbecause I have one. AI FOX: Howdy, partner. Finley Flicker Fox here ready\nto charm the boots off you. JOANA CARRASQUEIRA: Hello. What's the weather like\ntoday in Mountain View? AI FOX: Well now, it's a fine\nday in Mountain View, sunny and quite pleasant. Perfect for moseying\non out and about. JOANA CARRASQUEIRA: I agree. Are there any hiking trails\nnear the Google campus? AI FOX: Why sure as shooting. There are plenty of trails\nnear the Google campus, perfect for a grand\nadventure and some fresh air. JOANA CARRASQUEIRA:\n[SPEAKING PORTUGUESE] AI FOX: Well now, the best\nway to get to those trails is to head towards the foothills\nfollowing the marked paths. You'll find them\nright along the way. JOANA CARRASQUEIRA:\n[SPEAKING PORTUGUESE] AI FOX: You betcha. There are a few\ncozy cafes nearby, perfect for a spell\nof rest and a good cup of coffee after your hike. JOANA CARRASQUEIRA:\nThank you so much. AI FOX: It was my\npleasure, partner. Happy trails, and\nenjoy your coffee. JOANA CARRASQUEIRA: Thank you. So, as you could see, there's\nsome really cool experiences that we're bringing\ninto AI Studio. Audio is getting better; more\nnatural experiences with voice. And in case you\ndidn't notice, I even",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_005",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 641.14,
    "timestamp_end": 791.74,
    "text": "JOANA CARRASQUEIRA: Thank you. So, as you could see, there's\nsome really cool experiences that we're bringing\ninto AI Studio. Audio is getting better; more\nnatural experiences with voice. And in case you\ndidn't notice, I even changed the language in how\nI interacted with the model. I spoke in Portuguese,\nmy mother tongue, and it actually replied\nwith very good information. So what I did\nhere, Josh is going to show you exactly\nwhat is happening on the API side of things\nin just one second. I have one prompt that I just\nwant to very quickly show you. Sorry. Roll a dice twice, and\nwhat's the probability of the result being seven? OK, let's just run\nthis very quickly, because I just want to show\nyou one thing before I hand it over to Josh. So as you can see,\nthought summaries. The model is actually\nshowcasing how it thinks, and you can see\nthe summaries here. We have the result. And\nthen, basically, what is available in the UI\nin AI Studio is also available in the API. And Josh is going to\nshow you that right now. JOSH GORDON: Thanks. OK, great. So very briefly,\nwe have something called the Gemini Developer\nAPI, which is really great. It's the easiest possible way to\ndevelop with Google's foundation models. The best place to get\nstarted is ai.google.dev. There is a whole lot of\ncapabilities in the API. It's got code execution. It's got function calling. I remember sitting down\nwith the team to build this from a blank piece of paper. Starting about two years\nago, we had basically you could prompt it with\ntext, and now we have image understanding,\nvideo understanding, but now we can also\ngenerate images and videos that Joana will show you later. Very, very briefly,\nai.google.dev has all of our\ndeveloper documentation. There's lots of\nreally great guides, there's information about\nthe models, everything you need to get started. We also have the\nGemini API Cookbook. We have a link to\nthis at the end. It's basically goo.gle/cookbook. G-O-O.G-L-E/cookbook. And this will take you to\na whole slew of notebooks",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_006",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 777.32,
    "timestamp_end": 892.14,
    "text": "there's information about\nthe models, everything you need to get started. We also have the\nGemini API Cookbook. We have a link to\nthis at the end. It's basically goo.gle/cookbook. G-O-O.G-L-E/cookbook. And this will take you to\na whole slew of notebooks that the team has put together. And basically, all\nof these notebooks are end-to-end examples\nthat show you one thing that you might be interested\nin, like what's the best way to do code execution? What's the best way to\ndo function calling? You'll find that\nin the cookbook. I also very, very quickly\nwant to show you how easy it is to get started with the API. So basically, in\nGoogle AI Studio, you don't need a credit\ncard or anything like that, in about a minute you can\njust click Get API key. Create your key. Now, if you're doing\nthis for the first time, behind the scenes,\nthis will automatically create a Cloud project\nfor you, but that detail is not important. Basically, now I have an API key\nand I'm ready to install the SDK and call the model. If you open up any of the\nnotebooks in the cookbook, well, let's just say-- it's in a different\ndirectory here, but let's just say\nwe've opened up-- eh, we'll just say we\nopened up this one, which is in the quickstarts directory. And this shows you\nexactly what Joana showed, how to get the\nthinking summaries. You can add your API\nkey in Google Colab. If you zoom in, you\ncan hit Add new secret. And in this particular notebook,\nit's called Google API key. But you could call\nit whatever you like. So you would add\nGoogle API key there, you would paste your\nkey there, and now you're ready to run this. So if you do Runtime and Run\nall, you're calling the API and you're running\nall the examples. You can also, directly\nin Google Colab, we have this thing where you\ncan grab an API key straight inside Google Colab. So it's just really\nquick and easy to do. OK, we can go back\nto the slides. So very, very\nquickly, as a recap, Gemini Developer API is the\neasiest way to get started. It's super lightweight,\nit's fast to install,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_007",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 880.26,
    "timestamp_end": 979.77,
    "text": "So it's just really\nquick and easy to do. OK, we can go back\nto the slides. So very, very\nquickly, as a recap, Gemini Developer API is the\neasiest way to get started. It's super lightweight,\nit's fast to install, and you can get up and running\nhonestly in about a minute. OK? I will use the clicker. This is the flow to get\nstarted with Google AI Studio. Go to Google AI Studio. Get your key. Try one of the code\nexamples on ai.google.dev or in the cookbook. I see people taking pictures. That makes me happy. Please try this. We spent so much time\non making this easy, and I hope it works for you. If not, please file an\nissue and we'll get on it. This is the Gen AI SDK\nfor the Gemini API, and this is something\nwe've been rolling out. It's our latest SDK. We've been rolling\nit out gradually over the course of\nthe last six months. It's super user friendly. It's really easy to use. Really, the only point\nI want to make here, because I don't want\nto read the code examples or the\ndocumentation to you, you can call the API\nin a few lines of code. Basically add your key, select\na model, write a prompt, you can go ahead and call it. You can also get access\nto advanced functionality in one line of code. So if you'd like to get the\nthinking summaries that Joana showed you, you can just\nadd a ThinkingConfig, say include the thoughts. Now, you've got the\nthinking summaries. And a good use\ncase for this could be any time you need to\nexplain the model's reasoning. Particularly, you can\nimagine, if you're building an education\napp or a tutoring app, you can get the\nthinking summaries. In addition to\nreally cool things that you can do with\na single line of code, there's some more advanced stuff\nthat you can do with the SDK as well. So I know there's a lot\nof code on this slide, but we've talked a lot\nabout building agents and agentic experiences. In this example,\nyou could imagine that you have a Python\nfunction on your laptop called weather_function. And maybe that calls\nyour own weather server to get the weather.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_008",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 967.05,
    "timestamp_end": 1087.58,
    "text": "but we've talked a lot\nabout building agents and agentic experiences. In this example,\nyou could imagine that you have a Python\nfunction on your laptop called weather_function. And maybe that calls\nyour own weather server to get the weather. What you can do is you can pass\nthe definition of that function to the Gemini API\nin JSON, including the function name and the\nparameters that it takes. Then what you can do is\nyou can write a prompt. So here, the prompt happens\nto be \"What's the temperature in London?\" When you send the\nprompt and the function to the model, what\nthe model will do is assess whether it makes sense\nto call that function based on your prompt. If so, it won't\nactually call it. But you can see in the\nfunction_call.name that it returns and the\nfunction_call.args, it returns the name of the\nfunction and the arguments to pass to it. So if you want, you're\nready to call this function on your laptop. And we have code that you can\ncopy and paste to do that. What's really cool too is this\nworks with multiple functions at the same time. So you can imagine you have a\nfunction like schedule a meeting or something like that, and\nyou can very easily-- well, with some work, you could build\nan agent to actually do that. So function calling\nis super important, and it works extremely well. So now, Joana's going\nto talk about GenMedia. JOANA CARRASQUEIRA: Awesome. So as you can see, what\nyou can build in the UI within AI Studio, also\navailable in the API. And also, just building on the\ncapabilities of our foundation models, our core\nintelligence also encompasses a powerful suite\nof generative media models. And they are designed to\ntransform creative experiences across content generation\nacross different modalities like images, video, and audio. And I would also like to\ndemo one of these new apps that we have in AI Studio. So I'm going back to\nthe laptop, and I'm going to show you something\nthat the team also just created. So AI Studio also got a facelift\nand has some new features.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_009",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 1066.46,
    "timestamp_end": 1237.45,
    "text": "And I would also like to\ndemo one of these new apps that we have in AI Studio. So I'm going back to\nthe laptop, and I'm going to show you something\nthat the team also just created. So AI Studio also got a facelift\nand has some new features. And the chat interface\nis still the same, but you've seen the talk to\nGemini live during the keynote. We have the new\nGenerative Media Console, which allows you to\ncreate and interact with our most creative models. And then we have\nthe Build, which is where all these new\napps are coming to. So I just wanted to show\nyou very quickly this one. There we go. And then we basically\ncan choose here what are the sounds\nthat we want. And this is all powered by\nLyria, our music generation model. [GENERATED MUSIC PLAYING] And just for the\ninterest of time, I'm not going to\nkeep playing it. But you can see some of the\ncapabilities of these models that we're bringing\nto AI Studio. We'll continue to the slides. In the Console,\nas you could see, you have access to\nour image generation, our video generation, and music\ngeneration models with applets to get you started. And so that's a really\ncool thing for you to play with after this session. Some of our videos,\nvery realistic images with a really good understanding\nof real-world physics and dynamics, improved quality,\nand more and more capabilities coming to these models. And this is the example\nthat I just showed. We've made Lyria RealTime our\ninteractive music generation model, which powers MusicFX DJ. It's available in the\nAPI and AI Studio, and you can check\nour API documentation for more information. This also allows\neveryone to interact, to create, and to perform\ngenerative music in real time. It's really cool. You might remember that in the\nshow before the first keynote, you might have\nseen this console. That's exactly why I wanted to\nshow you this particular app in this session. But there's a lot more that\nyou can try afterwards. And shifting the\ngears towards Gemma, earlier this year we\nreleased Gemma 3, which",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_010",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 1223.76,
    "timestamp_end": 1374.39,
    "text": "That's exactly why I wanted to\nshow you this particular app in this session. But there's a lot more that\nyou can try afterwards. And shifting the\ngears towards Gemma, earlier this year we\nreleased Gemma 3, which is our most advanced model. And it comes in four sizes,\n1B, 4B, 12B, and 27B, and offers developers the\nflexibility to optimize performance for\ndiverse applications, from efficient\non-device inference, to also scalable\nCloud deployment. And in particular, 4B, 12B, and\n27B is multimodal, multilingual, and has a long context\nwindow up to 128,000 tokens. And the fact that it's available\nin more than 140 languages is really cool, because\n80% of our users are actually outside\nthe United States. And you heard during\nthe keynote as well that MedGemma is our\nmost capable collection of open models for multimodal\nmedical text and image comprehension. It's a really good starting\npoint for building medical application, and it's\navailable in 4B and 27B. You can download the\nmodel and adapt it to your use case via\nprompting, fine-tuning, or agentic workflows. And we also announced Gemma 3n. It's optimized for on-device\noperation on phones, tablets, and laptops. And as you can\nsee, the Gemmaverse is booming with all these\nnew variants coming and being developed all the time. Chill Gemma DolphinGemma,\nnow MedGemma, SignGemma-- so many different\ncapabilities and options that it's truly exciting to see. And one last thing that we\nare really excited about is the fact that now\nwe brought to AI Studio the possibility to deploy\nthe Gemma models directly from AI Studio into\nCloud Run with one click. So you can use the\nGen AI SDK to call it and just requires\na two-line change. Change API key, change\nbase URL, and you're set. That's the easiest deployment. And now, Josh is going to\ntell you all about frameworks. JOSH GORDON: Thanks. OK, so we've talked a lot\nabout foundation models, Gemini and Gemma. Now, let's talk a little\nbit about the frameworks that Google and the\ncommunity use to build them. So a lot of cool stuff to cover.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_011",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 1363.027,
    "timestamp_end": 1474.6200000000001,
    "text": "JOSH GORDON: Thanks. OK, so we've talked a lot\nabout foundation models, Gemini and Gemma. Now, let's talk a little\nbit about the frameworks that Google and the\ncommunity use to build them. So a lot of cool stuff to cover. Let's start with the\neasiest possible way to get started to fine tune a model. So in the developer keynote,\nGus showed a version of Gemma that speaks emoji. And this is a language that\nhe came up with his daughter. One way to do that is you\ncould just prompt the model to speak emoji. And in a lot of cases, you\ncan get away with a prompt. But if you have a very\nlarge amount of data, or maybe you're building a\nreally serious application like something in health or\nmedicine, what you can do is you can fine tune\nthe model to work even better with your data. And a really, really\ngreat thing about this is the truth is it\nsounds complicated, but it's not in practice. All you really need is\na two-column CSV file. And here, what you're\nlooking at is something with a prompt and a response. And if you've got\na couple thousand rows using our framework Keras-- and Keras is my favorite way by\nfar of doing just applied AI. That means using AI in practice. You can tell I care a\nlot about-- both of us care a lot about\nhealth and medicine, so there's a lot of wonderful,\nmore than you could ever count, opportunities to do\ngood in the world in those fields using\ntechnologies like this. You can train the model to\ndo something really useful. So we have a really great\ntutorial about this. It's honestly about\nfive key lines of code. You import a model of\nGemma from Keras hub. This model is already\ninstruction tuned. You can prompt it\nin a line of code, and you can also\ndo LoRA fine-tuning in about a line of code, which\nalso sounds fancy, but it's not. So Keras is great\nfor Applied AI. If you're doing research,\nwe have a really wonderful framework called JAX. JAX is a Python machine\nlearning library, and I guess I have two\nthings to say about it. One is that at the\nhighest scales, JAX is the best place to go.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_012",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 1459.79,
    "timestamp_end": 1567.51,
    "text": "If you're doing research,\nwe have a really wonderful framework called JAX. JAX is a Python machine\nlearning library, and I guess I have two\nthings to say about it. One is that at the\nhighest scales, JAX is the best place to go. So it scales really easily\nto tens of thousands of accelerators. It's super powerful. We use it to build\nGemini and Gemma. The community uses it to build\na bunch of really large, awesome foundation models as well. But one thing I like about JAX-- because I'm operating at\na much simpler level-- at its core, it's a Python\nmachine learning library with a NumPy API. And when a new model\ncomes out or a new paper, it takes me a long\ntime to understand it. What I like to do is\nbasically implement it line by line in NumPy, and I\nvery carefully just understand the input, the\noutput, the shapes, and debug it just in NumPy. And what's really\nwonderful, if you use JAX, you can do that in NumPy. There's transforms that\nyou can read about. You can add a line of code\nlike grad to get the gradients. You can add a line of code\njit to jit compile your model. And now, without\nchanging anything else, you can run it on GPUs and TPUs. So JAX core gives you\nthis really good way to think very carefully through\ndifferent techniques in machine learning. And then, when you're\nready, you can scale them up without really\nchanging your code. And that's really,\nreally awesome. On top of JAX, which is\nout of scope for this talk, there's a huge\necosystem of libraries. So there's great\nlibraries for Google in the community for things like\noptimizers, and checkpoints, and implementing\nneural networks. So you don't have to do that\nfrom scratch if you want. But just as I'm\nlearning things, if you do it totally from\nscratch once, you really can-- at least it helps\nme get my head around it, even though it takes\na little while. If you want to skip\nthat part and you want to go straight\nto just show me a super optimized large language\nmodel implemented in JAX that's ready to scale\nto hundreds or even",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_013",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 1555.152,
    "timestamp_end": 1662.2379999999998,
    "text": "even though it takes\na little while. If you want to skip\nthat part and you want to go straight\nto just show me a super optimized large language\nmodel implemented in JAX that's ready to scale\nto hundreds or even thousands of\naccelerators, then there's two really cool GitHub\nlibraries that I'd point you to. MaxText, as you might guess,\nhas reference implementations of large language models. And MaxDiffusion has,\nas you might guess, reference implementations\nand models that you can use to\ngenerate beautiful images and stuff like that. Those can take some\nwork, but we're working on making them\nsuper user friendly. But right now,\nthey're designed for-- I think the way I\nthink about it is-- well, anyway, they take\nsome work to scale, but they're great. Using JAX-- this just\ncame out yesterday. I wanted to point you to\nnew, really amazing work from the community. And so we've been talking about\nGoogle's foundation models. This is a new foundation model\nthat Stanford University just released. This is called Marin. It happens to be built with\nJAX and TPUs, which is great. But what's really\nspecial about it is that Marin is a\nfully open model. And so, in addition to sharing\nthe weights in the architecture, they've shared the data sets\nthat they use to train it, the code they use to\nfilter the data sets, the experiments that worked, the\nexperiments that didn't work. So this is a really great\nfoundation for open science and building these really\ncool models in the open. And they trained this model\nusing Google's TPU Research Cloud. And this is a collection of TPUs\nthat if you're a researcher, you can apply for access to. And it's basically a\nfree-of-charge cluster of TPUs that you can use to do really\ncool research like this. Very briefly, we talked\nabout doing LoRA training-- or, excuse me, LoRA\npost-training in Keras. And now, I'll show you a\nlittle bit about what we're working on for tuning in JAX. So we're working on a\nnew library called Tunix. And the vision here-- it's\nvery, very early stage--",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_014",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 1651.95,
    "timestamp_end": 1751.97,
    "text": "or, excuse me, LoRA\npost-training in Keras. And now, I'll show you a\nlittle bit about what we're working on for tuning in JAX. So we're working on a\nnew library called Tunix. And the vision here-- it's\nvery, very early stage-- we're building it\nwith the community. So we're working\nwith researchers from these great universities. And the vision is to\nmake it a really easy to use library for developers\nbut also a really good framework for researchers to implement the\nlatest post-training algorithms in JAX. And we're working\non a bunch now. I think it's going\nto be really good. And stay tuned. So that's Tunix. In addition to the libraries,\nvery briefly I just want to talk about\ninfrastructure. So TPUs, hardware, out of scope. But there's a really\ncool software package that I wanted to briefly\nmention called XLA. And XLA, it's\nbasically a compiler for your machine learning code. The way this works\nis that when you use a library like JAX or Keras\nor TensorFlow, or even PyTorch, what you're doing is you're\nwriting code in Python. And then somehow it gets\ncompiled, and optimized, and run on GPUs and TPUs. And XLA is the compiler that\nwe use at Google to do that. It powers our entire\nproduction stack. It's used by some of the\nlargest large language model builders in the world. And what it does is it\ntakes your Python code, does a whole bunch\nof optimizations, and gets it ready to\nrun on accelerators. One thing that's really cool\nabout XLA is it's portable. So if you run an XLA, you're\nnever locked into TPUs. You can use your\nexact same code to run on GPUs and other\ntypes of accelerators. So it's really great for that. We like it a lot. The important thing here is that\nPyTorch now also works with XLA. So if you're a\nPyTorch developer, it has a wonderful ecosystem,\nreally great libraries. If you want, you can use PyTorch\nXLA to train your models on TPUs and get all the really good\nprice performance benefits that come with that. In addition to\ntraining models, we've done great work with\nthe vLLM community.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_015",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 1739.61,
    "timestamp_end": 1851.45,
    "text": "If you want, you can use PyTorch\nXLA to train your models on TPUs and get all the really good\nprice performance benefits that come with that. In addition to\ntraining models, we've done great work with\nthe vLLM community. So now, you can also\nserve your PyTorch models using vLLM on TPUs. And vLLM is a super\npopular inference engine. We've added TPU\nsupport, so that's available to PyTorch\ndevelopers now as well, and we're also working on\nadding JAX support to vLLM. Here's some more really\ngreat work that's happening with the community. So this is a new partnership\nbetween Red Hat, NVIDIA, and Google. And it's working on a\nproject called LLM-d. And the vision here, this\nis for distributed serving. The vision here is\nto bring the very best of serving into\nopen source and make it available to everybody and\nto have this work with both JAX and PyTorch. So really cool new project. There's some more sophisticated\nstuff which you can check out. And stay tuned for this,\nit's going to be really good. OK. So at warp speed, we have\ntalked about basically foundation models Google has,\ndifferent frameworks that we use to train them, different\nways that you can serve them on the Cloud. Now, let's briefly look\nat how you can deploy them on mobile devices. The way that you would do this\nis using Google AI Edge, which is basically a framework for\ndeploying machine learning models on things\nlike Android, iOS, get them running in the browser,\nand also on embedded devices. And I know it's Google\nI/O. A lot of you are mobile developers,\nso a lot of this is probably intuitive to you. But if you're coming from\nI'm a Python machine learning developer, I work\nin the back end, this is all really\nawesome points. There's many good\nreasons why you might want to deploy on mobile. One is latency. So you can imagine, if\nyou're doing something like sign language recognition\nand maybe the user is holding up their hand and they're signing,\nyou don't want to drop frames. And if you're sending those\nframes to a server on the cloud,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_016",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 1840.035,
    "timestamp_end": 1931.482,
    "text": "So you can imagine, if\nyou're doing something like sign language recognition\nand maybe the user is holding up their hand and they're signing,\nyou don't want to drop frames. And if you're sending those\nframes to a server on the cloud, unless you happen to have\nthe world's fastest internet connection, you're probably\ngoing to drop frames. But if you have that gesture\nrecognition model running locally, you're not going to. So that's one huge advantage. Others, of course, are privacy. Data doesn't need\nto leave the device. I mean, a lot of this offline. I know this is obvious\nto mobile folks, but if you're working\non an airplane, maybe you want to run your\nmachine learning model there. Cost savings is a really\nimportant one too. So if you're serving a model\nto lots of users on the cloud, you might be paying\nfor the compute that you need to serve it. But of course, if it's\nrunning on the phone, the compute's happening\nlocally, so you don't need to bother with\nserving infrastructure. There's a lot of really cool\nnew stuff in Google AI Edge. On our side, we've added support\nfor things like the latest Gemma models. And by the way, this is for both\nclassical machine learning-- well, \"deep learning,\" which is\nnow suddenly becoming classical. Things like gesture recognition,\nwhich were state-of-the-art four years ago, now that's classical\nML because we're talking about large language models\nand generative AI. But you can run small large\nlanguage models on device. We have a new really awesome\ncommunity with Hugging Face, and there's a lot of really\nsmart people putting together models that are ready to\nrun pre-optimized on device. And we have a private\npreview-- this is coming soon-- for AI Edge Portal, which is\nbasically a testing service. So you submit your model\nto a cloud service, and it runs it on a fleet of\nreal devices of different sizes just to verify that\nit works really well. So if you're interested\nin mobile development, check out Google AI Edge. Google AI Edge,\nit's really cool.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_017",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 1920.12,
    "timestamp_end": 2074.09,
    "text": "and it runs it on a fleet of\nreal devices of different sizes just to verify that\nit works really well. So if you're interested\nin mobile development, check out Google AI Edge. Google AI Edge,\nit's really cool. And with that, I'll\nhand it over to Joana to talk about what's next. JOANA CARRASQUEIRA: Awesome. Thank you, Josh. And you've heard it in the\nkeynotes in the previous session with Demis and Sergey,\nwe're pushing the boundaries of what's possible to build with\nAI here at Google and Google DeepMind. And we're really excited to\nbring all this innovation and put it in the\nhands of developers, in the hands of the community. And it's never been a better\ntime to build and co-create together. So we really believe\nin a future where AI is changing various fields\nacross scientific discovery, health care, and so many more. And we're going to achieve\nthis radical abundance in a safe and\nresponsible way, and we want to get there with\nyou, with the community. So let's have a look\nat some of the domains that we believe that have a\nhuge potential for developers and humanity at scale. AlphaEvolve, a\nGemini-powered coding agent for designing\nadvanced algorithms. A self-improving coding agent. And we all know that\nlarge language models can summarize documents. They can generate code. You can even\nbrainstorm with them. But with AlphaEvolve,\nwe're really expanding these\ncapabilities, and we are targeting fundamental\nand highly complex problems on\nmathematics and coding. AlphaEvolve leverages\nGemini Flash and Pro, and it's one of the big\npromises for the future. Another one-- and I'm really\nexcited about AI co-scientist. It's another\nscientific breakthrough that we're seeing, especially\nin the medical, in medicine, and research fields. And our goal is to accelerate\nthe speed up discovery and drug development. And with AI\nco-scientist, a scientist can give a research goal to\nthe agent in natural language, and then the AI\nco-scientist is designed to give you an overview, and a\nhypothesis, and a methodology.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_018",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 2055.719,
    "timestamp_end": 2210.57,
    "text": "development. And with AI\nco-scientist, a scientist can give a research goal to\nthe agent in natural language, and then the AI\nco-scientist is designed to give you an overview, and a\nhypothesis, and a methodology. So in order to do so, it uses\na coalition of different agents that work together. And we have the generation,\nagent review, ranking, evolution, proximity,\nand meta review that are all created\nwithin the inspiration and driven from the\nscientific method in itself. So it's another huge\nbreakthrough and another domain that we'll continue\nto see evolving here at Google DeepMind. And lastly, an area where we're\nseeing tremendous progress and we expect to continue\nhaving more future breakthroughs is in domain-specific models. And Gemini robotics\nmodels, which are currently in private early access, are\nadvanced vision, language, action models with the\naddition of physical actions as a new output\nmodality specifically for controlling robots. These models are robot agnostic,\nand it uses multi-embodiment which is a technique that can be\nused on anything from humanoids to large-scale\nindustrial machinery. So this is really,\nreally exciting. And Gemini robotics has been\nfine tuned to be dexterous. And that's why you can see so\nmany different cool use cases and applications here on stage,\nfrom folding an origami, which is something a bit more complex,\nand just holding a sandwich bag. So many new innovations\nare coming to you, are coming to life,\nand we'll continue pushing the boundaries of\nwhat's possible across all these different domains. And now, if you\nwant to learn more, there's many ways that you\ncan keep engaging with us, that you can keep\ngiving us feedback. We're also active\non social media, and we have a developer forum\nwhere you can interact directly with Googlers. So in order to learn\nmore, Josh, what do our developers have to do? JOSH GORDON: We have\njust a few links for you. So no problem. But we talked about a lot of\ndifferent tools in the stack. JOANA CARRASQUEIRA: We did.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "4TE-KFXvhAk__chunk_019",
    "schema_version": "canonical_v1",
    "video_id": "4TE-KFXvhAk",
    "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
    "timestamp_start": 2201.15,
    "timestamp_end": 2283.1,
    "text": "So in order to learn\nmore, Josh, what do our developers have to do? JOSH GORDON: We have\njust a few links for you. So no problem. But we talked about a lot of\ndifferent tools in the stack. JOANA CARRASQUEIRA: We did. JOSH GORDON: So I don't\nwant to read the slide, but let me just point you\nto a couple highlights. ai.google.dev is the\nbest place to go to get started with Gemini and Gemma. We have a cookbook for Gemini. We have a cookbook for Gemma. Google AI Studio is\naistudio.google.com. If you're interested in JAX\nand Keras, there are the links. If you happen to be\ninterested in XLA, please check it\nout-- openxla.org. Google AI Edge is\nat the very bottom if you're a mobile\ndeveloper and you're interested in mobile deployment. And just to be clear, there's\nso many amazing things in the Google AI stack we didn't\nhave time to talk about today. Vertex has really amazing tools\nfor enterprise developers. But please, start\nhere, have fun, and, yeah, we're\naround after the talk. JOANA CARRASQUEIRA:\nYes, absolutely. And the Developer Relations\nteam is just outside. We have some really\ncool demo stations that you can experience. Engage with the team. Check out the sessions\ntomorrow, especially on the Gemini API,\nGemmaverse, and robotics. We have a lot of\ncool stuff that we want to put in the\nhands of developers. Many early access\nprograms as well. Stay in touch, stay\nengaged, and let's co-create the future of AI together. Thank you so much. JOSH GORDON: Thanks a lot. Thanks [APPLAUSE] [MUSIC PLAYING]",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_000",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 0.0,
    "timestamp_end": 124.52999999999999,
    "text": "[MUSIC PLAYING] ALEX KANTROWITZ: All\nright, everybody, we have an amazing\ncrowd here today. We're going to be\nlive-streaming this. So let's hear you make\nsome noise so everybody can hear that you're here. Let's go. [CHEERING] Not bad. I'm Alex Kantrowitz I'm the host\nof \"Big Technology\" podcast, and I'm here to speak with you\nabout the frontiers of AI with two amazing guests. Demis Hassabis, the CEO\nof DeepMind, is here. Google DeepMind. Good to see you, Demis. DEMIS HASSABIS:\nGood to see you too. ALEX KANTROWITZ: And we\nhave a special guest. Sergey Brin, the co-founder\nof Google, is also here. [CHEERS, APPLAUSE] All right. So this is going to be fun. Let's start with\nthe frontier models. Demis, this is for you. With what we know today\nabout frontier models, how much improvement is\nthere left to be unlocked, and why do you think so\nmany smart people are saying that the gains\nare about to level off? DEMIS HASSABIS: I think we're\nseeing incredible progress. You've all seen it today, all\nthe amazing stuff we showed in the [INAUDIBLE] keynote. So I think we're\nseeing incredible gains with the existing techniques,\npushing them to the limit. But we're also inventing new\nthings all the time as well. And I think to get all\nthe way to something like AGI may require one or\ntwo more new breakthroughs. And I think we have\nlots of promising ideas that we're cooking\nup and we hope to bring into the main\nbranch of the Gemini branch. ALEX KANTROWITZ: All right. And so there's been this\ndiscussion about scale. Does solve all problems,\nor does it not? So I want to ask you, in terms\nof the improvement that's available today, is\nscale still the star, or is it a supporting actor? DEMIS HASSABIS: I think I've\nalways been of the opinion that you need both. You need to scale to the\nmaximum the techniques that you know about. You want to exploit them to\nthe limit, whether that's data or compute scale. And at the same time, you want\nto spend a bunch of effort on what's coming next maybe six\nmonths, a year down the line,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_001",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 112.475,
    "timestamp_end": 235.85,
    "text": "that you know about. You want to exploit them to\nthe limit, whether that's data or compute scale. And at the same time, you want\nto spend a bunch of effort on what's coming next maybe six\nmonths, a year down the line, so you have the next innovation\nthat might do a 10x leap in some way to intersect with the scale. So you want both in my opinion. I don't know. Sergey, what do you think? SERGEY BRIN: I mean, I agree. It takes both. You can have algorithmic\nimprovements and simply compute improvements-- better chips,\nmore chips, more power, bigger data centers. I think that\nhistorically, if you look at things like\nthe n-body problem and simulating just\ngravitational bodies and things like that, as you plot it,\nthe algorithmic advances have actually beaten out the\ncomputational advances even with Moore's law. If I had to guess, I would say\nthe algorithmic advances are probably going to be\neven more significant than the computational advances. But both of them\nare coming up now, so we're getting the\nbenefits of both. ALEX KANTROWITZ:\nAnd Demis, do you think the majority\nof your improvement is coming from building\nbigger data centers and using more chips? There's talk about how the\nworld will be just wallpapered with data centers. Is that your vision? DEMIS HASSABIS: Well, no. Look, I mean, we're definitely\ngoing to need a lot more data centers. It's amazing that--\nit's still amazes me, from a scientific point\nof view, we turn sand into thinking machines. It's pretty incredible. But actually, it's not\njust for the training. Now we've got these models\nthat everyone wants to use. And actually, we're seeing\nincredible demand for 2.5 Pro. And I think Flash we're\nreally excited about how performant that is for the\nincredible sort of low cost. I think the whole world's going\nto want to use these things. And so we're going to need a\nlot of data centers for serving. And also for\ninference time compute You saw you saw\nDeepthink today-- 2.5 Pro Deepthink. The more time you give\nit, the better it will be.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_002",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 220.78,
    "timestamp_end": 330.92,
    "text": "I think the whole world's going\nto want to use these things. And so we're going to need a\nlot of data centers for serving. And also for\ninference time compute You saw you saw\nDeepthink today-- 2.5 Pro Deepthink. The more time you give\nit, the better it will be. And certain tasks, very high\nvalue, very difficult tasks, it will be worth letting it\nthink for a very long time. And we're thinking about how\nto push that even further. And again, that's going\nto require a lot of chips at runtime. ALEX KANTROWITZ: OK. So you brought up\ntest time compute. We've been about a year into\nthis reasoning paradigm, and you and I have\nspoken about it twice in the past as\nsomething that you might be able to add on to\ntraditional LLMs to get gains. So I think this is a pretty\ngood time for me to be like, \"What's happening?\" Can you help us contextualize\nthe magnitude of improvement we're seeing from reasoning? DEMIS HASSABIS:\nWell, we've always been big believers\nin what we're now calling this thinking paradigm. If you go back to our very early\nwork on things like AlphaGo and AlphaZero, our agent\nwork on playing games, they will all have this type\nof attribute of a thinking system on top of a model. And actually, you can\nquantify how much difference that makes if you look at\na game like chess or Go. We had versions of\nAlphaGo and AlphaZero with the thinking turned\noff that was just the model telling you its first idea. And it's not bad. It's maybe master level--\nsomething like that. But then, if you\nturn the thinking on, it'll be way beyond\nworld champion level. It's like a 600\nELO-plus difference between the two versions. So you can see that\nin games, let alone for the real world, which\nis way more complicated. And I think the gains\nwill be potentially even bigger by adding this thinking\ntype of paradigm on top. Of course, the challenge\nis that your models-- and I talked about this\nearlier in the talk-- need to be a kind\nof world model. And that's much harder\nthan building a model of a simple game of course.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_003",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 316.6,
    "timestamp_end": 445.65999999999997,
    "text": "bigger by adding this thinking\ntype of paradigm on top. Of course, the challenge\nis that your models-- and I talked about this\nearlier in the talk-- need to be a kind\nof world model. And that's much harder\nthan building a model of a simple game of course. And it has errors in it,\nand those can compound over longer term plans. But I think we're making\nreally good progress on all those fronts. SERGEY BRIN: Yeah, look,\nI mean, as Demis said, DeepMind really pioneered a lot\nof this reinforcement learning work. And what they did with AlphaGo\nand AlphaZero, as you mentioned, showed, as I recall, something\nyou would take 5,000 times as much training to match what\nyou were able to do with still a lot of training and the\ninference time compute that you were doing with Go. So it's obviously\na huge advantage. And obviously,\nlike most of us, we get some benefit by thinking\nbefore we speak, although-- ALEX KANTROWITZ: Not always. [LAUGHTER] SERGEY BRIN: I always\nget reminded to do that. But I think that\nthe AI's obviously are much stronger once\nyou add that capability. And I think we're just\nat the tip of the iceberg right now in that sense. It's been less than a year\nthan these models have really been around. DEMIS HASSABIS:\nEspecially if you think about, obviously with an\nAI, during its thinking process it can also use a bunch\nof tools or even other AIs during that thinking\nprocess to improve what the final output is. So I think it's going to be an\nincredibly powerful paradigm. ALEX KANTROWITZ: Deepthink\nis very interesting. I'm going to describe it-- I'm trying to describe it right. It's basically a bunch of\nparallel reasoning processes working and then\nchecking each other. And then it's like\nreasoning on steroids. Now, Demis, you mentioned that\nthe industry needs a couple more advances to get to AGI. Where would you put\nthis type of mechanism? Is this one of those that\nmight get the industry closer? DEMIS HASSABIS: I think so. I think it's maybe part\nof one, shall we say. And there are others\ntoo that we need",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_004",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 432.22,
    "timestamp_end": 550.7199999999999,
    "text": "Where would you put\nthis type of mechanism? Is this one of those that\nmight get the industry closer? DEMIS HASSABIS: I think so. I think it's maybe part\nof one, shall we say. And there are others\ntoo that we need to-- maybe this can be part\nof improving reasoning. Where does true\ninvention come from, where you're not just\nsolving a maths conjecture, you're actually proposing\none or hypothesizing a new theory in physics? I think we don't\nhave systems yet that can do that type of creativity. I think they are coming. And these types\nof paradigms might be helpful in\nthings like thinking and then probably\nmany other things. I mean, I think we\nneed a lot of advances on the accuracy of the world\nmodels that we're building. I think you saw that with\nVeo, the potential Veo 3. It amazes me how it\ncan intuit the physics of the light and the gravity. Having someone who-- I used to\nwork on computer games, not just the AI but also graphics\nengines in my early career, and remember having to\ndo all of this by hand, and program all of the\nlighting, and the shaders, and all of these things. Incredibly complicated stuff\nwe used to do in early games, And now it's just intuiting\nit within the model. It's pretty astounding. ALEX KANTROWITZ:\nI saw you shared an image of a frying pan with\nsome onions and some oil. [INTERPOSING VOICES] ALEX KANTROWITZ: There was no\nsubliminal messaging about that? DEMIS HASSABIS: Not really. Not really. Just maybe a subtle,\nsubtle message. ALEX KANTROWITZ: OK. So we said the word or the\nacronym AGI a couple of times. There's, I think, a movement\nwithin the AI world right now to say let's\nnot say AGI anymore. The term is so overused\nas to be meaningless. But Demis, it seems like\nyou think it's important. Why? DEMIS HASSABIS: Yeah, I\nthink it's very important. But I think maybe I need to\nwrite something about this also with Shane Legg, who's\nour chief scientist, who was one of the people who\ninvented the term 25 years back. I think there's two things\nthat are getting a little bit conflated.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_005",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 537.46,
    "timestamp_end": 648.49,
    "text": "But I think maybe I need to\nwrite something about this also with Shane Legg, who's\nour chief scientist, who was one of the people who\ninvented the term 25 years back. I think there's two things\nthat are getting a little bit conflated. One is, what can a typical\nperson do, an individual do? And we're all very\ncapable, but we can only do-- however capable\nyou are, there's only a certain slice of\nthings that one is expert in. Right? Or you could say, can you do\nwhat 90% of humans can do? That's obviously going to be\neconomically very important and I think from a product\nperspective also very important. So it's a very\nimportant milestone. So maybe we should say that's\ntypical human intelligence. But what I'm interested in,\nand what I would call AGI, is really a more\ntheoretical construct. Which is, what is\nthe human brain as an architecture able to do? And the human brain is an\nimportant reference point, because it's the only evidence\nwe have maybe in the universe that general\nintelligence is possible. And there, it would have\nto be able to-- you'd have to show your system\nwas capable of doing the range of things even\nthe best humans in history were able to do with the\nsame brain architectures. Not one brain, but the\nsame brain architecture. So what Einstein\ndid, what Mozart was able to do, what\nMarie Curie, and so on. And that, it's clear to me,\ntoday's systems don't have that. And then the other thing why I\nthink it's sort of overblown, the hype today on AGI, is\nthat our systems are not consistent enough\nto be considered to be fully general yet. They're quite general, So they\ncan do thousands of things. You've seen many\nimpressive things today. But every one of us have\nexperience with today's chatbots and assistants. You can easily,\nwithin a few minutes, find some obvious\nflaw with them-- some high school math thing\nthat it doesn't solve, some basic game it can't play. It's not very difficult to find\nthat those holes in the system. And for me, for something\nto be called AGI,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_006",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 633.76,
    "timestamp_end": 767.01,
    "text": "find some obvious\nflaw with them-- some high school math thing\nthat it doesn't solve, some basic game it can't play. It's not very difficult to find\nthat those holes in the system. And for me, for something\nto be called AGI, it would need to be consistent,\nmuch more consistent across the board\nthan it is today. It should take a\ncouple of months for maybe a team of\nexperts to find a hole in it, an obvious hole in it. Whereas today, it takes an\nindividual minutes to find that. ALEX KANTROWITZ: Sergey,\nthis is a good one for you. Do you think that AGI\nis going to be reached by one company and\nit's game over, or could you see Google\nhaving AGI, OpenAI having AGI, Anthropic having\nAGI, China having AGI? SERGEY BRIN: Wow,\nthat's a great question. I mean, I guess I would suppose\nthat one company or country or entity will reach AGI first. Now, it is a little\nbit of a spectrum. It's not a completely\nprecise thing. So it's conceivable that there\nwill be more than one roughly in that range at the same time. After that, what happens? I mean, I think it's\nvery hard to foresee. But you could certainly\nimagine there's going to be multiple\nentities that come through. And in our AI space,\nwe've seen when we make a certain\nkind of advance, other companies are quick\nto follow and vice versa. When other companies\nmake certain advances, it's kind of a\nconstant leapfrog. So I do think there's\nan inspiration element that you see. And that would probably\nencourage more and more entities to cross that threshold. ALEX KANTROWITZ: Demis,\nwhat do you think? DEMIS HASSABIS: Well, I\nthink we probably do-- I think it is\nimportant for the field to agree on a definition of AGI. So maybe we should try\nand help that to coalesce. Assuming there is\none, there probably will be some organizations\nthat get there first. And I think it's important\nthat those first systems are built reliably and safely. And I think after that,\nif that's the case, we can imagine using them to\nshard off many systems that",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_007",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 751.95,
    "timestamp_end": 874.91,
    "text": "will be some organizations\nthat get there first. And I think it's important\nthat those first systems are built reliably and safely. And I think after that,\nif that's the case, we can imagine using them to\nshard off many systems that have safe architectures sort of\nbuilt provably underneath them. And then you could have\npersonal AGIs and all sorts of things happening. But it's quite difficult,\nas Sergey says, to predict, sort of see beyond the\nevent horizon to predict, what that's going to be like. ALEX KANTROWITZ: Right. So we talked a little bit\nabout the definition of AGI. And a lot of people have\nsaid AGI must be knowledge-- the intelligence of the brain. What about the\nintelligence of the heart? Demis, briefly, does. AI have to have emotion\nto be considered AGI? Can it have emotion? DEMIS HASSABIS: I think it will\nneed to understand emotion. I don't know if-- I think it will be a sort of\nalmost a design decision if we want it to mimic emotions. I don't see any reason\nwhy it couldn't in theory. But it might be\ndifferent, or it might be not necessary, or,\nin fact, not desirable for them to have the emotional\nreactions that we do as humans. So I think, again, it's a bit\nof an open question as we get closer to this AGI\ntime frame of events, which I think is more than\na 5 to 10-year time scale. So I think we have\na bit of time. Not much time, but\nsome time to research those kinds of questions. ALEX KANTROWITZ: When I think\nabout how the time frame might be shrunk, I wonder\nif it's going to be the creation of\nself-improving systems. And last week, I almost\nfell out of my chair reading this headline about something\ncalled AlphaEvolve, which is an AI that helps\ndesign better algorithms and even improve\nthe way LLMs train. So, Demis, are you trying to\ncause an intelligence explosion? DEMIS HASSABIS: No, not\nan uncontrolled one. Look, I think it's an\ninteresting first experiment. It's an amazing system. We've got a a great\nteam that's working on that, where it's interesting\nnow to start pairing",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_008",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 862.76,
    "timestamp_end": 997.9499999999999,
    "text": "DEMIS HASSABIS: No, not\nan uncontrolled one. Look, I think it's an\ninteresting first experiment. It's an amazing system. We've got a a great\nteam that's working on that, where it's interesting\nnow to start pairing other types of\ntechniques, in this case evolutionary\nprogramming techniques, with the latest foundation\nmodels, which are getting increasingly powerful. And I actually want to see in\nour exploratory work a lot more of these kind of\ncombinatorial systems and pairing different\napproaches together. And you're right, that\nis one of the things, the self-improvement--\nsomeone discovering a kind of self-improvement loop-- would be one way where things\nmight accelerate further than they're even going today. And we've seen it before\nwith our own work, with things like AlphaZero\nlearning chess, and Go, and any two-player\ngame from scratch within less than 24 hours,\nstarting from random with self-improving processes. So we know it's possible. But again, those are in\nquite limited game domains which are very well described. So the real world is far\nmessier and far more complex. So it remains to be seen\nif that type of approach can work in a more general way. ALEX KANTROWITZ: Sergey, we've\ntalked about some very powerful systems, and it's a race. It's a race to\ndevelop these systems. Is that why you\ncame back to Google? SERGEY BRIN: I mean, I think,\nas a computer scientist, it's a very unique\ntime in history. Honestly, anybody who's\na computer scientist should not be retired right now. They should be working on AI. That's what I would just say. I mean, there's just\nnever been a greater sort of problem, an opportunity,\na greater cusp of technology. So I wouldn't say it's\nbecause of the race. Although, we fully\nintend that Gemini will be the very first AGI. Let me clarify that. But to be immersed in this\nincredible technological revolution, it's unlike-- I went through\nthe web 1.0 thing. It was very exciting, and\nwe had mobile, we had this, we had that. But I think this is\nscientifically far more exciting.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_009",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 976.22,
    "timestamp_end": 1121.52,
    "text": "But to be immersed in this\nincredible technological revolution, it's unlike-- I went through\nthe web 1.0 thing. It was very exciting, and\nwe had mobile, we had this, we had that. But I think this is\nscientifically far more exciting. And I think ultimately\nthe impact on the world is going to be even greater. Inasmuch as the web and mobile\nphones have had a lot of impact, I think AI is going to be\nvastly more transformative. ALEX KANTROWITZ: So what\ndo you do day to day? [LAUGHTER] SERGEY BRIN: I think\nI torture people like Demis, who's amazing. By the way, he tolerated\nme crashing this fireside. I'm across the street\npretty much every day, and people who are working\non the key Gemini text models, on the pre-training, on\nthe post training, mostly those. I periodically delve into\nsome of the multimodal work-- Veo 3, as you've all seen. But I tend to be pretty deep\nin the technical details. And that's a luxury I\nreally enjoy, fortunately, because guys like Demis\nare minding the shop. And yeah, that's just where\nmy scientific interest is. It's deep in the algorithms\nand how they can evolve. ALEX KANTROWITZ: OK. Let's talk about the\nproducts a little bit, some that were\nintroduced recently. I just want to ask you a broad\nquestion about agents, Demis. Because when I look at\nother tech companies building agents, what we see in\nthe demos is usually something that's contextually aware,\nhas a disembodied voice, is often interacted-- you often interact\nwith it on a screen. When I see DeepMind\nand Google demos, oftentimes it's\nthrough the camera. It's very visual. There was an announcement\nabout smart glasses today. So talk a little bit about-- if that's the right\nread-- why Google is so interested in having an\nassistant or a companion that is something that sees\nthe world as you see it. DEMIS HASSABIS: Well,\nit's for several reasons. Several threads come together. So as we talked\nearlier, we've always been interested in agents. That's actually the heritage\nof DeepMind actually. We started with agent-based\nsystems in games.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_010",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 1109.872,
    "timestamp_end": 1213.9099999999999,
    "text": "DEMIS HASSABIS: Well,\nit's for several reasons. Several threads come together. So as we talked\nearlier, we've always been interested in agents. That's actually the heritage\nof DeepMind actually. We started with agent-based\nsystems in games. We are trying to\nbuild AGI, which is a full general intelligence. Clearly, that would\nhave to understand the physical environment, the\nphysical world around you. And two of the massive use\ncases for that, in my opinion, are a truly useful\nassistant that can come around with\nyou in your daily life and not just stuck on your\ncomputer or on one device. We want it to be useful in your\neveryday life for everything. And so it needs to come\naround you and understand your physical context. And then the other big\nthing is, I've always felt for robotics to\nwork, you sort of want what you saw with\nAstra on a robot. And I've always felt that\nthe bottleneck in robotics isn't so much the hardware--\nalthough, obviously there's many, many companies and\nworking on fantastic hardware, and we partner with\na lot of them-- but it's actually the\nsoftware intelligence that I think is always\nwhat's held robotics back. But I think we're in a\nreally exciting moment now, where finally, with\nthese latest versions, especially 2.5 Gemini and more\nthings that we're going to bring in-- this kind of Veo\ntechnology and other things-- I think we're going to have\nreally exciting algorithms to make robotics finally work\nand realize its potential. Which could be enormous. So And then in the\nend, AGI needs to be able to do all of those things. So for us-- and that's\nwhy you can see we always had this in mind--\nthat's why Gemini was built from the beginning,\neven the earliest versions, to be multimodal. And that made it\nharder at the start, because it's harder to make\nthings multimodal than just text only. But in the end, I\nthink we're reaping the benefits of\nthose decisions now-- and I see many of\nthe Gemini team here in the front row-- of\nthe correct decisions we made.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_011",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 1203.7,
    "timestamp_end": 1325.8100000000002,
    "text": "because it's harder to make\nthings multimodal than just text only. But in the end, I\nthink we're reaping the benefits of\nthose decisions now-- and I see many of\nthe Gemini team here in the front row-- of\nthe correct decisions we made. They were the hardest decisions,\nbut we made the right decisions. And now, you can see the\nfruits of that with all of what you've seen today actually. ALEX KANTROWITZ:\nSergey, I've been thinking about whether to ask\nyou a Google Glass question. SERGEY BRIN: Fire away. ALEX KANTROWITZ: What did you\nlearn from Glass that Google might be able to apply\ntoday now that it seems like smart glasses\nhave made a reappearance? SERGEY BRIN: Wow. Yeah, a great question. I learned a lot. I mean, that was-- I definitely feel like\nI made a lot of mistakes with Google Glass,\nI'll be honest. I am still a big believer\nin the form factor, so I'm glad that we have it now. And now it looks\nlike normal glasses. It doesn't have\nthe thing in front. I think there was a\ntechnology gap honestly. Now, in the AI world, the\nthings that these glasses can do to help you out without\nconstantly distracting you, that capability is much higher. There's also just I didn't\nknow anything about consumer electronics supply chains\nreally and how hard it would be to build that and\nhave it be at a reasonable price point. Managing all the\nmanufacturing and so forth. This time, we have\ngreat partners that are helping us build this. So that's another step forward. What else can I say? I do have to say I miss the\nairship with the wingsuiting skydivers for the demo. [LAUGHTER] SERGEY BRIN: Honestly, it would\nhave been even cooler here at Shoreline amphitheater\nthan it was up in Moscone back in the day. But maybe we'll have to-- we should probably polish\nthe product first this time. DEMIS HASSABIS: This time. We'll do it that way\naround this time. SERGEY BRIN: Make sure\nit's ready and available. And then we'll do\na really cool demo. So that's probably a smart move. DEMIS HASSABIS: Yeah. What I will say\nis, I mean, look,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_012",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 1317.83,
    "timestamp_end": 1439.09,
    "text": "We'll do it that way\naround this time. SERGEY BRIN: Make sure\nit's ready and available. And then we'll do\na really cool demo. So that's probably a smart move. DEMIS HASSABIS: Yeah. What I will say\nis, I mean, look, we've got obviously an\nincredible history of glass devices and smart devices. We can bring all those\nlearnings to today. And I'm very excited about\nour new glasses, as you saw. But what I was always talking\nto our team and [INAUDIBLE] and the team about is that-- I mean, I don't know if\nSergey would agree, but-- I feel like the universal\nassistant is the killer app for smart glasses. And I think that's what's\ngoing to make it work. Apart from the fact that the\nhardware technology has also moved on and improved\na lot is I feel like this is the actual killer\napp, the natural killer app for it. ALEX KANTROWITZ: OK. Briefly, on video\ngeneration, I sat in the audience in the\nkeynote today and was fairly blown away by the\nlevel of improvement we've seen from these models. And you had filmmakers talking\nabout it in the presentation. I want to ask you,\nDemis, specifically about model quality. If the internet fills\nwith video that's been made with\nartificial intelligence, does that then go\nback into the training and lead to a\nlower quality model than if you were training just\nfrom human-generated content? DEMIS HASSABIS: Yeah. Look, we know there's\na lot of worries about this so-called\nmodel collapse. I mean, video is just one\nthing, but in any modality. Text as well. There's a few things\nto say about that. First of all, we're very\nrigorous with our data quality management and curation. We also, at least for all\nof our generative models, attach SynthID to them. So there's this invisible\nAI-made watermark that is very robust-- has held up now for a year, 18\nmonths, since we released it. And all of our\nimages and videos are embedded with this watermark. So we can detect, and\nwe're releasing tools to allow anyone to\ndetect, these watermarks and know that was an\nAI-generated image or video.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_013",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 1423.12,
    "timestamp_end": 1534.23,
    "text": "And all of our\nimages and videos are embedded with this watermark. So we can detect, and\nwe're releasing tools to allow anyone to\ndetect, these watermarks and know that was an\nAI-generated image or video. And of course that's\nimportant to combat deepfakes and misinformation, but\nit's also, of course, you could use that to filter\nout, if you wanted to, whatever was in your training data. So I don't actually see\nthat as a big problem. Eventually, we may\nhave video models that are so good you\ncould put them back into the loop as a source\nof additional data. Synthetic data, it's called. And there, you've just got to\nbe very careful that you're actually creating from\nthe same distribution that you're going\nto model, you're not distorting that distribution\nsomehow, the quality is high enough. We have some experience of this\nin a completely different way with things like AlphaFold,\nwhere there wasn't actually enough real experimental data\nto build the final AlphaFold, so we had to build an\nearlier version that then predicted about a\nmillion protein structures. And it had a confidence\nlevel on that, and we selected the top\n300,000-400,000 and put them back in the training data. So there's lots very\ncutting edge research to mix synthetic\ndata with real data. So there are also\nways of doing that. But on the terms of the\nvideo-generated stuff, you can just exclude\nit if you want to, at least with our own work,\nand hopefully other gen media companies follow suit and put\nrobust watermarks in also, obviously, first and\nforemost to combat deepfakes and misinformation. ALEX KANTROWITZ: OK,\nwe have four minutes. I got four questions left. We now move to the miscellaneous\npart of my questions. So let's see how many we\ncan get through in as fast as we can get through them. Let's go to Sergey\nwith this one. What does the web\nlook like in 10 years? SERGEY BRIN: What does the\nweb look like in 10 years? I mean-- DEMIS HASSABIS:\nYou've got one minute. SERGEY BRIN: Boy, I\nthink 10 years, because",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_014",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 1522.98,
    "timestamp_end": 1635.0919999999999,
    "text": "Let's go to Sergey\nwith this one. What does the web\nlook like in 10 years? SERGEY BRIN: What does the\nweb look like in 10 years? I mean-- DEMIS HASSABIS:\nYou've got one minute. SERGEY BRIN: Boy, I\nthink 10 years, because of the rate of progress in\nAI, is so far beyond anything we can see-- ALEX KANTROWITZ: Best guess. SERGEY BRIN: Not just the web. I mean, I don't know. I don't think we really\nknow what the world looks like in 10 years. ALEX KANTROWITZ: OK. Demis? DEMIS HASSABIS: Well, I\nthink that's a good answer. I do think the web\nin the nearer term is going to change quite a lot. If you think about\nan agent-first web, does it really need to-- it doesn't necessarily need to\nsee renders and things like we do as humans using the web. So I think things will be\npretty different in a few years. ALEX KANTROWITZ: OK. This is kind of an\nunder-over question. AGI before 2030 or after 2030? SERGEY BRIN: 2030. Boy, you really put\nit on that fine line. I'm going to say before. ALEX KANTROWITZ: Before? SERGEY BRIN: Yeah. ALEX KANTROWITZ: Demis. DEMIS HASSABIS: I'm just after. ALEX KANTROWITZ: Just after. OK. SERGEY BRIN: No pressure, Demis. DEMIS HASSABIS: Exactly. [LAUGHTER] DEMIS HASSABIS: I'll have to\ngo back and get working harder. ALEX KANTROWITZ: Is that-- SERGEY BRIN: I can ask for it. He needs to deliver it. DEMIS HASSABIS: Yeah, exactly. [LAUGHTER] SERGEY BRIN: Stop sandbagging. We need it next week. DEMIS HASSABIS: That's true. ALEX KANTROWITZ: I'll\ncome to the review. All right. So would you hire someone that\nused AI in their interview? Demis? DEMIS HASSABIS: Oh. In their interview? It depends how they used it. I think using today's\nmodels, tools, probably not. But I think that would be-- well, it depends how they\nwould use it actually I think is probably the answer. Sergey. SERGEY BRIN: I mean, I\nnever interviewed at all. So. [LAUGHTER] SERGEY BRIN: I don't know. I feel it would be hypocritical\nfor me to judge people exactly how they interview. DEMIS HASSABIS: Yeah, I\nhaven't either, actually. So [INAUDIBLE] on that.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_015",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 1622.46,
    "timestamp_end": 1740.3200000000002,
    "text": "SERGEY BRIN: I mean, I\nnever interviewed at all. So. [LAUGHTER] SERGEY BRIN: I don't know. I feel it would be hypocritical\nfor me to judge people exactly how they interview. DEMIS HASSABIS: Yeah, I\nhaven't either, actually. So [INAUDIBLE] on that. I've never done a job interview. SERGEY BRIN: Yeah. ALEX KANTROWITZ: OK. So Demis, I've been\nreading your tweets. You put a very\ninteresting tweet up where there was a\nprompt that created some sort of natural scene. DEMIS HASSABIS: Oh yeah. ALEX KANTROWITZ:\nHere was the tweet. \"Nature to simulation at the\npress of a button does make you wonder,\" with a\ncouple of emojis. And people ran with that and\nwrote some headlines saying \"Demis thinks we're\nin a simulation.\" Are we in a simulation? DEMIS HASSABIS: Not in the way\nthat Nick Bostrom and people talk about. I do think, though-- I don't think this\nis some kind of game, even though I wrote\na lot of games. I do think that ultimately\nunderlying physics is information theory. So I do think we're in a\ncomputational universe, but it's not just a\nstraightforward simulation. I can't answer\nyou in one minute. But I think the fact\nthat these systems are able to model real\nstructures in nature is quite interesting\nand telling. And I've been thinking\na lot about our work we've done with AlphaGo, and\nAlphaFold, and these types of systems. I've spoken a little about it. Maybe at some\npoint I'll write up a scientific paper\nabout what I think that really means in\nterms of what's actually going on here in reality. ALEX KANTROWITZ: Sergey,\nyou want to make a headline? [LAUGHTER] SERGEY BRIN: Well, I think that\nargument applies recursively, right? If we're in a simulation, then\nby the same argument, whatever beings are making\nthe simulation are themselves in a simulation\nfor roughly the same reasons, and so on and so forth. So I think you're going\nto have to either accept that we're in an infinite\nstack of simulations, or that there's got to be\nsome stopping criteria. ALEX KANTROWITZ: And\nwhat's your best guess?",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "Uh-7YX8tkxI__chunk_016",
    "schema_version": "canonical_v1",
    "video_id": "Uh-7YX8tkxI",
    "title": "Google I/O 2025 \u2013 Uh-7YX8tkxI",
    "timestamp_start": 1726.35,
    "timestamp_end": 1780.87,
    "text": "and so on and so forth. So I think you're going\nto have to either accept that we're in an infinite\nstack of simulations, or that there's got to be\nsome stopping criteria. ALEX KANTROWITZ: And\nwhat's your best guess? SERGEY BRIN: I think that we're\ntaking a very anthropocentric view when we say\nsimulation, in the sense that some kind of\nconscious being is running a simulation\nthat we are then in, and that they have\nsome kind of semblance of desire and consciousness\nthat's similar to us. I think that's where it\nkind of breaks down for me. So I just don't think\nthat we're really equipped to reason about one level\nup in the hierarchy. ALEX KANTROWITZ: OK. Well, Demis, Sergey,\nthank you so much. This has been such a\nfascinating conversation. SERGEY BRIN: Thank you. ALEX KANTROWITZ:\nAnd Thank you all. [APPLAUSE, MUSIC]",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_000",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 0.0,
    "timestamp_end": 144.54,
    "text": "[MUSIC PLAYING] DEVIN CHASANOFF: Hello, hello. MARK THOMPSON: We're\nso excited to be here. Welcome to our friends\nonline, and welcome to all of our friends in the room. This is \"What's new in Angular,\"\nor, as I originally called it-- I'm not kidding. This is true-- all\nthe fantastic updates coming to Angular\ndevelopers everywhere. But they made me change it. I wasn't too happy, but we'll\nstill keep this party going. Because you know what? Angular has had this incredible\nmoment, incredible moment in the developer world. We've been shipping features to\nimprove the developer experience for everyone. Listen, we've been\nworking hard to bring you the new features\nthat are so exciting and that enable you to build\nnew application experiences. And I'll be honest\nwith you, I get to do the work that\nI love, on a team that I love, with a community\nthat I love being a part of. So you all are fantastic. Give yourselves\naround of applause. [APPLAUSE] Yeah. It's OK to be happy\nfor yourselves. People are like, hmm. So we get to see the joy\nthat developers experience from improved APIs, better\nintegrations, and again, that overall better experience. Now, we take a lot\nof pride that we've worked so diligently to reduce\nthe pain points and making things easier for\nyou as developers. But say this next part with me. But wait, there's more. That's right. There's more. See, the Angular team is working\non something really special. And I bet that you\ncan guess what it is. It's the next version of\nAngular, Angular version 20. [APPLAUSE, CHEERS] So we're going to\nshare news with you across three different areas. We'll talk about new features. We'll talk about new\nintegrations and some of the other teams at Google\nthat we're partnering with. And we're also going to\ntalk about new directions. Where's the Angular team going. What other things are\nwe going to be doing? So to tell you all\nabout it, I need you to give me a just\nenormous round of applause to welcome Devin to the front. [APPLAUSE, CHEERS] DEVIN CHASANOFF:\nThanks, everybody.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_001",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 125.92,
    "timestamp_end": 281.53000000000003,
    "text": "What other things are\nwe going to be doing? So to tell you all\nabout it, I need you to give me a just\nenormous round of applause to welcome Devin to the front. [APPLAUSE, CHEERS] DEVIN CHASANOFF:\nThanks, everybody. What would a\nsession like this be without telling developers\nabout the one thing they care about most? That's right. You guessed it,\nintegration testing. I'm kidding. I'm kidding, new features. Oh, my God, they\nwere about to leave. It's OK, new features. And we know that building\napps that your users love goes hand in hand with the\ndelightful developer experience. I am just thrilled to\nshare what the Angular team has been up to lately. This team has been on\na roll, just cooking. And looking back at\nall this progress, I'm convinced that\nwhat I thought were groundbreaking\nfeatures at the time were really just paving\nthe way for even more incredible improvements. And today, I'm going to\ntell you the next part of the story for Angular\nSignals, server-side rendering, and authoring improvements,\nstarting with Angular Signals. When it comes to state\nmanagement in Angular, we've got a new mantra over\nhere on the Angular team. Work smarter, not harder,\nthanks to Angular Signals. Last year, we released\nsignals in developer preview. At a high level, Signals are\nAngular's modern approach to reactivity, a hassle free way\nto manage application state that reduces the amount of imperative\ncode that you have to write. As I like to put it, Angular's\nnew signal-based approach takes the management\nout of state management, which I love because when\nit comes to writing code, we're all really lazy. Wait, who wrote this? Busy, we're all really busy. We're not lazy. A signal is a wrapper around a\nvalue that notifies consumers when its value has changed. There are several\nvariations, including a signal, your basic\nsignal, as well as computed. Now, you might be\nasking yourself, hey, signals have\nbeen around since v17. Why are you bringing this\nup in a talk that was almost",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_002",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 265.52,
    "timestamp_end": 426.08,
    "text": "There are several\nvariations, including a signal, your basic\nsignal, as well as computed. Now, you might be\nasking yourself, hey, signals have\nbeen around since v17. Why are you bringing this\nup in a talk that was almost titled all the fantastic updates\ncoming to Angular developers? Well, for one,\nsignal and computed are now stable, which is a huge\nmilestone for the framework. And in v19, we\nintroduced linked signal. A linked signal is\na writable signal dependent on some other state. Another way to think about\nit, in its most basic form, is a computed signal that\nyou can set directly. But linked signal also\ngives you the ability to access prior\nstate, which makes it a game changer in\nthe signal ecosystem, as I'll illustrate\nin just a little bit. When you factor\nin linked signal, Angular Signals\ngive you the ability to create a reactive,\nself-managing graph of your application state,\nwhich means less development time, means fewer bugs, and\nsuperior maintainability. This really is a paradigm\nshift and a new way of thinking about state management. And a large part of\nthe reason for this, and I thought this\nwas so interesting, signals have no concept of time. Think about that. So rather than managing\napplication state in response to events that occur over time\nand tracking those events, you design a reactive\nsystem for a computing state at any given point in time. It's a subtle but\nreally important distinction that you begin\nto pick up on the more you work with signals. Oh, and did I mention\nlinked signal is being upgraded to stable in v20? Back in V19, we also introduced\na new experimental type of signal called a\nResource for reactively fetching asynchronous data. Or another way to think about\nit is a container for data fetched asynchronously. And as a reactive primitive,\nResource can update and respond to other signals\nchanging value, and it can notify downstream consumers\nwhen its value has changed. Let's look at an example\ninspired by an app I built earlier\nthis year that uses",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_003",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 401.55,
    "timestamp_end": 549.24,
    "text": "And as a reactive primitive,\nResource can update and respond to other signals\nchanging value, and it can notify downstream consumers\nwhen its value has changed. Let's look at an example\ninspired by an app I built earlier\nthis year that uses Gemini to create an interactive,\nchoose your own adventure graphic novel builder. And as a side note, if\nyou're interested in this, we open-sourced\nthe code so you can go to our angular/examples\nGitHub repository to check it out. Create your own graphic novels. And we also live-streamed\nus building this live, alongside our incredible\nAngular community. So you can check out our YouTube\nchannel if you want to see more. One of the app's features\nis generating images. Because, well, it wouldn't be a\ngraphic novel without graphics, now, would it? And in addition to\ndisplaying images, we also wanted to let users\nknow the loading status as well as give them the ability\nto retry failed requests. And great news, Resource handles\nall of this functionality right out of the box. Let's look at some code. Here we define a Resource. And it calls an\nasynchronous function to generate an\nimage using Gemini through the Genkit library. Resource has a\nparams field, which we've populated with a\nsignal called imgDescription. Any time this signal\nchanges, the Resource will run the\nasynchronous function defined in our loader field. The loader function takes\nthe params as a parameter, and here, you can see the\nloader function uses data in the params, our\nsignal, to call Genkit's asynchronous\nrunFlow method to generate an image with\nGemini and return the image URL. Resource exposes a\nstatus field as well as this convenient\nisLoading method. So in this case, we can show\na loading spinner to the user while the request is loading. And Resource also\nhas a reload method. So in the case of\nan error, which is represented by an empty\nstring in our application, we can show the user a piece\nof UI that, when clicked, retries the request. And finally, because the\nimage resource is effectively",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_004",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 534.73,
    "timestamp_end": 669.5200000000001,
    "text": "So in the case of\nan error, which is represented by an empty\nstring in our application, we can show the user a piece\nof UI that, when clicked, retries the request. And finally, because the\nimage resource is effectively just a container for our\nasynchronously fetched data, we can dynamically set\nthe image source, thereby rendering our beautiful image. And with that, we've created\nan entire image generation pipeline, complete with\nloading status, retry, and we did it all with a single\nresource and five lines of code. That's pretty cool. And, again, you can see\nthat we've done this by defining how application\nstate will be determined rather than writing code\nthat determines it ourselves. In addition to\ngenerating images, this app also generates\nthe story text for our graphic novel. Let's look at how we can combine\nResource with linked signal to accomplish our goal. MARK THOMPSON: Wait. Wait. Devin, did you generate\nthat story text with Gemini? Because I'm pretty sure that\nwe get bonuses every time we mention Gemini. DEVIN CHASANOFF: Oh,\nright, right, right. How's this? MARK THOMPSON: OK, better. DEVIN CHASANOFF: I know what\nyou're thinking, too subtle. Bam. How's that? MARK THOMPSON: Yeah,\nnow we're cooking. DEVIN CHASANOFF: All right. Anyhow, here you can\nsee our story resource that relies on changes\nto a user input signal to determine if we should\nfetch new story parts. And the function assigned\nto our loader field returns an array of\nthe latest three parts of our story, the latest three\npanels of our graphic novel. However, we're not just\ninterested in the latest three parts of the story. We want to show the\nusers the whole story. Because what's the fun in\ncreating a graphic novel if you can't just\nread the entire thing? And this is where linked\nsignal comes in handy. Using this signature\nof linkedSignal, we can reactively\nupdate its value to the result of the computation\nfield any time our source, in this case, our\nstoryResource.value, changes. And the computation field\nleverages the previous state",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_005",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 651.02,
    "timestamp_end": 814.2299999999999,
    "text": "Using this signature\nof linkedSignal, we can reactively\nupdate its value to the result of the computation\nfield any time our source, in this case, our\nstoryResource.value, changes. And the computation field\nleverages the previous state of our linkedSignal, appends\nthe most recent values returned from our server, and\ngives us the full story, all stored in this\nsingle linked signal. This simply wouldn't be\npossible with a standard signal. And Resource is such an\nintegral piece of the puzzle that we released several\nexperimental variations to streamline\ndeveloper workflows, starting with httpResource,\nwhich basically serves the same purpose as a resource,\nbut it's designed specifically for fetching data\nfrom HTTP endpoints. And this method signature\nis pretty slick. With httpResource,\nyou don't need to define params or\na loader function. Instead, you effectively\nembed your params directly into the request. So in this example, we'll make a\nrequest to the API user endpoint any time our current\nuser ID signal changes. And I also want to highlight a\nnew streaming resource pattern that we released in\nversion 19.2 that you can use to reactively\nupdate the UI as chunks are streamed from the server. With all of these improvements,\ndeveloper experience is only part of\nthe signal story. Because signals'\nfine-grained reactivity model can also provide\nsignificant performance benefits. For one, signals provide\nan efficient mechanism for computing state. And this gets even more\nexciting when we factor in our next major update. Introducing, Zoneless Angular. [CHEERS, APPLAUSE] Yeah. I should have stayed on that\nslide for an extra second. [LAUGHS] Well, historically, Angular\nhas relied on Zone.js to manage change detection\nand updating the DOM. And although signals\nprovide us with the ability to see exactly which pieces\nof UI need to be updated, Zone.js still runs\nchange detection in response to all\nbrowser events, not just the ones that result\nin updates to our templates. And that can lead to a\nlot of unnecessary checks.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_006",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 796.83,
    "timestamp_end": 948.07,
    "text": "to see exactly which pieces\nof UI need to be updated, Zone.js still runs\nchange detection in response to all\nbrowser events, not just the ones that result\nin updates to our templates. And that can lead to a\nlot of unnecessary checks. But not anymore,\nbecause in Angular v20, we'll be upgrading Zoneless\nAngular to developer preview so that developers can remove\nZone.js from their applications altogether. With this change, developers\nhave explicit control over when and how change\ndetection runs, for example, by running mark for\nchange or by leveraging signals' inherent\nknowledge of state changes. And they can trigger\nupdates more precisely, making Zoneless plus Signals\na powerful combination. Like I said, work\nsmarter, not harder. And while we're on the\ntopic of performance, let's shift our focus to\nserver-side rendering. SSR helps developers\nimprove website performance by rendering HTML on the server\nthat contains initial page state. And then once the HTML is\ndelivered to the browser, Angular initializes\nthe app and makes use of the data contained\nwithin the HTML. And since then, we've\nintroduced several pieces to the server-side\nrendering story. The first of which\nI'll be covering today is route-level\nrender mode config APIs to provide greater control\nwhen using hybrid rendering with server-side routing. Hybrid rendering has been\naround for a few years now and combines the benefits\nof server-side rendering, pre-rendering, also known\nas static site generation, as well as client-side rendering\nto optimize your Angular application. And in version 19, we\nintroduced the hybrid rendering config API to render different\nparts of your application using different strategies. And this new feature\ngives you complete control over your app's rendering to\nenhance performance, search engine optimization, and\noverall user experience, resulting in\ngreater flexibility, built-in internationalization,\nand a seamless dev server experience. That last one is actually\none of my favorites.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_007",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 929.68,
    "timestamp_end": 1081.3600000000001,
    "text": "over your app's rendering to\nenhance performance, search engine optimization, and\noverall user experience, resulting in\ngreater flexibility, built-in internationalization,\nand a seamless dev server experience. That last one is actually\none of my favorites. I just love being able to\nrun ng serve with an SSR app and get the full benefits\nof hot module replacement without having to rebuild. Oh, it's so nice, so nice. And we've got some\nnifty CLI commands that you can use\nto easily integrate this functionality into new\nand existing Angular apps. So once you're set up\nwith server routing, you can use the\nRender Mode field to declare which\nroutes should be rendered from the server\nas fully-populated HTML, pre-rendered at build time\nwith static-side generation, or rendered in the browser\nwith client-side rendering. There's even more to\nthe SSR story than that. In V19, we also introduced\nincremental hydration, which is why I've been taking\nthese small sips throughout. Stay hydrated, my friends. So, in case you\nneed a refresher, hydration is the\nprocess of restoring a server-side rendered\napplication in the client by doing things like reusing\nserver-rendered Dom structures, persisting application\nstate, and transferring application data\nthat was previously fetched on the server. Hydration improves\napplication performance by avoiding the extra work\nto recreate Dom nodes. With incremental\nhydration, you can leave sections of your\napplication dehydrated and incrementally trigger\ntheir hydration as needed. So this demo here shows\nincremental hydration in action for a\nserver-side rendered page. Components that are initially\ndehydrated are shown in gray. And at the beginning,\neverything besides that top bar is grayed out because we haven't\nyet downloaded the JavaScript. Then you can see the color\neffect applied as the user click event triggers hydration for\neach of these components. In Angular, we handle\nthis using defer blocks with hydrate triggers. As shown in this example, our\nlarge component isn't hydrated",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_008",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 1061.09,
    "timestamp_end": 1213.68,
    "text": "Then you can see the color\neffect applied as the user click event triggers hydration for\neach of these components. In Angular, we handle\nthis using defer blocks with hydrate triggers. As shown in this example, our\nlarge component isn't hydrated until after a user\ninteracts with it, and we can use a placeholder\nuntil that time comes. This is yet another opportunity\nfor Angular developers to improve application\nperformance by producing smaller initial bundles, while\nstill providing end users with an experience comparable\nto full application hydration. But wait, there's more. But wait, there's more. MARK THOMPSON: Devin,\ndid you just glitch? DEVIN CHASANOFF: No. Oh, no, no, I'm just\nreally, really excited about this next feature\nbecause in Angular V19, event replay is\nenabled by default. A common problem in server-side\nrendered apps in any framework is the gap that occurs between\na user event and the browser downloading and\nexecuting the code to actually handle that event. This visual demonstrates\nevent replay in action. So when the browser renders\nthe app for the first time, it hasn't yet downloaded\nthe JavaScript. And while it's downloading,\nthe user clicks the Add to Cart button four times. In the background,\nEvent Dispatch records all of these\nevents, and when the JavaScript responsible\nfor handling this event is done downloading,\nEvent Dispatch replays the events,\nresulting in all four items being added to our cart. Last May, we shared the\nEvent Dispatch library to address this\nexact use case, which you can enable by\nconfiguring your hydration provider with event replay. Shifting focus--\ndid anyone catch me breeze over hot module\nreplacement earlier? Well, let's take a\nsecond to talk about one of my favorite developer\nexperience improvements. Hot module replacement,\nor HMR, is a technique used by development servers to\navoid reloading the entire page when only part of an\napplication has changed so that you can see changes\nimmediately reflected in the browser. This can be extremely\nuseful when you're, say,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_009",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 1198.82,
    "timestamp_end": 1363.86,
    "text": "used by development servers to\navoid reloading the entire page when only part of an\napplication has changed so that you can see changes\nimmediately reflected in the browser. This can be extremely\nuseful when you're, say, updating HTML or\nCSS, but you want to preserve the state of your\napplication in the browser. Next, I'm thrilled to\nshare some updates related to authoring in Angular. The future, it's standalone. We've gotten a ton\nof positive feedback from the community about\nstandalone components. In V14, we introduced\nstandalone in developer preview, and this made it possible\nto build applications without NgModules\nfor the first time. And since then, standalone\nhas been stabilized. And starting in V19, we made\nstandalone true the default for components,\npipes, and directives. In addition to making it easier\nto learn Angular and manage projects, standalone\ncomponents have paved the way for several new features, like\ndeferrable views, simplified route level lazy loading,\nas well as the directive composition API. In V19.2, we also introduced\nsupport for untagged literals and Angular templates, so you\ncan take advantage of string interpolation with\nliteral expression right in your templates. This could be useful for things\nlike improved readability, dynamic class naming and\nstyling, enhanced string manipulation, inline expression\nevaluation, and more. Here, we're able to cleanly\ninterpolate the call width property into the\nclass list of this div. And with this setup, we can\neasily change the column width by updating the\ncall width signal, while maintaining the\nlayout class styling. And finally, have you started\nusing Angular's new let syntax? We are listening\nto your feedback, and the new let syntax solves\none of the community's most upvoted issues by\nenabling developers to define and reuse variables\ninside Angular templates. Is it just me, or did we\nship a ton of features? From signals to SSR and\nauthoring improvements, we're working hard to\nship what you need. [APPLAUSE] Thanks.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_010",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 1342.66,
    "timestamp_end": 1504.48,
    "text": "to define and reuse variables\ninside Angular templates. Is it just me, or did we\nship a ton of features? From signals to SSR and\nauthoring improvements, we're working hard to\nship what you need. [APPLAUSE] Thanks. I'm relatively new to this team,\nhaving joined earlier this year. One of the most impressive\nthings I've noticed is how deeply\ninvested this team is in making Angular the best\nframework on the planet for building web applications. I mean, just look\nat how many features we've shipped since\nthis time last year. And we're not done yet. We're still listening. We're still building,\nand Angular V20 is coming to you next week. We can't wait to share even\nmore exciting updates with you. And with that-- [APPLAUSE] Give it up. With that, I'm going\nto pass it back to Mark to talk about some\nexciting new integrations. MARK THOMPSON:\nAll right, thanks, Devin, fantastic\nwork, so, so exciting. So let's talk new\nintegrations because software is built in components of\nsystems that work together with one goal, to help your\nusers accomplish tasks, whatever they've set out to do. So let's explore some new\nintegrations that will empower Angular developers everywhere. Now in version 20, we're going\nto update our testing story with a pretty meaningful\nchange because we're going to be adding support for a\nvery popular third-party testing framework. But before we talk\nabout that, let's talk about how we got here. You see, previously, we used\nKarma as our test runner, and we would export the results. Everyone used it, and people\nhad a pretty good time. But Karma was deprecated, so\nit put us in a unique position with a unique opportunity. Because did it mean\nthat we could finally see the end of unit testing? [CLAPPING] [LAUGHTER] Woo. OK, so the people who clapped,\nyou did not pass my test. [LAUGHTER] But I appreciate that energy. Well no, no, we\ndefinitely could not be at the end of unit\ntesting, but it put us in opportunity for a\nfresh start because here's what we thought about.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_011",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 1484.56,
    "timestamp_end": 1615.04,
    "text": "OK, so the people who clapped,\nyou did not pass my test. [LAUGHTER] But I appreciate that energy. Well no, no, we\ndefinitely could not be at the end of unit\ntesting, but it put us in opportunity for a\nfresh start because here's what we thought about. The external developer community\nhas advanced testing so much, and there are so many\noptions out there. So we figured that we should\ntry to find the best options to present to the\nAngular community to help you achieve\nyour best development. So as we're searching\nfor a long-term solution, we knew that we had to\nchoose something that hit three major points for us. First, there needed\nto be a well-lit path because here on\nthe Angular team, we're very proud of our work\nthat we've done to always bring the community along with\nus whenever we introduce a significant change. Then we're looking for\nstrong community support because we really want to\nmake Angular a wonderful place to build web apps. But we had to choose a tool that\nhas strong community support so that we will have\nconfidence that it will be a long-lasting solution. And finally, it has to\nimprove the overall developer experience. There are so many\noptions to consider, but if it doesn't increase the\noverall developer experience, then it's just not going to fit\nfor our wonderful community. Developers have two\noptions right now. You can test in a\nnode-based environment, or you can test in a\nbrowser-based environment. And so for node testing,\nnode-based testing, we have experimental\nJest support. And if you want to do\nsomething in the browser, we have experimental\nsupport for Web Test Runner. But then we thought,\ncould we find something that could\npotentially let you test in node environments\nand in the browsers? Because at a high level, that\nwould solve some problems that some development\nteams are facing right now. So with that in mind,\nwe are happy to announce that in Angular\nversion 20, we're introducing support for Vitest,\nexperimental support for Vitest. [CHEERING] Whoo-hoo.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_012",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 1599.35,
    "timestamp_end": 1729.095,
    "text": "that some development\nteams are facing right now. So with that in mind,\nwe are happy to announce that in Angular\nversion 20, we're introducing support for Vitest,\nexperimental support for Vitest. [CHEERING] Whoo-hoo. We knew you would\nbe happy about that. We're not surprised. We heard you. You've been telling us\nthis for a while, that you want us to try this out. So now, as a developer\nand an Angular developer, you have three things\nthat you can choose from. If you want to do\nnode testing, you have experimental Jest support\nfor browser, Web Test Runner, and then you can try for\nnode in-browser testing support for Vitest. But here's where your\nexpertise comes into play because we need your help. You see, we aren't going\nto support all three in the long term. It's not realistic. We would love for you to\ntry these out, give us some feedback,\nand then that will help us to be guided\ntoward the right solution for this amazing community. And we'll let you know\nin an upcoming release. All right, now let's\nturn our attention to what's probably the most\nimportant metric for developers, aside from revenue, and that\npeople really care about for their applications,\nperformance. So let's say it again. OK, let's turn our\nattention-- no, just kidding. The Angular team is\nsteadfast in our approach to delivering features to\nhelp you create performance applications, but we\nnoticed that while we had great tooling for\ndebugging in IDEs, if you want to capture profiling data,\nit was a little bit challenging. So let's take a look at a very\ncommon tool in Chrome DevTools. That's the Flame chart. Now, Flame charts are\nincredibly helpful, but even if we're\nlooking at this example, look at the number of\nentries that you find there. There's so much data,\nso much robust data. There is so much opportunity. But it can be hard\nto figure out which parts are your\napplication and then which parts are from a different\nlibrary or for something else. You know what we thought, I\nwish there was a better way. DEVIN CHASANOFF: Mark,\nthere is a better way.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_013",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 1715.57,
    "timestamp_end": 1850.75,
    "text": "But it can be hard\nto figure out which parts are your\napplication and then which parts are from a different\nlibrary or for something else. You know what we thought, I\nwish there was a better way. DEVIN CHASANOFF: Mark,\nthere is a better way. MARK THOMPSON: Ah, yes,\nthere is, because we are so, so thrilled to announce that\nwe partnered with the Chrome DevTools team to create a\ncustom track in Chrome DevTools for Angular developers. [APPLAUSE] This is really cool. So in this new track, we\nsurface custom profiling data specific to you and your\nAngular applications in language that you are familiar\nwith and understand. You can quickly identify\nyour code from other code from maybe a\nthird-party library. Now the custom track\nis clearly labeled to give you notice at\na glance that you're viewing profile information\nspecific to Angular. But we didn't stop there. You see, the\nentries are actually color coded to give you even\nmore important information. For example, entry points like\nchange detection and component processing, those are blue, but\nthen, generated code is purple, and TypeScript code is green. Now the custom track is\neven more feature rich because you can click on\nany one of those entries and then get even more\ndata about that entry. So you're probably thinking,\nhow do I get started with this? First, you got to be\non Angular version 20, so be sure to update next\nweek when it comes out. Second, be on the latest\nversion of Chrome. Then when you head over\nto the profiling panel, you'll be able to see\nthis new custom track. Now we got to give a huge\nshout out and thank you to the Chrome DevTools team for\ntheir incredible partnership on this. By leveraging the power of the\nrecently introduced performance panel extensibility API,\nwe were able to create this awesome new feature. Now, this new integration\nwith Chrome DevTools will further empower\nyou to create applications and experiences\nthat your users will love. All right, if you like what\nwe've been talking about so far, let's hear some noise. [CHEERING]",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_014",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 1836.42,
    "timestamp_end": 1968.64,
    "text": "Now, this new integration\nwith Chrome DevTools will further empower\nyou to create applications and experiences\nthat your users will love. All right, if you like what\nwe've been talking about so far, let's hear some noise. [CHEERING] That's pretty good. That's pretty good. I love it, love it, love it. Let's talk about more. Let's talk about platforms. So as we continue our\nintegration story, there's another update\nwith the Google team that you may be familiar\nwith, and that's Firebase. Firebase has been really\non fire because they've had some amazing\nfeature releases and announcements like Genkit,\nFirebase App Hosting, and so much more. And our teams have\nbeen working together to improve the\ndeveloper experience. We all know about the data\nstory, but how can Firebase help you get your apps in\nfront of more users? Since Angular apps can\nnow be either client side only or full stack with SSR,\nyou need a deployment solution that works in both scenarios. Now, previously, you could\ndo something like this. You could use Cloud\nFunctions for Firebase to host your server-side\ncode, and then deploy your client-side\ncode using Firebase hosting. It worked, but it\nwasn't ideal, and we knew we could do a\nbetter experience. So we spent time working with\nthe engineers on Firebase to ensure that when you're using\nFirebase App Hosting to deploy your app, you have a\ntop-notch experience. Check this out. You can literally deploy\nyour Angular SSR app from the Firebase console\nwith a linked repository. And then those new rollouts\ncome as you add commits. And guess what? You get to leverage\nthe incredible power of the Firebase platform\nthat's powered by Google Cloud. But you could also create back\nends and configure deployments using the Firebase CLI. Now, no matter which\nroute you choose, the outcome is still the same. You can deploy\nupdates to production by leveraging\nautomatic rollouts, commit a change to\nthe configured branch, and then an automatic\nrollout will happen for you. So with Angular and\nFirebase App Hosting,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_015",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 1956.76,
    "timestamp_end": 2099.7400000000002,
    "text": "You can deploy\nupdates to production by leveraging\nautomatic rollouts, commit a change to\nthe configured branch, and then an automatic\nrollout will happen for you. So with Angular and\nFirebase App Hosting, you are positioned to take that\nnew app that you've been working on and share it with the world. All right, let's talk\nabout new directions. I think you're really\ngoing to like this. We believe that AI has\nalready changed the way that we work as developers. And what's wild is that\nthese are early days. We don't even know how\nthings will evolve over time. There's so much opportunity, so\nmany cool things on the horizon, and it feels like it\nwas just yesterday that I was in this very\ntent, this very room, watching my colleagues\nAddy Osmani and David East give a talk about AI\ntooling and development workflows. It feels like I was in here. I was in here. Look at that head. That's me, the same person. And I had a front row\nseat to experience and to learn about all the\nnew tooling and AI features, just like you now\nhave a front row seat to learn about what's going\nto happen with Angular and AI. Listen, we understand AI is\ntoo transformational for us to just force it into Angular. So we've been thinking about\nmeaningful ways that we can do-- we can incorporate it. So it is with much excitement-- I'm so excited. I'm going to tell you. I'm going to tell you-- so much\nexcitement that we are launching the new AI portal,\nAngular.dev/AI. Now on Angular.dev/AI, you'll\nbe able to find guidance, best practices, coding\nexamples, recipes in AI. And guess what? This is just the first\nversion that we're launching, and we're going to continue\nto grow this resource. Here's an example of the type\nof recipe that you'll find on Angular.dev/AI. Now it's so common to want to\nsimulate that streaming effect when you're getting a response\nfrom an LLM, that typing effect. Well, what if I\ntold you that you could do that with\nAngular's resource API and a single primitive? Let's talk about it. First, using the\nresource API, there's",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_016",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 2083.09,
    "timestamp_end": 2236.1400000000003,
    "text": "when you're getting a response\nfrom an LLM, that typing effect. Well, what if I\ntold you that you could do that with\nAngular's resource API and a single primitive? Let's talk about it. First, using the\nresource API, there's a new property called\nstream that you can specify on the configuration object. Stream takes an async function. Now, from there, we define a\nsignal that when it's returned, it represents the\ndata being streamed. You can update that\nsignal just as you would using the normal API interface. And then this is just the\nclient-side implementation. Let's see, how do we link\nthis to an LLM response? So on the server side, this\nis just an express route. We're using the Gemini API\nto create a content stream. Then, you can\nstream the response to the client just using the\nwrite function on the response API. Now for the sake of this talk,\nwe've abbreviated this example, but I bet you could guess where\nyou can find the completed working code, Angular.dev/AI. I appreciate that energy. All right, listen, we have\nlinks to multi-hour development sessions led by Devin\nand myself walking you through integrations\nand implementations with tools like Firebase\nAI logic and Genkit. And we've also taken\ncare that while we are showing lots\nof Google AI tech, we also give guidance that is\nagnostic to our technology. Now we think you're really\ngoing to like what we've done, and we can't wait for\nyou all to check out this new part of Angular.dev. There is still so much\nmore awesome to come. Let's pass it back\nover to Devin. DEVIN CHASANOFF: All\nright, now that we've talked about new features,\nnew integrations, and of course, exciting\ndevelopments in AI, let's shift our\nfocus to the future and discuss Angular's roadmap. As we progress into\n2025, we remain hyper focused on two\nimportant things-- improving the Angular\ndeveloper experience and improving\nframework performance. In the long term, we're\nthinking about some game-changing improvements, such\nas selector-list components,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_017",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 2220.2,
    "timestamp_end": 2371.41,
    "text": "hyper focused on two\nimportant things-- improving the Angular\ndeveloper experience and improving\nframework performance. In the long term, we're\nthinking about some game-changing improvements, such\nas selector-list components, which could pave the way for\neven more exciting new features like single-file components\nand even incremental adoption. Near term, we're\nfocused on delivering one of the community's most\nhighly requested feature to enhance the signal ecosystem. So in addition to\nthe signal types I've already talked\nabout today, we've also released different-- more\nsignals like input, output, model, and more. And yet, there's still one\npiece of the puzzle that's missing that I'm excited\nto share with you today, signal forms. [APPLAUSE] I knew you all would be\nexcited about this one. I'm not going to give\naway too much just yet, but I will say signal forms will\ngo a long way in rounding out the signal ecosystem, while\nalso making forms easier to work with in Angular. Today, we explored\nrecently released features, new integrations, and\nAngular's plan for the future. Through this discussion,\nI hope it's clear how hyper-focused we are\non developer experience and framework performance. However, I wanted\nto call out a theme that I noticed in looking\nat all these new features. Have you noticed that\neach of these features, they just seem to build on\neach other more and more? As we keep improving\nAngular, each change paves the way for new developer\nexperience and performance improvements. On the new features\nfront, I showed you how we made incredible\nprogress rounding out the signal ecosystem. And with Zoneless moving\nto developer preview, we can leverage signal's\ninherent state awareness to create highly\nperformant web apps. I also shared how we\nstrengthened Angular's SSR story with features like incremental\nhydration, a routing config APIs, and event replay. SSR, are you kidding me? This is awesome! And we've also been working\nhard to improve your developer experience by publishing\na variety of authoring",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "eIeJmYdYMQo__chunk_018",
    "schema_version": "canonical_v1",
    "video_id": "eIeJmYdYMQo",
    "title": "Google I/O 2025 \u2013 eIeJmYdYMQo",
    "timestamp_start": 2355.06,
    "timestamp_end": 2452.48,
    "text": "with features like incremental\nhydration, a routing config APIs, and event replay. SSR, are you kidding me? This is awesome! And we've also been working\nhard to improve your developer experience by publishing\na variety of authoring improvements, such as standalone\ncomponents, the new let syntax, and support for untagged\ntemplate-- untagged template literals. Mark shared a lot of exciting\nnew integration updates, including testing\nimprovements, a custom Angular track, and the Chrome\nperformance panel, and new ways to supercharge your\napps with the power of Firebase. Going forward, we're so\nexcited for you to check out Angular's new AI portal\nthat Mark shared, Angular.dev/AI so that you can\nlearn to create AI-powered apps that your users will love. To take advantage of all of\nthese new features we've covered today, upgrade to the\nlatest version of Angular, and don't forget to tune in to\nour Angular V20 developer event on May 29, at 9:00 AM\nPacific on YouTube. To everyone watching\nhere at Shoreline and watching the live\nstream, I got to tell you, don't sleep on this Angular team\nbecause the best is yet to come. Thank you all so much. [APPLAUSE] [MUSIC PLAYING]",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_000",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 0.0,
    "timestamp_end": 152.73999999999998,
    "text": "[MUSIC PLAYING] GUS MARTINS: Hi, everyone. Good morning. I'm very happy to be here. Welcome to my \"very dear\nto my heart\" session, What's New in the Gemmaverse? It's something we really, really\nwork very hard to bring to you. So one question that we get a\nlot is, what is Gemma, right? What's this new thing? We have Gemini,\nbut what's Gemma? Let me give you one step back\nand try to explain to you why we do all that we do. In Google DeepMind,\nwe, of course, try to push AI research\nas much as possible. But the important\nthing about that is-- and we really want\nto solve intelligence. How can we make\nsystems be intelligent? This is a very hard task. It's not something that\nyou can just pull it off. We need everyone to collaborate. And that's why we think\nthat's very important to make our research\navailable to everyone, to make it available\nto developers, to other researchers,\nto regular users, just so that it\nevolves with everyone, and everyone can collaborate. We've been publishing\ngreat work for a while. I guess you've seen AlphaFold,\nAlphaZero, AlphaStar, all of these Alpha-- AlphaEvolve, all of them. That's not the real name. But the idea is we\npush this technology because we want to make sure\nthat everyone benefits from it. And when we wanted to\ncreate an open model, the idea was, first,\nAI for everyone. Make sure that everyone\nhave access to it. And that's why\nbeing open matters. That's why we want everyone\nto be able to try the model, change the model, make it their\nversion, make it their own. That's very important to us. And another very important\nthing that we have in mind is making this as\nresponsible as possible. We know that these models,\nthey bring a lot of power. But we also would like to have\nthe same responsible perspective on it so that we go\nbeyond benefiting people, protecting people from the\nharms that they could cause. So that's the idea of--\nall these models, whenever we train them, we\nkeep responsibility as one of the main\nthings for all of them. And that's how we got to Gemma.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_001",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 137.68,
    "timestamp_end": 279.98,
    "text": "protecting people from the\nharms that they could cause. So that's the idea of--\nall these models, whenever we train them, we\nkeep responsibility as one of the main\nthings for all of them. And that's how we got to Gemma. Gemma is our family\nof open models. They are based on\nGemini technology. And they are available for\nyou in many flavors and sizes. I'm going to go into\nsome details soon. We launched them last year. And we've just completed\na one-year anniversary. It was a beautiful journey. Last year, we\npresented them here. So if you know me, you've seen\nme talk about them for a while now. When we released Gemma\n1, we had two sizes, which was a 2B and a 7B. But what does that mean? So when we say \"a model,\" a\nmodel is basically a very long math equation, right? Very long. And for each a\ntimes x, b times x-- for each one of these a, b, c,\nwe would say that's a parameter. When we say 2B, we say that\nthere are 2 billion parameters. So it's like a\ngigantic math equation, with 2 billion parameters. That's like a layman\nversion of what it is. Don't tell this to a researcher. But the idea is that 2B\nis a very large model, but it's kind of small\ncompared to other models. And we release also a 7B. And people really,\nreally like them. And since then, Gemma\nhas been downloaded over 150 million times. I guess we're close to 160,\nsince we created this slide. So things are moving super fast. We are very, very happy\nwith the results so far. Since our launch, we released\nmany variants ourselves. We created multiple versions. So for example, there's\na bunch of names here. Let me go a little bit--\nfor example, PaliGemma. PaliGemma was a model we\nreleased last year at Google I/O, right here, which\nis the first Gemma model that has vision. You could give an image\nand ask a question. And it was the best VLM,\nVisual Language Model, so far. It was great. That's why we did a PaliGemma\n2 in the end of the year, so we kept pushing. We also had other\nvariants, like ShieldGemma, which I'll talk\na little bit more",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_002",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 264.95,
    "timestamp_end": 407.81,
    "text": "And it was the best VLM,\nVisual Language Model, so far. It was great. That's why we did a PaliGemma\n2 in the end of the year, so we kept pushing. We also had other\nvariants, like ShieldGemma, which I'll talk\na little bit more later, which the idea\nis a safety classifier. So you can do-- I'll explain the details\nlater, but the idea is to make your AI systems even\nmore responsible as a whole. Aside from the regular\nvariants, we also published more core\nmodels, as we say. When we say core models,\nwe release Gemma 1, and then we release\nGemma 2, and then we release Gemma 3,\nbeginning of this year. And yesterday, we released\nGemma 3n that I'm going to talk a little bit more,\nin a little bit. And more than tools, we\nalso released some tools to go around our models. For example, we have\nGemma Scope and Gemma.cpp. Gemma Scope is some\ntooling for you to see inside of all\nthe layers of the models and help you interpret,\nwhy some results? You give a prompt. Why you have this result? As I said, these\nmodels are quite large. So understanding how you go\nfrom \"Luna loves strawberries\" to some emojis. It might be very\nhard to explain that. So Gemma Scope helps with that. Gemma.cpp is our implementation\nof our Gemma models completely on C++. Then you would say, oh,\nyeah, why would you do that? When we do that, it enables\nyou to run these models on CPU only and super fast. So if you have a computer that\ndoesn't have a strong GPU, which I guess the majority\nof people have that, you can still use these\nmodels only on CPU, with a very good performance. And the fun thing\nis, in March, we released Gemma 3, our\nmost capable model so far. We had four sizes-- 1B, 4B, 12B, and 27B. And they were all state\nof the art, which I'll go into details in a little bit. But the idea was, we've learned\na lot with the community. We've received a\nlot of feedback. We've learned from our own--\nfrom researchers, from everyone, and we want to make the\nbest models possible. So what came with Gemma 3? First of all, community\nreally liked for the models",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_003",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 389.85,
    "timestamp_end": 525.89,
    "text": "We've received a\nlot of feedback. We've learned from our own--\nfrom researchers, from everyone, and we want to make the\nbest models possible. So what came with Gemma 3? First of all, community\nreally liked for the models to see beyond just text. So, of course, Gemma\n3 is multi-modal. There's 4, 12, and 27B. They can all receive an image,\nand they can do more than that. They can do a conversation. So you can ask a question\nabout a meme-- an image-- and you can continue with\na multi-turn conversation. You can add more images. You can do all of that. That's part of the\nbase model already. More than that, one thing for\nGemma was always it was English only. But we got feedback from the\ncommunity for two things-- the model could generate\ntexts in other languages, and we were very\nsurprised, which was-- they had very high\nquality in other languages. But we always said,\nit's English only. So what we did is we, OK, let's\nmake it really multilingual. So Gemma 3 now supports\nover 140 languages. And it's quite good in many\nof them in the benchmarks, in all the ones that\nwe could run at least. So this is a very\nstrong capability. I guess, I don't know-- let me tell you a\nlittle secret here. I don't know if you\nnoticed, English is not my first language. Yeah. Can you believe that? [LAUGHTER] So for me, having a\nmodel that can really help in multiple languages\nand multiple cultures, it's really, really\nimportant, dear to my heart. As you can see, there's\na lot of users of Gemma that are not from US,\nand they don't speak English as their main language. Maybe I'm part of\nthat 64% over there. So the idea of having a model\nthat embraces multiple languages and cultures, part\nof what we want to make in terms\nof accessibility, right, that we want\neveryone to be part of it. And the other thing that\nwe improved for Gemma 3 is we made the\ncontext window longer. What is a context window? When you write a prompt\nto these models, this text you're sending, let's say\nthat's the context window.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_004",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 510.55,
    "timestamp_end": 656.32,
    "text": "And the other thing that\nwe improved for Gemma 3 is we made the\ncontext window longer. What is a context window? When you write a prompt\nto these models, this text you're sending, let's say\nthat's the context window. So the larger the context window\nof a model, the more information you can give per prompt\nand the longer you can do a conversation. The other important thing\nis, when you add an image-- for example, you give an image\nand ask, describe this image-- all of that eats\nup-- let's say, like, that's your context window. So the longer or the larger\ncontext window you have, the more your model can process. And you can also have\nlonger conversations. I guess 128, you can put a book. You can put multiple-- I guess 500 images. You can do a lot of stuff. You can put all these--\nyou can put a video and ask questions\nabout the video. So it's very, very powerful. And before we were at 8k. So the community complained. We improved that,\nas you can see here. So we were from 8k to-- the\n1B has 32k context window. And the 4-- it's 12 there,\nit shouldn't be 14-- 27B have 128k context\nwindow, which is great. The 1B has a shorter context\nwindow because it's aimed for smaller devices. One other aspect\nof context windows, of course, there is no magic. If you give a larger, more\nprompt, it eats up memory. So if you're using a 1B model,\nwe expect you to be using a weaker machine so you will\nhave less memory anyway. Otherwise, you'll go for\nthe 4B, which is stronger. Now, here's a fun graph of how\nthe models keep the quality, while we increase\nthe context window. And we say that the context\nwindow for our models is 128k. You can see it\nover there, right? But if you look at the green\none, the green is the 27B, the lowest line over there. If you try a 512k, it will\nstill do good results. But, of course, we\nbenchmark until 128. That's what we say it's good. But it can go a\nlittle bit further. [CHUCKLES] Let's see some\nexamples of how the model works. Here, I'm giving an image. And I'm asking, how\nmuch is this ticket?",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_005",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 639.32,
    "timestamp_end": 776.84,
    "text": "But, of course, we\nbenchmark until 128. That's what we say it's good. But it can go a\nlittle bit further. [CHUCKLES] Let's see some\nexamples of how the model works. Here, I'm giving an image. And I'm asking, how\nmuch is this ticket? And the answer to\nthe right, you can see there that, first\nof all, the model can not understand what's\nthis and extract the price, which is in Czech Koruna-- which I've never\nused that currency. Sorry if I'm saying wrong. But the model can see and\nunderstand and parse that. And then the fun thing is we\ncan follow up with a question. How long is this ticket valid? Of course, you just read\nwhat's written there. But did you see that\nin the ticket before? Could you answer that yourself? Can I go back one slide, please? Can you go back\none slide, please? Yes. Can you see there, how\nlong is this ticket? Even for me, when I was looking\nfor the first time, I was like, where does this\ninformation come from? So on top left, the\ninformation is there, right? And the model knows that. And that's what it says there. So it lasts for 24 hours. So the model has all these, and\nyou can keep the conversation. Another cool thing\nthat the model can do is, of course, every\nfeature that's a capability, I told you, they\nall work together. It's not one or the other. They all work together. So on this example, can\nyou translate this picture? I'm using the vision capability,\nthe text understanding, of course, and the multilingual\nbecause that sign has at least four or five languages. Four, right? And it translates from all of\nthem, without any extra process. It just knows all of them,\nextracts the information, and gives you context\nof where the sign is. And this is an example, when\nI saw for the first time, I was like, well,\nthis is very strong. I didn't expect that to\nhappen to work so well. And one important aspect\nfor us is, as I said, the feedback you give. You give us a lot of feedback. Sometimes they're harsh, but\nI like the feedback anyway. Thank you very much. And the idea is, why\ndoes that matter?",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_006",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 764.11,
    "timestamp_end": 901.24,
    "text": "And one important aspect\nfor us is, as I said, the feedback you give. You give us a lot of feedback. Sometimes they're harsh, but\nI like the feedback anyway. Thank you very much. And the idea is, why\ndoes that matter? It's because we want you all\nto succeed with us so Gemma can really help you\non your applications, on your systems, whatever it is. So your feedback\nis very important. So my colleague here that\nwill present after me asked, what else-- what do\nyou want from Gemma? We asked openly. We got a lot of replies. Of course people said, we\nwant larger context window. We fixed that. We want smaller models. Remember, I told you that\nthe first model was 2 and 7. And then we released-- Gemma 2 was 2, 9, 27. People wanted smaller models. We did the 1B model, and all\nmodels are now quantized. We release the\nquantized versions that are smaller models, but\nthey still keep the quality. So the 27B, it's like-- I'll be very honest. We have the 27B. It's great, super\npowerful, blah, blah. But the 12B, which is like the\nmiddle son, let's say like that, it's amazing. The quality is so strong, and\nyou don't need too much hardware to run that. That's amazing. Can we have more modalities? Yes, we added vision so\nyou can do image and video. We also added more languages. As I told you, 140 languages. Community loved that. And we need system prompts. OK, we got there. Although, to be\nhonest, people want system prompts is\nlike system prompt is, before the\nconversation, you tell the model how it should behave\nand put some ground rules. We don't have a\nsystem prompt role, but if you add what you\nwant to the first message, the model will follow that. The Gemma models are very strong\nat following instructions. So whatever you say, do x, y,\nand z, it will do x, y, and z. So if you say,\ntalk like a pirate, the most basic system prompt,\nit will talk like a pirate during the conversation. So we don't have\na role for system prompts, but you can add to the\nfirst message, and it will work. You can add at any moment\nduring a conversation,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_007",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 888.38,
    "timestamp_end": 1038.17,
    "text": "the most basic system prompt,\nit will talk like a pirate during the conversation. So we don't have\na role for system prompts, but you can add to the\nfirst message, and it will work. You can add at any moment\nduring a conversation, and it will, from there,\ncontinue with your instructions. And people love also the\npersonality of Gemma. Personality is how\nthe model replies. When you ask a question,\nthe way it answers. People like its warm-- it has a strong personality. It's approachable. And that's why we have\na LMArena score so high. And as you can see here-- I guess those\nnumbers are updated. Those are the LM scores for our\nthree larger models, the 27 12, and 4B. And what's so great about this? First of all, this\nis from LMArena. This is a benchmark that people\ngo and blind vote on prompts. So you have a prompt. It gives you two answers, and\nyou say which one you prefer. So this is from the voice of\nthe people, let's say like that. There are, of course, criticism. But this is one of the\nbest benchmarks we have. It's people's choice. On this benchmark, you can see\nthe 27B is one of the top ones. And it's an open--\nremember, open model. Anyone can download\nand use right now. We are one of the best\nopen models, period. And the other thing is our\n27B needs 1 GPU to run, while the other open models that\nare large-- that are better, let's say like that, they\nneed, like, 30 GPUs, 10 GPUs. Do you have 10 GPUs to run now? Do you have access? So that's one of the\nimportant things. You can see the small\ndots over there. And you can see the\n12B, the middle kid. The 12B is really close. It's super strong. And you can see\nthe 4B over there. I'm not going to\nsay who is after. That doesn't matter. But the idea is that our 4B is\nreally, really strong for 4B, 4 billion. Can run basically\nmost of your devices. I'm not saying phones. And here, we see\nour capabilities in terms of specific languages. So if you look for\nGemma over there, it's right there at the bottom. You can see we were top\nin French, top in Spanish,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_008",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1021.19,
    "timestamp_end": 1181.07,
    "text": "I'm not saying phones. And here, we see\nour capabilities in terms of specific languages. So if you look for\nGemma over there, it's right there at the bottom. You can see we were top\nin French, top in Spanish, second in Japanese,\nthird in German. It's great all around\nthe multiple languages. Again, open model, right? This is very, very strong. And as of yesterday,\nwe released Gemma 3n, our newest model that can run on\nas little as 2 gigabytes of RAM. Gemma 3n has a lot\nof development. I'm very impressed by\nthe research behind it. And it's optimized\nfor on-device. So the architecture is\nfocused on making own device, or on small devices,\nit to run faster. It's the same model. It's the base model\nthat will be also used for Gemini Nano,\nwhich comes embedded in many, many Android\ndevices already. So it's the same architecture. So we're sharing that\nas open models now. We expanded the capabilities. Gemma 3 has vision,\ntext, and video, right? Gemma 3n, we added\naudio understanding. So now you can talk\ndirectly to the model. And this is very,\nvery interesting. This is very new. Oh, let me run because\nI have my friend. And we are releasing two sizes-- 4B and 2B. Today, you can test these models\nalready on Google AI Studio. Let me show you this video. [VIDEO PLAYBACK] - Can you tell me\nabout this replica? - Certainly. This is a replica of a\nVirgin Galactic SpaceShipOne, a suborbital spacecraft. It's a model of the vehicle used\nfor testing and demonstration flights. - Can you please save\nthis to my notes? - Absolutely. Creating a note. GUS MARTINS: This is\nrunning completely local. There's zero internet, and it\nwas just saved to Google Keep. - What can I add to the\nsystem to make it faster? - Interesting question. Adding caching layers can\nsignificantly improve speed. Consider implementing\na CDN or caching proxy to store\nfrequently-accessed content. - Write a super short poem\nthat highlights her skills. - Amazing Zoey, so bright. Counting to 100, what a sight. - What is the total square\nfootage of the largest room?",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_009",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1160.47,
    "timestamp_end": 1302.5800000000002,
    "text": "to store\nfrequently-accessed content. - Write a super short poem\nthat highlights her skills. - Amazing Zoey, so bright. Counting to 100, what a sight. - What is the total square\nfootage of the largest room? - The largest room\nis the living room. Its dimensions are\n18 feet by 12 feet. The area is 18 times 12,\nwhich is 216 square feet. - Can you guess which\nfamous figure is this? - This is a cartoon\ndrawing of Einstein. The wavy hair, the mustache,\nand the mischievous grin are all telltale signs. [END PLAYBACK] GUS MARTINS: Nice. Thank you. Thank you. [APPLAUSE] This is all running\ncompletely on-device, of course in an\ninternal app we have. It's not available\nto everyone to test. But this is the capability\nthat Gemma 3n offers. And all of it-- on-device,\nyou ask, can the model speak? No, that's using text to\nspeech from the Android phone, the native one\nthat everyone has. But that's cool. But how do you\nbuild using Gemma? How do you take these\nmodels and make your own? For that, I would like\nto call friend Omar to continue and explain\nto you all the details. Thank you. [APPLAUSE] Thank you. [CHEERING] OMAR SANSEVIERO:\nThank you, guys. So I'm very excited to be\nhere to talk about Gemma, the fantastic\nopensource ecosystem, and how to build products\nwith the Gemma models. As Gus mentioned, we have been\ntalking with the community. We have been talking with\nstartups, with researchers, enterprises, companies,\ndevelopers, and as part of that, we want to make Gemma as\neasy as possible to use. We want to make Gemma very\naccessible to the developer ecosystem. So the easiest way to get\nstarted is with AI Studio. So if you go to AI Studio\nat ai.dev, at the right, you can pick among\nmany different models. And within those, of course,\nyou can try out Gemma. So you can test your prompts. You can see if the Gemma models\nwork for your own use case. There are different model sizes. And depending on\nwhat you want to do, different model sizes may work. Just a few weeks\nago, we released also Image Input in AI Studio.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_010",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1288.3,
    "timestamp_end": 1409.02,
    "text": "You can see if the Gemma models\nwork for your own use case. There are different model sizes. And depending on\nwhat you want to do, different model sizes may work. Just a few weeks\nago, we released also Image Input in AI Studio. So you can try out an image. So for example,\nhere, we uploaded an image of a croissant. And you can ask,\ndescribe the image, and then the model will give\nyou a description of the image input. So let's say that you\ngo to your studio. You try out Gemma. The model works well\nfor you, and you want to deploy this in production. So in the same interface,\nthere is a new button, when you have Gemma selected,\nthat says Deploy in Google Cloud Run. Sorry, you have a button\nthat says Export the Code. And it will give\nyou a code snippet in Python, or in\nTypeScript, or in Rest, or in Apps Script, whichever is\nyour favorite language, which you can use to run and build\nyour own applications using Gemma. This is using the Google APIs. The code is actually\nvery simple. You just, first,\ninstantiate a client. You specify the token,\nthe Gemini API token. Then you pass the\ntext, the content. You can also pass images. And then on the right side,\nyou will create a text. You will generate a text. So you pass the name of the\nmodel, so in this case, Gemma 3, the 27-billion-parameter\nversion. You pass the text, and you\npass the content configuration. So a new feature in\nAI Studio is that when you have a Gemma model\nselected, there's this new button called\nDeploy in Cloud Run, that I was mentioning\nbefore, that allows you to create an endpoint\nthat will automatically scale. So if you wanted to\nscale to 100 GPUs, or if you wanted to scale down\nto 0 when there is no usage, Google Cloud Run will\ntake care of all of it. And the very nice thing\nof this is that you don't need to change the code. There are just two\nthings-- two lines of code, if you saw the details,\nthat you need to change. You need to change the URL, and\nyou need to change the token. So you just change\ntwo lines of code,",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_011",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1398.0,
    "timestamp_end": 1517.53,
    "text": "don't need to change the code. There are just two\nthings-- two lines of code, if you saw the details,\nthat you need to change. You need to change the URL, and\nyou need to change the token. So you just change\ntwo lines of code, but the rest is exactly\nthe same code as before. But that's not the\nonly way to use Gemma. Gemma is designed to\nbe easily integrated into your favorite\nopensource tools. So of course, there\nis Google AI Studio, but we also support Gemma\nin Keras, or in vLLM, or in JAX, or Ollama, or Hugging\nFace Transformers, PyTorch, MediaPipe. So there are many\ndifferent opensource tools that you have probably used. And the idea is to make Gemma\nas easy as possible to user, to enable developers to\nrun the models locally. Because that's the cool\nthing of open models. You don't need an\nAPI necessarily. You can run these models in your\nown laptops, in your own phones. And we'll talk a\nbit more about that. But this is a good\nopportunity for me to talk a bit about Kaggle. Kaggle is the platform, where AI\nbuilders, learners, developers, researchers participate\nin AI challenges. There are over 24 million\npeople participating here. And here, you can\ndiscover models. You can share knowledge. And within those, of course,\nyou can find the Gemma models. You can find the\nGemma models, read the model card, which is like\na documentation of a model. You can download\nthe checkpoints. OK. Let's say that now you want\nto go and download and run Gemma in your computer. Who here knows Ollama? OK, many of you. That's great. So Ollama is a tool that is\ndesigned to very easily, very seamlessly run Gemma models-- well, open models locally\nin your own computer. So with a simple command,\nollama run gemma3, it will download and\ncreate a local API, or a local chat interface\nthat you can use to experiment with the Gemma models. The nice thing of Ollama is\nthat it's very well integrated into many opensource tools. So if you use LLaMA\nindex, for example, you can consume ollama-- you can use ollama to consume\ndifferent open models.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_012",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1503.94,
    "timestamp_end": 1627.45,
    "text": "The nice thing of Ollama is\nthat it's very well integrated into many opensource tools. So if you use LLaMA\nindex, for example, you can consume ollama-- you can use ollama to consume\ndifferent open models. Or same for langchain. So if you have\nused langchain, you can combine langchain and\nollama to very easily use-- to use open models,\nsuch as Gemma 3. Ollama, by the way, is also\nintegrated in Google Cloud Run. So you can very easily\ndeploy a model with Ollama through Google Cloud Run. Ollama is just one of\nmany different open tools to run open models. So another one very\npopular is Keras. Keras provides\nhigh-level APIs that are designed for very quickly\nprototyping and building with models. You can build models. You can experiment. You can train very easily. It gives you very nice,\nsimple abstractions that let you try out\nthe Gemma capabilities and build products with it. So here you can see the code. First, we load the Gemma 3\n4-billion-parameter model with a specific precision. So I'll talk a bit\nmore about precisions in a couple of minutes. But in this case, we\nare using bfloat16. The second part is parsing\nthe actual text to the model. And you may see there that\nthere are some weird words-- start_of_turn, end_of_turn. What's that? So when you go and\nyou train a model, you use a specific format. So when you train a\nmodel with conversations, you use a specific format. And the idea is that when you\nactually want to run a model that was trained-- so if you\ndownload Gemma 3, for example-- you want to use the same\nprompt structure, the same text structure as what was\nused to train the model. So some libraries,\nsuch as Ollama, take care of doing\nall of this for you, while other libraries,\nsuch as Keras, give you more fine-grained\ncontrol on the prompts and allow you to\ncontrol how things work. Just a couple of months ago,\nwith the Gemma 3 launch, we also announced\nGemma Python Library. It's powered by JAX. It also gives you\nhigh-level abstractions that let you run the Gemma models. And it has three key components.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_013",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1615.56,
    "timestamp_end": 1743.99,
    "text": "Just a couple of months ago,\nwith the Gemma 3 launch, we also announced\nGemma Python Library. It's powered by JAX. It also gives you\nhigh-level abstractions that let you run the Gemma models. And it has three key components. It has a tokenizer. It has the model, the model\nconfiguration, let's say. And it has the actual\nmodel parameters. And this JAX library gives\nyou this sampler class that lets you combine the\nmodel, the parameters, and the tokenizer\ninto a single class. And then when you do\n\"sampler.sample Roses are red,\" it will keep\ngenerating the text. Finally, if you\nknow Hugging Face, they have this library\ncalled Transformers, which gives you a\nvery high level, also a pipeline as a\nclass, that lets you say, I want to do text\ngeneration, with a Gemma 3 12-billion-parameter model. And just like that, you can\nspecify, OK, the user is saying, who are you? Answer in pirate speak. You pass that to the model, and\nthe model will keep generating. So as you can see, Gemma\nis deeply integrated into many of your\nfavorite opensource tools. And we are talking with\nall of these developers to make it as easy as\npossible to run them. Now, these models are\nrunning in your computer. And as Gus was\nmentioning, we have models that go from\n1 billion parameters to 27 billion parameters. And you may be\nwondering, can I run this model in my own computer? So each parameter is stored with\na number, a specific precision. So previously, I\nmentioned bfloat16. That means that there are 16\nbits for every single parameter. So kind of rough napkin\nmath-- so this is just an approximation-- is that for\nevery single billion parameters, you need 2 gigabytes of VRAM. So you need a GPU\nof 2 gigabytes. So the 27-billion-parameter\nmodel, it's 54 gigabytes. And probably most of you\ndon't have that at home. So now you may be wondering,\ncan I actually use these models? So there is a family\nof techniques called \"quantization.\" Quantization reduces the\nprecision of the parameters. So you can go down to 8 bits per\nparameter, 4 bits, even 2 bits.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_014",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1729.78,
    "timestamp_end": 1851.198,
    "text": "So now you may be wondering,\ncan I actually use these models? So there is a family\nof techniques called \"quantization.\" Quantization reduces the\nprecision of the parameters. So you can go down to 8 bits per\nparameter, 4 bits, even 2 bits. And as we reduce the\nprecision, the quality begins to degrade a bit. So the models may become\nworse than the bfloat16. So you may be\nwondering, OK, can I reduce the precision while\nmaintaining the same quality? And this is a good moment\njust to show, with int4, so if we quantize the model, now\nthe 27-billion parameter just needs 14 gigabytes of VRAM,\nwhich is quite good, actually. I personally really like the\n12-billion parameter model. It's a very nice size. Just 6.6 gigabytes\nof VRAM to load a model, which is a size for\nmost normal computer GPUs. So it's a very friendly size. A few weeks ago, we released\na QAT, Quantization-Aware Training, checkpoints of Gemma. QAT is a technique that allows\nyou to make the model more robust to quantization. It's a very small\nadditional training stage. And what that means is that\nonce you quantize the model down to int4, the model will\npreserve the same quality as a larger model. So it will be a bit worse, but\nyou don't have so much quality degradation. So now with the\nGemma 3 models, you can also try out\nthe QAT checkpoints that are specifically\naimed to make it very easy to run the models locally. So I've been talking\na lot about how to run and how to load\nthe Gemma models. And now I would love to talk a\nbit more about the Gemmaverse, which is composed of tens\nof thousands of variants, tools, libraries built by\nyou, the developer ecosystem, and also researchers. It's a vibrant ecosystem\nwith huge adoption. And just to understand\nit a bit more, it's good to talk about\npretraining and post-training. So in LLMs, there are\nusually two main stages. In the pretraining, you usually\ntrain a model from scratch. That needs lots of\ndata, lots of compute. And the outcome of that stage\nis that it's a model that will predict the next token.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_015",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1835.89,
    "timestamp_end": 1948.44,
    "text": "So in LLMs, there are\nusually two main stages. In the pretraining, you usually\ntrain a model from scratch. That needs lots of\ndata, lots of compute. And the outcome of that stage\nis that it's a model that will predict the next token. It's not the one that you\nwould put in a chat interface or in a product. There is also the\npost-training stage. So if you have heard about RLHF,\nQLoRA, PEFT for finetuning, that's post-training. And the idea is to make the\nmodel better conversational. So I'll talk a bit more\nabout that in a minute. But for Gemma, we released\nboth the pretrained and the post-trained\ncheckpoints. The pretrained checkpoints\nwork as a foundation for the community to\nbuild their own models with their favorite\nfinetuning tools. And the post-training checkpoint\nis a generalist conversational model. But there are many cases\nin which you will want to fine tune your own models. So when will you\nwant to do that? If you want to add\nnew capabilities; if you want to make the model\nbetter for Spanish, for example, so for your own\nlanguages; if you want to make the model\nbetter for your own domain, so if you have your\nown company data and you want the model to\nlearn about your company data, you can fine-tune. And these are\nmodels that you own, that you can host in your own\ncomputers, in your own servers. So you already saw\nthe LMArena score. So what you see here is\nactually the chat model. It's the post-train checkpoint,\nwhich is a generalist conversational model. But again, you can fine-tune\nGemma for your own use cases. So if have your own\ninternal leaderboards for your own use cases, this is\nwhere finetuning will come in. And there are many\ndifferent finetuning tools. So again, I mentioned already\nKeras and Hugging Face. There is Unsloth\nand Axolotl, which are two other very\npopular opensource libraries to fine-tune models. I will not dive too much into\nthe code for the sake of time. But with Transformers,\nfor example, there is trl, which is a\nhigh-level wrapper that",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_016",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 1936.56,
    "timestamp_end": 2061.52,
    "text": "are two other very\npopular opensource libraries to fine-tune models. I will not dive too much into\nthe code for the sake of time. But with Transformers,\nfor example, there is trl, which is a\nhigh-level wrapper that allows you to very easily,\njust with a SFTTrainer, do the training of a model. Or Unsloth. Unsloth is a wrapper around\nHugging Face Transformers that makes it very, very, very\neasy to fine-tune models with very limited compute. So in a free Google Colab,\nin a Kaggle instance, for example, you can use\nUnsloth to train models. And this is extremely quickly,\nwithout losing any quality. So it's a very strong\ntechnique to do finetuning. And there are over 70,000\nGemma-based models built by the community. The top three are\nmultilingual examples. So we have seen people training\nGemma for Korean conversations. We have seen GemmaX, which\nis Gemma for translation. We have seen SILMA Kashif,\nwhich is a retrieval augmented generation for\nArabic and English. But we have also\nseen people using Gemma to change the way\nthey develop models, that they build models. So Lumina Image\nis a way in which they combine Gemma\nwith a few other models to do image generation. Or OmniAudio, which\nis a model that can also take audio as input. Or finally, SimPO, which is a\ndifferent post-train technique. It's a different RLHF\ntechnique based on Gemma. So we are seeing people\nbuilding research, also, on top of the open\nmodels that we release. It's been fantastic to see\nwhat the community built. Just a couple of months ago, we\ndid a Kaggle competition, focused on enabling\nthe community to train Gemma for their own languages. And we saw thousands of people\nparticipating, contributing, sharing knowledge,\ncollaborating. So this was very exciting. And now I'm happy\nto hand it over again to Gus, who will\ntalk a bit more about some of his favorite case studies. Thanks. [APPLAUSE] GUS MARTINS: Thank you. Nice. Yeah, you thought you didn't\nhave me anymore, right? So let's talk about how\npeople are leveraging Gemma even further.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_017",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 2041.86,
    "timestamp_end": 2186.4599999999996,
    "text": "again to Gus, who will\ntalk a bit more about some of his favorite case studies. Thanks. [APPLAUSE] GUS MARTINS: Thank you. Nice. Yeah, you thought you didn't\nhave me anymore, right? So let's talk about how\npeople are leveraging Gemma even further. We've seen lots of variants\nthat Omar just showed, but we have some\nvery cool use cases. For example, we have our\nfriends from AI Singapore. They built the SEA-LION model. And the idea is they\nwanted to create a Gemma version\nthat was very good on Southeast Asian languages. There's a bunch of those. And Gemma was good. There are many models\nthat try to do that. Our model was good. But what they did is\nthey finetuned Gemma, a Gemma 2 model, a 9B, to be the\nbest Southeast Asian language model, period. And that's their scores\nthere you can see. And we've been working with them\nto make even further updates to that and enable\na lot of people to have access, in\ntheir own language, to a large language model. Beyond that, we have our\ncolleagues on Insait. And they built the first\nnational Bulgarian LLM, let's say like that. And what they did is they\nfinetuned also a Gemma 2 model-- again, a model that\nwas English only, to be the best\nBulgarian LLM, period. And it's offered to\ncitizens in Bulgaria so every citizen can use. It's used by the government. So this is exactly what we want. We want to enable people to\nbring this to their communities, to their citizens, which\nis very, very cool, and giving this\npower to everyone, making it accessible\nto everyone. So we're very proud\nof these use cases. And in terms of\nvariants, there's, of course, the community ones. We are absurdly happy with them. But we also keep pushing our own\nvariants, going into new fields. I don't know if you saw. Last month, we released--\nwe announced DolphinGemma, which is our first large\nlanguage model for dolphin communication. The idea is, of\ncourse, to create-- with this model, we create a\nshared vocabulary with dolphins. And then we can, at\nsome point, start to understand how\nto talk to them.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_018",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 2170.59,
    "timestamp_end": 2315.26,
    "text": "which is our first large\nlanguage model for dolphin communication. The idea is, of\ncourse, to create-- with this model, we create a\nshared vocabulary with dolphins. And then we can, at\nsome point, start to understand how\nto talk to them. We are not there yet. I don't want to sell hype. That's not what I do. This model is helping\nresearchers get there. And we are using decades\nof field research data to train models together. This is very cool. It's coming soon. We are going to open that\nfor everyone to play. Maybe you do a version\nfor whales or for dogs. I don't know. Up to you. So this is great. It's coming soon. We also released,\nyesterday, MedGemma, which is a Gemma variant\nfor the health industry. The idea here is you can-- Gemma is good on visual and all. But when you start to use\nmedical vocabulary or images from health-related\nfield, of course Gemma is not great on that. It wasn't trained to do that. That's what MedGemma\nhelps you address. They fine-tune Gemma 3 to\nbe great for medical use cases, both image and text. The 4B and the 27B, these are\navailable on Hugging Face, Vertex, and we are bringing\nit to even more surfaces. So this is very good for you to\nstart playing on health apps. And why does this matter? When we talk about\nhealth-related applications, this is a field where\nyou cannot just, oh, let me send your data\nto any cloud, anywhere. Let me do that. You cannot do that. There's a lot of regulations. So running completely local on\nyour computer, on your server, that's very important. And MedGemma enables that. Gemma enables that. MedGemma enables that. We also mentioned\nyesterday, SignGemma, where we're expanding\nGemma to sign language. So you'll be able to do a\nsign-- this is not a real sign. But you'll be able to do\nsign language with the model. It will translate that to text. At first, it will be American\nSign language and English, but we are working\nwith the community to make this in as many\nsign languages as possible. This is coming soon-- hopefully really, really soon.",
    "source": "youtube",
    "speaker": "unknown"
  },
  {
    "id": "o7Bv4r08FBM__chunk_019",
    "schema_version": "canonical_v1",
    "video_id": "o7Bv4r08FBM",
    "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
    "timestamp_start": 2300.77,
    "timestamp_end": 2426.7999999999997,
    "text": "It will translate that to text. At first, it will be American\nSign language and English, but we are working\nwith the community to make this in as many\nsign languages as possible. This is coming soon-- hopefully really, really soon. And this is also\nanother field we're expanding with our variants\nto help, again, engage the community,\nreach more people, help more people benefit\nfrom these kind of tools. Another variant that I'm\nvery proud of is ShieldGemma. ShieldGemma, there's 1 and 2. ShieldGemma 2 specifically\nwas released in March. It's a model as a\nsafety classifier, but Gemma 2 is\nspecialized for image. So you give an\nimage and a prompt, and it can classify the\nsafety of that image based on your prompt. And it's based on\nthe Gemma 3 4B, which is already very strong. And how would you use that? You would put on a system that\nyou have an image input or text, and then you put your Gemma to-- you could use that\nto protect your LLM. That's after. You could put your Gemma after\nyour model generates an image, for example. And this is how you do a-- the safety classifier\nwas built, right? You have a policy,\nimages, and you train it. So Gemma works in\nmany, many places, helping your system become\nmore responsible, safe, and all like that. And this is all great and all,\nbut what I really want to know is what you are\ngoing to build next. Please reach out to us. We are all on social, or we will\nbe there on the Q&A after this. I would love to\nunderstand how you're using Gemma, how you\nwant to use Gemma, if Gemma can help\nyou on your business. We've been talking\nto the community. We know how important it is. Well, a lot of people know me\nfrom talking to the community because I really think that\nwe have to work all together. Your opinion is\nextremely important. So thank you very much. [MUSIC PLAYING] [APPLAUSE]",
    "source": "youtube",
    "speaker": "unknown"
  }
]
[
  {
    "query": "differences between Gemini developer API features and other AI platforms",
    "reranked_docs": [
      {
        "id": "5818d7d2-3012-5c4b-a76a-e93a3f646453",
        "doc_id": "4TE-KFXvhAk__chunk_002",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 230.74,
        "timestamp_end": 380.44,
        "text": "it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it because of its\nefficiency and speed, and it's now even better at\nalmost every single dimension. So we improved\nall the benchmarks across reasoning, coding,\nmultimodality, and also long context. Then we have our Gemini 2.0\nFlash, which is fast and cheap, works fine, and our\nGemini Nano, which is optimized for on-device tasks. And as you've heard, we've\nbeen shipping relentlessly. And I would like to give\nyou just a quick highlight of everything that we've\nbeen shipping in AI Studio and the Gemini API. There's a talk\ntomorrow that I would like to invite you to attend,\nwhich is by Shrestha Basu Mallick, a group product\nmanager on Gemini API, and Luciano Martins, our\ntechnical lead for Gemini API from DevRel. And they're going to do a deep\ndive into everything that's new within the Gemini\nAPI, so you definitely don't want to miss\nthat session tomorrow. But for now, just a glimpse to\nget you excited about what's new in AI Studio. We've built a new tab\nthat is called Build that instantly generates web apps. And it's really cool, because it\nenables developers and builders alike to prototype very\nquickly with natural language. We have a new generative media\nexperience in AI Studio as well, and I'm going to\ndemo all of this so you can see how\nit actually works. And we are always\nlistening to the community. We listen to your\nfeedback, and we always build with developers in mind. And that's why that\nsome of these features were actually\nrequested by community, and that's what happened with\nthe built-in new dashboard. You requested. We built it. And we also have some\nnew Native Audio and TTS support in AI Studio. On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 3.838045835494995
      },
      {
        "id": "51a07f69-109c-5809-b618-2760e83de9e5",
        "doc_id": "gHHjDRDNUNU__chunk_007",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 890.57,
        "timestamp_end": 1021.23,
        "text": "We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there. If you are a Google\nColab user, you have ways to interact with\nthe Gemini models as well. So the key idea is to\nmake it easier for you. No matter where you are having\nyour development experience, We are trying to bring\nGemini API closer to you. SHRESTHA BASU MALLICK:\nThat's correct. And I think this is\nalso a good place to make a plug for Google\nAI Studio, which of course, everyone knows about and loves. It is the place where\na lot of our developers first test out the\ncapabilities of the API before committing to\nbuilding applications at scale with the API. And Google AI Studio now\nhas code generation also. LUCIANO MARTINS: Absolutely. SHRESTHA BASU MALLICK: So this\nis a quick, high level overview of all the components that are\navailable through the Gemini API. It's pretty standard. You send in your prompt, you get\na response back from the model. If there's a tool\ncall needed, we do support a set of\nfirst party Google tools, as well as function calling. So in terms of the first party\nGoogle tools that we support, we, of course, support\nGoogle Search as a tool. The best search engine\nout there is now available as a tool for the\nmodel to call when it needs to. Now in addition to the\ninformation that you get out of Search, if you want to\nretrieve more in-depth content from web pages for\napplications such as you're building your own version\nof a research agent, so you pull some\ninformation from Search. You pull a set of\nURLs, but if you want to extract a little\nmore in depth content, we just released a new tool\nat I/O called URL Context. And then, of course, you can\ntake all that information. And if you want to create\nbeautiful charts out of it or run some analysis,\nwe make code execution available as a tool to\nyou via the API as well. Then, of course, you\nhave function calling.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 1.841109275817871
      },
      {
        "id": "07bff983-b0de-5d12-9cb5-8b3d3c7df76c",
        "doc_id": "gHHjDRDNUNU__chunk_006",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 751.58,
        "timestamp_end": 904.85,
        "text": "mode called Deep Think. And the idea behind Deep\nThink is that the model 2.5 Pro can think through various\npossible answers to a problem before giving you\nthe best answer. So that is available\nin trusted testers, but will be rolled\nout more widely soon. And then we also have\nGemini Diffusion, which is the first time\nwe are testing a diffusion architecture as opposed to an\nautoregressive architecture. And these models are faster\nthan the fastest model out today and for almost\nsimilar, the performance. So try out both when you have\naccess to both Gemini Deep Diffusion and Deep Think. LUCIANO MARTINS: Perfect. SHRESTHA BASU MALLICK: All\nright, so the Gemini API. LUCIANO MARTINS: So yeah, if\nyou start thinking, especially after the keynote\nspeakers today, there are many surfaces where\nyou can interact with the Gemini models. So you have the Gemini app. You have Gemini available\non Google Cloud as the Code Assist, the assistant-- the\nCloud Assist inside the Console. You have Gemini on Workspace. So when you are talking\nabout developer experiences, we have the one\nspecific Gemini API with similar experiences as you\nhave on the no-code UI that we call AI Studio, where you\ncan start experimenting and developing your solutions\nusing the Gemini models, right? SHRESTHA BASU\nMALLICK: That's right. LUCIANO MARTINS: So\nbasically, the Gemini API is a very low barrier\nentry for having Gemini programmatic experiences. You have access to\nall public models there from the Gen Media ones,\nGemma, all the Gemini models, variants. You have a very generous\nfree of charge tier, where you can start\nexperimenting just after the session\nwithout concerns about credit cards, costs,\nbilling, anything of that. We keep developing\nmore SDKs for this API. So for now, you have available\none SDK for Python, JavaScript, Go, and Java. We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 1.0124142169952393
      },
      {
        "id": "b3910d5f-a24c-56c2-8f8e-90d35889ff56",
        "doc_id": "gHHjDRDNUNU__chunk_017",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2130.98,
        "timestamp_end": 2271.49,
        "text": "If you're building\na research agent, maybe you want to\nuse the Chat API. If you're building\na gaming agent, maybe you want to use the real\ntime API or a customer support agent. Of course, you can build\nresearch agents also on the real time API. But in general, you now\nhave these two APIs. And then we've talked\nabout tools a lot. So all I'll say is\ngive us feedback in terms of what are some\nof the other tools that you would see Google make available\nthrough the Gemini API. LUCIANO MARTINS: Yeah. So as you just said,\nthe Gemini 2.5 Pro-- 2.5 models are thinking\ncapable, so they can do more advanced and\nmore complex reasoning. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS:\nAnd maybe we could highlight that now,\nspecifically for the 2.5 Flash, we have the ability of\nusing what we just mentioned, the thinking budgets, where\nyou can calibrate how deep and how much the model\nwill do reasoning cycles. And also for both models,\nwe can have access to what we are calling\nthought summaries. SHRESTHA BASU MALLICK: Yes. LUCIANO MARTINS: So\nbasically you are not only counting on the model\nability to reason, but also you can see on your\nresponse which key steps or which key reasoning\nthoughts the model had to get to our conclusion. So basically, that's a\npretty basic Gemini API code using the Python SDK. For those of you who\nnever used the SDK before, it's pretty straightforward. So three key blocks. You need to import the SDK,\nyou instantiate the client, and then you interact\nwith the model you want. Here we are interacting\nwith the Gemini 2.5 Flash. You send your prompts. And then you have\none specific bit called ThinkingConfig,\nwhere, in this code, you are asking the API\nto include the thought summaries on your\nresponse, and that's to include thoughts\nequals true, and also the amount of tokens you want\nto use during the reasoning process. On your response,\nyou're going to have part of the response will\nbe the thought summaries. So it's separated from the\nfinal answer by intention. If you want to keep\nthose on your backend,",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -2.0424814224243164
      },
      {
        "id": "b0ee6a81-6918-5bbf-8b1e-baa4084f5b62",
        "doc_id": "gHHjDRDNUNU__chunk_002",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 254.53,
        "timestamp_end": 382.64,
        "text": "when we think about the key\nstandard benchmarks, Shrestha? SHRESTHA BASU\nMALLICK: Yeah, we'll go to the benchmarks\nin a second. But the message\nthat I want everyone to take away from this\nslide is whether it's a super complex task, or whether\nit's on-device processing, you now have a Gemini model\nthat you can use for it. And it's pretty powerful. And it's pretty price effective. LUCIANO MARTINS: Yeah,\nmaybe it is also worth mentioning, Shrestha,\nthat some folks here may be developing with\nGemini since it was released. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS: So using the\nAI Studio, using the Gemini API, the older versions\nof Gemini like 2.0, 1.5 are still available. But we really\nencourage everybody to start experimenting\nwith the newer ones with better capabilities\nand better performance. SHRESTHA BASU MALLICK:\nThat's correct. So 2.0 Flash continues to be one\nof our most widely-used models, just again, because of how good\nit is for the price it's at. And we have a lot of people\nwho are still using it. And then we also have some\npeople on the 1.5 models. But we're encouraging\npeople now, we're two generations\nahead, to start using the 2.5 series of\nmodels and give us feedback. LUCIANO MARTINS: Nice. So please tell us a little bit\nabout the benchmarks, Shrestha. SHRESTHA BASU MALLICK:\nYeah, next slide. Yeah, so we're not going\nto spend a lot of time on the benchmarks. But the two key points\nthat I'd like to hit here is number one, is one\nway in which we measure how good these models are is\nby putting them on LMArena, which is where developers\nand builders like yourself test out\nall these models and give it an ELO rating. And we're, of course, very\nproud that right now you have three Gemini models in\nthe top 10 in the LMArena. LUCIANO MARTINS:\nIncluding the top one. SHRESTHA BASU\nMALLICK: Including, how did I forget that? We are number one guys,\nand we're very proud of it. [CHEERING] Thank you. One of the things,\none of the use cases that has been really emerging\nin the last couple of months",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -2.766146659851074
      }
    ],
    "relevant_doc_ids": [
      "gHHjDRDNUNU__chunk_000"
    ]
  },
  {
    "query": "how to integrate Gemini API for building AI apps with text, images, and video",
    "reranked_docs": [
      {
        "id": "1bf79af9-fc9e-5248-b56a-1c7adf068aff",
        "doc_id": "4TE-KFXvhAk__chunk_007",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 880.26,
        "timestamp_end": 979.77,
        "text": "So it's just really\nquick and easy to do. OK, we can go back\nto the slides. So very, very\nquickly, as a recap, Gemini Developer API is the\neasiest way to get started. It's super lightweight,\nit's fast to install, and you can get up and running\nhonestly in about a minute. OK? I will use the clicker. This is the flow to get\nstarted with Google AI Studio. Go to Google AI Studio. Get your key. Try one of the code\nexamples on ai.google.dev or in the cookbook. I see people taking pictures. That makes me happy. Please try this. We spent so much time\non making this easy, and I hope it works for you. If not, please file an\nissue and we'll get on it. This is the Gen AI SDK\nfor the Gemini API, and this is something\nwe've been rolling out. It's our latest SDK. We've been rolling\nit out gradually over the course of\nthe last six months. It's super user friendly. It's really easy to use. Really, the only point\nI want to make here, because I don't want\nto read the code examples or the\ndocumentation to you, you can call the API\nin a few lines of code. Basically add your key, select\na model, write a prompt, you can go ahead and call it. You can also get access\nto advanced functionality in one line of code. So if you'd like to get the\nthinking summaries that Joana showed you, you can just\nadd a ThinkingConfig, say include the thoughts. Now, you've got the\nthinking summaries. And a good use\ncase for this could be any time you need to\nexplain the model's reasoning. Particularly, you can\nimagine, if you're building an education\napp or a tutoring app, you can get the\nthinking summaries. In addition to\nreally cool things that you can do with\na single line of code, there's some more advanced stuff\nthat you can do with the SDK as well. So I know there's a lot\nof code on this slide, but we've talked a lot\nabout building agents and agentic experiences. In this example,\nyou could imagine that you have a Python\nfunction on your laptop called weather_function. And maybe that calls\nyour own weather server to get the weather.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 3.803842544555664
      },
      {
        "id": "cc910b44-8924-521e-9890-82d926ac6eb6",
        "doc_id": "4TE-KFXvhAk__chunk_003",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 363.97,
        "timestamp_end": 521.659,
        "text": "On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API and also on [INAUDIBLE] API\nfor real generating audio. And some of the use\ncases here that we had in mind when\nwe built this were more dynamic audiobooks,\nsome more engaging podcasts, or, even for those of\nyou in customer support, producing more natural\nvoices into your workflows. We also have enhanced\ntooling, which is really cool, because now you\ncan use Grounding with Google Search, also with Code Execution\ntogether in just one API call, and URL context, which you\nheard during the keynote, which provides the model with\ndepth content from web pages. And since you can chain\nit to other tools, it's actually really powerful\nto build search agents. So really cool stuff. Lastly, just to call out that\nwe now offer Gemini SDK support for MCP, which reduces a\nlot of developer friction and simplifies building\nagentic capabilities. So you don't want to\nmiss tomorrow's talk to learn more about this. And Google AI Studio. Who here uses Google AI Studio? Can I see some hands? OK, awesome. Awesome. We have a lot to chat\nafter this session then. Google AI Studio is the\nperfect place for anyone to start developing with AI. It's the simplest way to test\nthe latest DeepMind models. We typically bring them\nto Google AI Studio so you can start prototyping\nand playing with the models. And you don't need Google\nCloud knowledge in order to set up your environment. It is free of charge, and you\ncan create, and test, and save your prompts. And there's also starter\napps that will inspire you. And that's exactly what I'm\ngoing to demo here today for you. I am going to show\nyou some of the work that the team has been doing. So if we go to Google AI Studio\nhere, if we go into Build, and Mumble Jumble is\nsomething that we've literally just created, and\nit's one of my favorite apps. So Mumble Jumble is one\nof those applications that",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 3.3824880123138428
      },
      {
        "id": "5818d7d2-3012-5c4b-a76a-e93a3f646453",
        "doc_id": "4TE-KFXvhAk__chunk_002",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 230.74,
        "timestamp_end": 380.44,
        "text": "it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it because of its\nefficiency and speed, and it's now even better at\nalmost every single dimension. So we improved\nall the benchmarks across reasoning, coding,\nmultimodality, and also long context. Then we have our Gemini 2.0\nFlash, which is fast and cheap, works fine, and our\nGemini Nano, which is optimized for on-device tasks. And as you've heard, we've\nbeen shipping relentlessly. And I would like to give\nyou just a quick highlight of everything that we've\nbeen shipping in AI Studio and the Gemini API. There's a talk\ntomorrow that I would like to invite you to attend,\nwhich is by Shrestha Basu Mallick, a group product\nmanager on Gemini API, and Luciano Martins, our\ntechnical lead for Gemini API from DevRel. And they're going to do a deep\ndive into everything that's new within the Gemini\nAPI, so you definitely don't want to miss\nthat session tomorrow. But for now, just a glimpse to\nget you excited about what's new in AI Studio. We've built a new tab\nthat is called Build that instantly generates web apps. And it's really cool, because it\nenables developers and builders alike to prototype very\nquickly with natural language. We have a new generative media\nexperience in AI Studio as well, and I'm going to\ndemo all of this so you can see how\nit actually works. And we are always\nlistening to the community. We listen to your\nfeedback, and we always build with developers in mind. And that's why that\nsome of these features were actually\nrequested by community, and that's what happened with\nthe built-in new dashboard. You requested. We built it. And we also have some\nnew Native Audio and TTS support in AI Studio. On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 2.997844696044922
      },
      {
        "id": "51a07f69-109c-5809-b618-2760e83de9e5",
        "doc_id": "gHHjDRDNUNU__chunk_007",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 890.57,
        "timestamp_end": 1021.23,
        "text": "We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there. If you are a Google\nColab user, you have ways to interact with\nthe Gemini models as well. So the key idea is to\nmake it easier for you. No matter where you are having\nyour development experience, We are trying to bring\nGemini API closer to you. SHRESTHA BASU MALLICK:\nThat's correct. And I think this is\nalso a good place to make a plug for Google\nAI Studio, which of course, everyone knows about and loves. It is the place where\na lot of our developers first test out the\ncapabilities of the API before committing to\nbuilding applications at scale with the API. And Google AI Studio now\nhas code generation also. LUCIANO MARTINS: Absolutely. SHRESTHA BASU MALLICK: So this\nis a quick, high level overview of all the components that are\navailable through the Gemini API. It's pretty standard. You send in your prompt, you get\na response back from the model. If there's a tool\ncall needed, we do support a set of\nfirst party Google tools, as well as function calling. So in terms of the first party\nGoogle tools that we support, we, of course, support\nGoogle Search as a tool. The best search engine\nout there is now available as a tool for the\nmodel to call when it needs to. Now in addition to the\ninformation that you get out of Search, if you want to\nretrieve more in-depth content from web pages for\napplications such as you're building your own version\nof a research agent, so you pull some\ninformation from Search. You pull a set of\nURLs, but if you want to extract a little\nmore in depth content, we just released a new tool\nat I/O called URL Context. And then, of course, you can\ntake all that information. And if you want to create\nbeautiful charts out of it or run some analysis,\nwe make code execution available as a tool to\nyou via the API as well. Then, of course, you\nhave function calling.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 2.8330163955688477
      },
      {
        "id": "bd290fa4-ea95-58d4-af8c-0a4fb9d68cc9",
        "doc_id": "4TE-KFXvhAk__chunk_006",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 777.32,
        "timestamp_end": 892.14,
        "text": "there's information about\nthe models, everything you need to get started. We also have the\nGemini API Cookbook. We have a link to\nthis at the end. It's basically goo.gle/cookbook. G-O-O.G-L-E/cookbook. And this will take you to\na whole slew of notebooks that the team has put together. And basically, all\nof these notebooks are end-to-end examples\nthat show you one thing that you might be interested\nin, like what's the best way to do code execution? What's the best way to\ndo function calling? You'll find that\nin the cookbook. I also very, very quickly\nwant to show you how easy it is to get started with the API. So basically, in\nGoogle AI Studio, you don't need a credit\ncard or anything like that, in about a minute you can\njust click Get API key. Create your key. Now, if you're doing\nthis for the first time, behind the scenes,\nthis will automatically create a Cloud project\nfor you, but that detail is not important. Basically, now I have an API key\nand I'm ready to install the SDK and call the model. If you open up any of the\nnotebooks in the cookbook, well, let's just say-- it's in a different\ndirectory here, but let's just say\nwe've opened up-- eh, we'll just say we\nopened up this one, which is in the quickstarts directory. And this shows you\nexactly what Joana showed, how to get the\nthinking summaries. You can add your API\nkey in Google Colab. If you zoom in, you\ncan hit Add new secret. And in this particular notebook,\nit's called Google API key. But you could call\nit whatever you like. So you would add\nGoogle API key there, you would paste your\nkey there, and now you're ready to run this. So if you do Runtime and Run\nall, you're calling the API and you're running\nall the examples. You can also, directly\nin Google Colab, we have this thing where you\ncan grab an API key straight inside Google Colab. So it's just really\nquick and easy to do. OK, we can go back\nto the slides. So very, very\nquickly, as a recap, Gemini Developer API is the\neasiest way to get started. It's super lightweight,\nit's fast to install,",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -0.41969528794288635
      }
    ],
    "relevant_doc_ids": [
      "gHHjDRDNUNU__chunk_000"
    ]
  },
  {
    "query": "what is Gemini model family and how do I use it for multi-modal data",
    "reranked_docs": [
      {
        "id": "2b78c20e-37f7-560e-866d-949eb6ae6e03",
        "doc_id": "gHHjDRDNUNU__chunk_009",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 1130.2,
        "timestamp_end": 1280.26,
        "text": "But Luciano and I thought we\nwould hit some of the highlights that we really want people\nto know more about and use. So one of the areas of feedback\nthat we used to get a lot is you can, of course, upload\nfiles to the Gemini API. You can also, if you're\nless than 20 megabytes, you can also pass some of\nthese media information inline. But now, you can also-- and we've had this feature\nout for a few weeks. You can also pass\na YouTube link. And the Gemini API can\nanalyze that information. When you pass media files\nlike videos, depending on how much you want to fit\ninto the context window, you have now, a choice between\nthree resolution settings. At the lowest setting,\nyou can process up to six hours of video. But then you also have more\nhigh resolution settings that you can use. We support dynamic frame rate\nper second in the Gemini API. You can read that more about\nthat in our documentation, or come talk to me and Luciano. We support video clipping. That's a new feature. And we support\nimage segmentation. One other point I want to make\non multimodal understanding is even in the days\nof 1.5 and 2.0, Gemini models were\nsome of the best models out there for multimodal\nunderstanding. The example I like to give\nhere is I was in Costa Rica, and my guide showed\nme-- this is night. And my guide showed me\noh, there's a glass frog somewhere there on a branch. I took a photo, but I did not\nsee the glass frog in real life or in the photo. But I passed it to Gemini. And Gemini not\nonly saw the frog, it identified the\nspecies correctly. So that's how good multimodal\nunderstanding is on the Gemini. Please try it out. We also support long context. And then we, along with long-- long context is we have some\nof the largest context windows out there. So like the equivalent\nof depending on whether you're using a model\nwith 1 million context or 2 million context, you can read\nthe equivalent of eight novels, have the model read it\nand/or entire code bases. But what can sometimes\nhappen with long context",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 0.5219815969467163
      },
      {
        "id": "51a07f69-109c-5809-b618-2760e83de9e5",
        "doc_id": "gHHjDRDNUNU__chunk_007",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 890.57,
        "timestamp_end": 1021.23,
        "text": "We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there. If you are a Google\nColab user, you have ways to interact with\nthe Gemini models as well. So the key idea is to\nmake it easier for you. No matter where you are having\nyour development experience, We are trying to bring\nGemini API closer to you. SHRESTHA BASU MALLICK:\nThat's correct. And I think this is\nalso a good place to make a plug for Google\nAI Studio, which of course, everyone knows about and loves. It is the place where\na lot of our developers first test out the\ncapabilities of the API before committing to\nbuilding applications at scale with the API. And Google AI Studio now\nhas code generation also. LUCIANO MARTINS: Absolutely. SHRESTHA BASU MALLICK: So this\nis a quick, high level overview of all the components that are\navailable through the Gemini API. It's pretty standard. You send in your prompt, you get\na response back from the model. If there's a tool\ncall needed, we do support a set of\nfirst party Google tools, as well as function calling. So in terms of the first party\nGoogle tools that we support, we, of course, support\nGoogle Search as a tool. The best search engine\nout there is now available as a tool for the\nmodel to call when it needs to. Now in addition to the\ninformation that you get out of Search, if you want to\nretrieve more in-depth content from web pages for\napplications such as you're building your own version\nof a research agent, so you pull some\ninformation from Search. You pull a set of\nURLs, but if you want to extract a little\nmore in depth content, we just released a new tool\nat I/O called URL Context. And then, of course, you can\ntake all that information. And if you want to create\nbeautiful charts out of it or run some analysis,\nwe make code execution available as a tool to\nyou via the API as well. Then, of course, you\nhave function calling.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -0.34487104415893555
      },
      {
        "id": "07bff983-b0de-5d12-9cb5-8b3d3c7df76c",
        "doc_id": "gHHjDRDNUNU__chunk_006",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 751.58,
        "timestamp_end": 904.85,
        "text": "mode called Deep Think. And the idea behind Deep\nThink is that the model 2.5 Pro can think through various\npossible answers to a problem before giving you\nthe best answer. So that is available\nin trusted testers, but will be rolled\nout more widely soon. And then we also have\nGemini Diffusion, which is the first time\nwe are testing a diffusion architecture as opposed to an\nautoregressive architecture. And these models are faster\nthan the fastest model out today and for almost\nsimilar, the performance. So try out both when you have\naccess to both Gemini Deep Diffusion and Deep Think. LUCIANO MARTINS: Perfect. SHRESTHA BASU MALLICK: All\nright, so the Gemini API. LUCIANO MARTINS: So yeah, if\nyou start thinking, especially after the keynote\nspeakers today, there are many surfaces where\nyou can interact with the Gemini models. So you have the Gemini app. You have Gemini available\non Google Cloud as the Code Assist, the assistant-- the\nCloud Assist inside the Console. You have Gemini on Workspace. So when you are talking\nabout developer experiences, we have the one\nspecific Gemini API with similar experiences as you\nhave on the no-code UI that we call AI Studio, where you\ncan start experimenting and developing your solutions\nusing the Gemini models, right? SHRESTHA BASU\nMALLICK: That's right. LUCIANO MARTINS: So\nbasically, the Gemini API is a very low barrier\nentry for having Gemini programmatic experiences. You have access to\nall public models there from the Gen Media ones,\nGemma, all the Gemini models, variants. You have a very generous\nfree of charge tier, where you can start\nexperimenting just after the session\nwithout concerns about credit cards, costs,\nbilling, anything of that. We keep developing\nmore SDKs for this API. So for now, you have available\none SDK for Python, JavaScript, Go, and Java. We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -0.8327459096908569
      },
      {
        "id": "b7ef9737-e40c-5ccb-b7cc-9cffc8fad35e",
        "doc_id": "4TE-KFXvhAk__chunk_008",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 967.05,
        "timestamp_end": 1087.58,
        "text": "but we've talked a lot\nabout building agents and agentic experiences. In this example,\nyou could imagine that you have a Python\nfunction on your laptop called weather_function. And maybe that calls\nyour own weather server to get the weather. What you can do is you can pass\nthe definition of that function to the Gemini API\nin JSON, including the function name and the\nparameters that it takes. Then what you can do is\nyou can write a prompt. So here, the prompt happens\nto be \"What's the temperature in London?\" When you send the\nprompt and the function to the model, what\nthe model will do is assess whether it makes sense\nto call that function based on your prompt. If so, it won't\nactually call it. But you can see in the\nfunction_call.name that it returns and the\nfunction_call.args, it returns the name of the\nfunction and the arguments to pass to it. So if you want, you're\nready to call this function on your laptop. And we have code that you can\ncopy and paste to do that. What's really cool too is this\nworks with multiple functions at the same time. So you can imagine you have a\nfunction like schedule a meeting or something like that, and\nyou can very easily-- well, with some work, you could build\nan agent to actually do that. So function calling\nis super important, and it works extremely well. So now, Joana's going\nto talk about GenMedia. JOANA CARRASQUEIRA: Awesome. So as you can see, what\nyou can build in the UI within AI Studio, also\navailable in the API. And also, just building on the\ncapabilities of our foundation models, our core\nintelligence also encompasses a powerful suite\nof generative media models. And they are designed to\ntransform creative experiences across content generation\nacross different modalities like images, video, and audio. And I would also like to\ndemo one of these new apps that we have in AI Studio. So I'm going back to\nthe laptop, and I'm going to show you something\nthat the team also just created. So AI Studio also got a facelift\nand has some new features.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -2.753065347671509
      },
      {
        "id": "bcce6feb-c12e-5253-92d4-926629505824",
        "doc_id": "gHHjDRDNUNU__chunk_022",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2746.57,
        "timestamp_end": 2767.29,
        "text": "more of the other features and\npossibilities with the models. And the last one is the Gemini\nCookbook, a GitHub repository that our team curates and keep\nupdating with a lot of sample experience for you. Thank you so much, Shrestha. Thank you so much you all. SHRESTHA BASU MALLICK: Thank you\nall for coming out to hear us. [CHEERING] [MUSIC PLAYING]",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -6.050359725952148
      }
    ],
    "relevant_doc_ids": [
      "gHHjDRDNUNU__chunk_000"
    ]
  },
  {
    "query": "how do Gemini embeddings work for semantic search and ranking",
    "reranked_docs": [
      {
        "id": "5ca49ab4-6582-50dd-a603-0cf29a8f97fb",
        "doc_id": "gHHjDRDNUNU__chunk_011",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 1387.2,
        "timestamp_end": 1519.6,
        "text": "So we are not just uploading\none PDF file with a lot of text and some charts and\nsome conclusions. You are also counting\non the Gemini ability to understand how the\ninformation on those documents or on those spreadsheets\nconnect to each other, especially if you have a more\ncomplex situation, where you are sending multiple PDF files or\nPDF files with spreadsheets and videos and everything. Without a huge, heavy\nlift from your side, you can count on Gemini\nto understand what's the message, what's\nthe information, what's the reasoning behind all that\ninformation and do the math or do the understanding\nfor you, right Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. I just want to give a special\ncall-out to two features here, which again, developers\nhave been asking us for. One is you can get bounding\nboxes through the API. And secondly, in addition\nto bounding boxes, you have what's called\nimage segmentation, which is get the bounding\nbox information for an object in the image. You get a classification of\nwhat that image might be, so that's sent to\nyou as metadata. And then you get this\nmasked segmentation of that specific area\nand that object as well. LUCIANO MARTINS: Nice. SHRESTHA BASU MALLICK: We\nalso support streaming. And you can set\nsystem instructions on top of whatever\nsystem instructions we already have in place. LUCIANO MARTINS: Yep. And I think that's the\nsame for the media models right, Shrestha? So now you have basically,\nthree main doors to use the media models. You have the Imagen 3,\nthe, Google DeepMind model with the best high\nquality image generated. You have one variant of Gemini\nthat we call Gemini Image Out that lets you\nalso generate images, but with two key\ndifferences from Imagen 3. First, you can have interleaved\noutputs, including text and images together, like\nhaving one explanation on step-by-step guide to do\nsome action, including visuals. And also, you can edit\nimages, right, Shrestha? So you can have one first\nversion of the image generated.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -3.7044289112091064
      },
      {
        "id": "cc910b44-8924-521e-9890-82d926ac6eb6",
        "doc_id": "4TE-KFXvhAk__chunk_003",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 363.97,
        "timestamp_end": 521.659,
        "text": "On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API and also on [INAUDIBLE] API\nfor real generating audio. And some of the use\ncases here that we had in mind when\nwe built this were more dynamic audiobooks,\nsome more engaging podcasts, or, even for those of\nyou in customer support, producing more natural\nvoices into your workflows. We also have enhanced\ntooling, which is really cool, because now you\ncan use Grounding with Google Search, also with Code Execution\ntogether in just one API call, and URL context, which you\nheard during the keynote, which provides the model with\ndepth content from web pages. And since you can chain\nit to other tools, it's actually really powerful\nto build search agents. So really cool stuff. Lastly, just to call out that\nwe now offer Gemini SDK support for MCP, which reduces a\nlot of developer friction and simplifies building\nagentic capabilities. So you don't want to\nmiss tomorrow's talk to learn more about this. And Google AI Studio. Who here uses Google AI Studio? Can I see some hands? OK, awesome. Awesome. We have a lot to chat\nafter this session then. Google AI Studio is the\nperfect place for anyone to start developing with AI. It's the simplest way to test\nthe latest DeepMind models. We typically bring them\nto Google AI Studio so you can start prototyping\nand playing with the models. And you don't need Google\nCloud knowledge in order to set up your environment. It is free of charge, and you\ncan create, and test, and save your prompts. And there's also starter\napps that will inspire you. And that's exactly what I'm\ngoing to demo here today for you. I am going to show\nyou some of the work that the team has been doing. So if we go to Google AI Studio\nhere, if we go into Build, and Mumble Jumble is\nsomething that we've literally just created, and\nit's one of my favorite apps. So Mumble Jumble is one\nof those applications that",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -3.7100746631622314
      },
      {
        "id": "5818d7d2-3012-5c4b-a76a-e93a3f646453",
        "doc_id": "4TE-KFXvhAk__chunk_002",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 230.74,
        "timestamp_end": 380.44,
        "text": "it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it because of its\nefficiency and speed, and it's now even better at\nalmost every single dimension. So we improved\nall the benchmarks across reasoning, coding,\nmultimodality, and also long context. Then we have our Gemini 2.0\nFlash, which is fast and cheap, works fine, and our\nGemini Nano, which is optimized for on-device tasks. And as you've heard, we've\nbeen shipping relentlessly. And I would like to give\nyou just a quick highlight of everything that we've\nbeen shipping in AI Studio and the Gemini API. There's a talk\ntomorrow that I would like to invite you to attend,\nwhich is by Shrestha Basu Mallick, a group product\nmanager on Gemini API, and Luciano Martins, our\ntechnical lead for Gemini API from DevRel. And they're going to do a deep\ndive into everything that's new within the Gemini\nAPI, so you definitely don't want to miss\nthat session tomorrow. But for now, just a glimpse to\nget you excited about what's new in AI Studio. We've built a new tab\nthat is called Build that instantly generates web apps. And it's really cool, because it\nenables developers and builders alike to prototype very\nquickly with natural language. We have a new generative media\nexperience in AI Studio as well, and I'm going to\ndemo all of this so you can see how\nit actually works. And we are always\nlistening to the community. We listen to your\nfeedback, and we always build with developers in mind. And that's why that\nsome of these features were actually\nrequested by community, and that's what happened with\nthe built-in new dashboard. You requested. We built it. And we also have some\nnew Native Audio and TTS support in AI Studio. On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -3.8081090450286865
      },
      {
        "id": "b3910d5f-a24c-56c2-8f8e-90d35889ff56",
        "doc_id": "gHHjDRDNUNU__chunk_017",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2130.98,
        "timestamp_end": 2271.49,
        "text": "If you're building\na research agent, maybe you want to\nuse the Chat API. If you're building\na gaming agent, maybe you want to use the real\ntime API or a customer support agent. Of course, you can build\nresearch agents also on the real time API. But in general, you now\nhave these two APIs. And then we've talked\nabout tools a lot. So all I'll say is\ngive us feedback in terms of what are some\nof the other tools that you would see Google make available\nthrough the Gemini API. LUCIANO MARTINS: Yeah. So as you just said,\nthe Gemini 2.5 Pro-- 2.5 models are thinking\ncapable, so they can do more advanced and\nmore complex reasoning. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS:\nAnd maybe we could highlight that now,\nspecifically for the 2.5 Flash, we have the ability of\nusing what we just mentioned, the thinking budgets, where\nyou can calibrate how deep and how much the model\nwill do reasoning cycles. And also for both models,\nwe can have access to what we are calling\nthought summaries. SHRESTHA BASU MALLICK: Yes. LUCIANO MARTINS: So\nbasically you are not only counting on the model\nability to reason, but also you can see on your\nresponse which key steps or which key reasoning\nthoughts the model had to get to our conclusion. So basically, that's a\npretty basic Gemini API code using the Python SDK. For those of you who\nnever used the SDK before, it's pretty straightforward. So three key blocks. You need to import the SDK,\nyou instantiate the client, and then you interact\nwith the model you want. Here we are interacting\nwith the Gemini 2.5 Flash. You send your prompts. And then you have\none specific bit called ThinkingConfig,\nwhere, in this code, you are asking the API\nto include the thought summaries on your\nresponse, and that's to include thoughts\nequals true, and also the amount of tokens you want\nto use during the reasoning process. On your response,\nyou're going to have part of the response will\nbe the thought summaries. So it's separated from the\nfinal answer by intention. If you want to keep\nthose on your backend,",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -6.201044082641602
      },
      {
        "id": "bcce6feb-c12e-5253-92d4-926629505824",
        "doc_id": "gHHjDRDNUNU__chunk_022",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2746.57,
        "timestamp_end": 2767.29,
        "text": "more of the other features and\npossibilities with the models. And the last one is the Gemini\nCookbook, a GitHub repository that our team curates and keep\nupdating with a lot of sample experience for you. Thank you so much, Shrestha. Thank you so much you all. SHRESTHA BASU MALLICK: Thank you\nall for coming out to hear us. [CHEERING] [MUSIC PLAYING]",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -10.762039184570312
      }
    ],
    "relevant_doc_ids": [
      "gHHjDRDNUNU__chunk_001"
    ]
  },
  {
    "query": "how to run Gemini Nano on Android devices for on-device intelligence",
    "reranked_docs": [
      {
        "id": "5818d7d2-3012-5c4b-a76a-e93a3f646453",
        "doc_id": "4TE-KFXvhAk__chunk_002",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 230.74,
        "timestamp_end": 380.44,
        "text": "it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it because of its\nefficiency and speed, and it's now even better at\nalmost every single dimension. So we improved\nall the benchmarks across reasoning, coding,\nmultimodality, and also long context. Then we have our Gemini 2.0\nFlash, which is fast and cheap, works fine, and our\nGemini Nano, which is optimized for on-device tasks. And as you've heard, we've\nbeen shipping relentlessly. And I would like to give\nyou just a quick highlight of everything that we've\nbeen shipping in AI Studio and the Gemini API. There's a talk\ntomorrow that I would like to invite you to attend,\nwhich is by Shrestha Basu Mallick, a group product\nmanager on Gemini API, and Luciano Martins, our\ntechnical lead for Gemini API from DevRel. And they're going to do a deep\ndive into everything that's new within the Gemini\nAPI, so you definitely don't want to miss\nthat session tomorrow. But for now, just a glimpse to\nget you excited about what's new in AI Studio. We've built a new tab\nthat is called Build that instantly generates web apps. And it's really cool, because it\nenables developers and builders alike to prototype very\nquickly with natural language. We have a new generative media\nexperience in AI Studio as well, and I'm going to\ndemo all of this so you can see how\nit actually works. And we are always\nlistening to the community. We listen to your\nfeedback, and we always build with developers in mind. And that's why that\nsome of these features were actually\nrequested by community, and that's what happened with\nthe built-in new dashboard. You requested. We built it. And we also have some\nnew Native Audio and TTS support in AI Studio. On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -0.49791890382766724
      },
      {
        "id": "1bf79af9-fc9e-5248-b56a-1c7adf068aff",
        "doc_id": "4TE-KFXvhAk__chunk_007",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 880.26,
        "timestamp_end": 979.77,
        "text": "So it's just really\nquick and easy to do. OK, we can go back\nto the slides. So very, very\nquickly, as a recap, Gemini Developer API is the\neasiest way to get started. It's super lightweight,\nit's fast to install, and you can get up and running\nhonestly in about a minute. OK? I will use the clicker. This is the flow to get\nstarted with Google AI Studio. Go to Google AI Studio. Get your key. Try one of the code\nexamples on ai.google.dev or in the cookbook. I see people taking pictures. That makes me happy. Please try this. We spent so much time\non making this easy, and I hope it works for you. If not, please file an\nissue and we'll get on it. This is the Gen AI SDK\nfor the Gemini API, and this is something\nwe've been rolling out. It's our latest SDK. We've been rolling\nit out gradually over the course of\nthe last six months. It's super user friendly. It's really easy to use. Really, the only point\nI want to make here, because I don't want\nto read the code examples or the\ndocumentation to you, you can call the API\nin a few lines of code. Basically add your key, select\na model, write a prompt, you can go ahead and call it. You can also get access\nto advanced functionality in one line of code. So if you'd like to get the\nthinking summaries that Joana showed you, you can just\nadd a ThinkingConfig, say include the thoughts. Now, you've got the\nthinking summaries. And a good use\ncase for this could be any time you need to\nexplain the model's reasoning. Particularly, you can\nimagine, if you're building an education\napp or a tutoring app, you can get the\nthinking summaries. In addition to\nreally cool things that you can do with\na single line of code, there's some more advanced stuff\nthat you can do with the SDK as well. So I know there's a lot\nof code on this slide, but we've talked a lot\nabout building agents and agentic experiences. In this example,\nyou could imagine that you have a Python\nfunction on your laptop called weather_function. And maybe that calls\nyour own weather server to get the weather.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -3.5495920181274414
      },
      {
        "id": "b0ee6a81-6918-5bbf-8b1e-baa4084f5b62",
        "doc_id": "gHHjDRDNUNU__chunk_002",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 254.53,
        "timestamp_end": 382.64,
        "text": "when we think about the key\nstandard benchmarks, Shrestha? SHRESTHA BASU\nMALLICK: Yeah, we'll go to the benchmarks\nin a second. But the message\nthat I want everyone to take away from this\nslide is whether it's a super complex task, or whether\nit's on-device processing, you now have a Gemini model\nthat you can use for it. And it's pretty powerful. And it's pretty price effective. LUCIANO MARTINS: Yeah,\nmaybe it is also worth mentioning, Shrestha,\nthat some folks here may be developing with\nGemini since it was released. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS: So using the\nAI Studio, using the Gemini API, the older versions\nof Gemini like 2.0, 1.5 are still available. But we really\nencourage everybody to start experimenting\nwith the newer ones with better capabilities\nand better performance. SHRESTHA BASU MALLICK:\nThat's correct. So 2.0 Flash continues to be one\nof our most widely-used models, just again, because of how good\nit is for the price it's at. And we have a lot of people\nwho are still using it. And then we also have some\npeople on the 1.5 models. But we're encouraging\npeople now, we're two generations\nahead, to start using the 2.5 series of\nmodels and give us feedback. LUCIANO MARTINS: Nice. So please tell us a little bit\nabout the benchmarks, Shrestha. SHRESTHA BASU MALLICK:\nYeah, next slide. Yeah, so we're not going\nto spend a lot of time on the benchmarks. But the two key points\nthat I'd like to hit here is number one, is one\nway in which we measure how good these models are is\nby putting them on LMArena, which is where developers\nand builders like yourself test out\nall these models and give it an ELO rating. And we're, of course, very\nproud that right now you have three Gemini models in\nthe top 10 in the LMArena. LUCIANO MARTINS:\nIncluding the top one. SHRESTHA BASU\nMALLICK: Including, how did I forget that? We are number one guys,\nand we're very proud of it. [CHEERING] Thank you. One of the things,\none of the use cases that has been really emerging\nin the last couple of months",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -4.014204025268555
      },
      {
        "id": "51a07f69-109c-5809-b618-2760e83de9e5",
        "doc_id": "gHHjDRDNUNU__chunk_007",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 890.57,
        "timestamp_end": 1021.23,
        "text": "We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there. If you are a Google\nColab user, you have ways to interact with\nthe Gemini models as well. So the key idea is to\nmake it easier for you. No matter where you are having\nyour development experience, We are trying to bring\nGemini API closer to you. SHRESTHA BASU MALLICK:\nThat's correct. And I think this is\nalso a good place to make a plug for Google\nAI Studio, which of course, everyone knows about and loves. It is the place where\na lot of our developers first test out the\ncapabilities of the API before committing to\nbuilding applications at scale with the API. And Google AI Studio now\nhas code generation also. LUCIANO MARTINS: Absolutely. SHRESTHA BASU MALLICK: So this\nis a quick, high level overview of all the components that are\navailable through the Gemini API. It's pretty standard. You send in your prompt, you get\na response back from the model. If there's a tool\ncall needed, we do support a set of\nfirst party Google tools, as well as function calling. So in terms of the first party\nGoogle tools that we support, we, of course, support\nGoogle Search as a tool. The best search engine\nout there is now available as a tool for the\nmodel to call when it needs to. Now in addition to the\ninformation that you get out of Search, if you want to\nretrieve more in-depth content from web pages for\napplications such as you're building your own version\nof a research agent, so you pull some\ninformation from Search. You pull a set of\nURLs, but if you want to extract a little\nmore in depth content, we just released a new tool\nat I/O called URL Context. And then, of course, you can\ntake all that information. And if you want to create\nbeautiful charts out of it or run some analysis,\nwe make code execution available as a tool to\nyou via the API as well. Then, of course, you\nhave function calling.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -4.132187843322754
      },
      {
        "id": "07bff983-b0de-5d12-9cb5-8b3d3c7df76c",
        "doc_id": "gHHjDRDNUNU__chunk_006",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 751.58,
        "timestamp_end": 904.85,
        "text": "mode called Deep Think. And the idea behind Deep\nThink is that the model 2.5 Pro can think through various\npossible answers to a problem before giving you\nthe best answer. So that is available\nin trusted testers, but will be rolled\nout more widely soon. And then we also have\nGemini Diffusion, which is the first time\nwe are testing a diffusion architecture as opposed to an\nautoregressive architecture. And these models are faster\nthan the fastest model out today and for almost\nsimilar, the performance. So try out both when you have\naccess to both Gemini Deep Diffusion and Deep Think. LUCIANO MARTINS: Perfect. SHRESTHA BASU MALLICK: All\nright, so the Gemini API. LUCIANO MARTINS: So yeah, if\nyou start thinking, especially after the keynote\nspeakers today, there are many surfaces where\nyou can interact with the Gemini models. So you have the Gemini app. You have Gemini available\non Google Cloud as the Code Assist, the assistant-- the\nCloud Assist inside the Console. You have Gemini on Workspace. So when you are talking\nabout developer experiences, we have the one\nspecific Gemini API with similar experiences as you\nhave on the no-code UI that we call AI Studio, where you\ncan start experimenting and developing your solutions\nusing the Gemini models, right? SHRESTHA BASU\nMALLICK: That's right. LUCIANO MARTINS: So\nbasically, the Gemini API is a very low barrier\nentry for having Gemini programmatic experiences. You have access to\nall public models there from the Gen Media ones,\nGemma, all the Gemini models, variants. You have a very generous\nfree of charge tier, where you can start\nexperimenting just after the session\nwithout concerns about credit cards, costs,\nbilling, anything of that. We keep developing\nmore SDKs for this API. So for now, you have available\none SDK for Python, JavaScript, Go, and Java. We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -6.214978218078613
      }
    ],
    "relevant_doc_ids": [
      "gHHjDRDNUNU__chunk_001"
    ]
  },
  {
    "query": "what are the different Gemini AI models and what are they best for",
    "reranked_docs": [
      {
        "id": "07bff983-b0de-5d12-9cb5-8b3d3c7df76c",
        "doc_id": "gHHjDRDNUNU__chunk_006",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 751.58,
        "timestamp_end": 904.85,
        "text": "mode called Deep Think. And the idea behind Deep\nThink is that the model 2.5 Pro can think through various\npossible answers to a problem before giving you\nthe best answer. So that is available\nin trusted testers, but will be rolled\nout more widely soon. And then we also have\nGemini Diffusion, which is the first time\nwe are testing a diffusion architecture as opposed to an\nautoregressive architecture. And these models are faster\nthan the fastest model out today and for almost\nsimilar, the performance. So try out both when you have\naccess to both Gemini Deep Diffusion and Deep Think. LUCIANO MARTINS: Perfect. SHRESTHA BASU MALLICK: All\nright, so the Gemini API. LUCIANO MARTINS: So yeah, if\nyou start thinking, especially after the keynote\nspeakers today, there are many surfaces where\nyou can interact with the Gemini models. So you have the Gemini app. You have Gemini available\non Google Cloud as the Code Assist, the assistant-- the\nCloud Assist inside the Console. You have Gemini on Workspace. So when you are talking\nabout developer experiences, we have the one\nspecific Gemini API with similar experiences as you\nhave on the no-code UI that we call AI Studio, where you\ncan start experimenting and developing your solutions\nusing the Gemini models, right? SHRESTHA BASU\nMALLICK: That's right. LUCIANO MARTINS: So\nbasically, the Gemini API is a very low barrier\nentry for having Gemini programmatic experiences. You have access to\nall public models there from the Gen Media ones,\nGemma, all the Gemini models, variants. You have a very generous\nfree of charge tier, where you can start\nexperimenting just after the session\nwithout concerns about credit cards, costs,\nbilling, anything of that. We keep developing\nmore SDKs for this API. So for now, you have available\none SDK for Python, JavaScript, Go, and Java. We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 3.0227229595184326
      },
      {
        "id": "b0ee6a81-6918-5bbf-8b1e-baa4084f5b62",
        "doc_id": "gHHjDRDNUNU__chunk_002",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 254.53,
        "timestamp_end": 382.64,
        "text": "when we think about the key\nstandard benchmarks, Shrestha? SHRESTHA BASU\nMALLICK: Yeah, we'll go to the benchmarks\nin a second. But the message\nthat I want everyone to take away from this\nslide is whether it's a super complex task, or whether\nit's on-device processing, you now have a Gemini model\nthat you can use for it. And it's pretty powerful. And it's pretty price effective. LUCIANO MARTINS: Yeah,\nmaybe it is also worth mentioning, Shrestha,\nthat some folks here may be developing with\nGemini since it was released. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS: So using the\nAI Studio, using the Gemini API, the older versions\nof Gemini like 2.0, 1.5 are still available. But we really\nencourage everybody to start experimenting\nwith the newer ones with better capabilities\nand better performance. SHRESTHA BASU MALLICK:\nThat's correct. So 2.0 Flash continues to be one\nof our most widely-used models, just again, because of how good\nit is for the price it's at. And we have a lot of people\nwho are still using it. And then we also have some\npeople on the 1.5 models. But we're encouraging\npeople now, we're two generations\nahead, to start using the 2.5 series of\nmodels and give us feedback. LUCIANO MARTINS: Nice. So please tell us a little bit\nabout the benchmarks, Shrestha. SHRESTHA BASU MALLICK:\nYeah, next slide. Yeah, so we're not going\nto spend a lot of time on the benchmarks. But the two key points\nthat I'd like to hit here is number one, is one\nway in which we measure how good these models are is\nby putting them on LMArena, which is where developers\nand builders like yourself test out\nall these models and give it an ELO rating. And we're, of course, very\nproud that right now you have three Gemini models in\nthe top 10 in the LMArena. LUCIANO MARTINS:\nIncluding the top one. SHRESTHA BASU\nMALLICK: Including, how did I forget that? We are number one guys,\nand we're very proud of it. [CHEERING] Thank you. One of the things,\none of the use cases that has been really emerging\nin the last couple of months",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 1.6326861381530762
      },
      {
        "id": "51a07f69-109c-5809-b618-2760e83de9e5",
        "doc_id": "gHHjDRDNUNU__chunk_007",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 890.57,
        "timestamp_end": 1021.23,
        "text": "We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there. If you are a Google\nColab user, you have ways to interact with\nthe Gemini models as well. So the key idea is to\nmake it easier for you. No matter where you are having\nyour development experience, We are trying to bring\nGemini API closer to you. SHRESTHA BASU MALLICK:\nThat's correct. And I think this is\nalso a good place to make a plug for Google\nAI Studio, which of course, everyone knows about and loves. It is the place where\na lot of our developers first test out the\ncapabilities of the API before committing to\nbuilding applications at scale with the API. And Google AI Studio now\nhas code generation also. LUCIANO MARTINS: Absolutely. SHRESTHA BASU MALLICK: So this\nis a quick, high level overview of all the components that are\navailable through the Gemini API. It's pretty standard. You send in your prompt, you get\na response back from the model. If there's a tool\ncall needed, we do support a set of\nfirst party Google tools, as well as function calling. So in terms of the first party\nGoogle tools that we support, we, of course, support\nGoogle Search as a tool. The best search engine\nout there is now available as a tool for the\nmodel to call when it needs to. Now in addition to the\ninformation that you get out of Search, if you want to\nretrieve more in-depth content from web pages for\napplications such as you're building your own version\nof a research agent, so you pull some\ninformation from Search. You pull a set of\nURLs, but if you want to extract a little\nmore in depth content, we just released a new tool\nat I/O called URL Context. And then, of course, you can\ntake all that information. And if you want to create\nbeautiful charts out of it or run some analysis,\nwe make code execution available as a tool to\nyou via the API as well. Then, of course, you\nhave function calling.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 1.0632293224334717
      },
      {
        "id": "bcce6feb-c12e-5253-92d4-926629505824",
        "doc_id": "gHHjDRDNUNU__chunk_022",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2746.57,
        "timestamp_end": 2767.29,
        "text": "more of the other features and\npossibilities with the models. And the last one is the Gemini\nCookbook, a GitHub repository that our team curates and keep\nupdating with a lot of sample experience for you. Thank you so much, Shrestha. Thank you so much you all. SHRESTHA BASU MALLICK: Thank you\nall for coming out to hear us. [CHEERING] [MUSIC PLAYING]",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -1.156635046005249
      },
      {
        "id": "b3910d5f-a24c-56c2-8f8e-90d35889ff56",
        "doc_id": "gHHjDRDNUNU__chunk_017",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2130.98,
        "timestamp_end": 2271.49,
        "text": "If you're building\na research agent, maybe you want to\nuse the Chat API. If you're building\na gaming agent, maybe you want to use the real\ntime API or a customer support agent. Of course, you can build\nresearch agents also on the real time API. But in general, you now\nhave these two APIs. And then we've talked\nabout tools a lot. So all I'll say is\ngive us feedback in terms of what are some\nof the other tools that you would see Google make available\nthrough the Gemini API. LUCIANO MARTINS: Yeah. So as you just said,\nthe Gemini 2.5 Pro-- 2.5 models are thinking\ncapable, so they can do more advanced and\nmore complex reasoning. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS:\nAnd maybe we could highlight that now,\nspecifically for the 2.5 Flash, we have the ability of\nusing what we just mentioned, the thinking budgets, where\nyou can calibrate how deep and how much the model\nwill do reasoning cycles. And also for both models,\nwe can have access to what we are calling\nthought summaries. SHRESTHA BASU MALLICK: Yes. LUCIANO MARTINS: So\nbasically you are not only counting on the model\nability to reason, but also you can see on your\nresponse which key steps or which key reasoning\nthoughts the model had to get to our conclusion. So basically, that's a\npretty basic Gemini API code using the Python SDK. For those of you who\nnever used the SDK before, it's pretty straightforward. So three key blocks. You need to import the SDK,\nyou instantiate the client, and then you interact\nwith the model you want. Here we are interacting\nwith the Gemini 2.5 Flash. You send your prompts. And then you have\none specific bit called ThinkingConfig,\nwhere, in this code, you are asking the API\nto include the thought summaries on your\nresponse, and that's to include thoughts\nequals true, and also the amount of tokens you want\nto use during the reasoning process. On your response,\nyou're going to have part of the response will\nbe the thought summaries. So it's separated from the\nfinal answer by intention. If you want to keep\nthose on your backend,",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -2.1112112998962402
      }
    ],
    "relevant_doc_ids": [
      "gHHjDRDNUNU__chunk_001"
    ]
  },
  {
    "query": "how does the Gemini 2.5 series compare to older versions",
    "reranked_docs": [
      {
        "id": "b0ee6a81-6918-5bbf-8b1e-baa4084f5b62",
        "doc_id": "gHHjDRDNUNU__chunk_002",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 254.53,
        "timestamp_end": 382.64,
        "text": "when we think about the key\nstandard benchmarks, Shrestha? SHRESTHA BASU\nMALLICK: Yeah, we'll go to the benchmarks\nin a second. But the message\nthat I want everyone to take away from this\nslide is whether it's a super complex task, or whether\nit's on-device processing, you now have a Gemini model\nthat you can use for it. And it's pretty powerful. And it's pretty price effective. LUCIANO MARTINS: Yeah,\nmaybe it is also worth mentioning, Shrestha,\nthat some folks here may be developing with\nGemini since it was released. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS: So using the\nAI Studio, using the Gemini API, the older versions\nof Gemini like 2.0, 1.5 are still available. But we really\nencourage everybody to start experimenting\nwith the newer ones with better capabilities\nand better performance. SHRESTHA BASU MALLICK:\nThat's correct. So 2.0 Flash continues to be one\nof our most widely-used models, just again, because of how good\nit is for the price it's at. And we have a lot of people\nwho are still using it. And then we also have some\npeople on the 1.5 models. But we're encouraging\npeople now, we're two generations\nahead, to start using the 2.5 series of\nmodels and give us feedback. LUCIANO MARTINS: Nice. So please tell us a little bit\nabout the benchmarks, Shrestha. SHRESTHA BASU MALLICK:\nYeah, next slide. Yeah, so we're not going\nto spend a lot of time on the benchmarks. But the two key points\nthat I'd like to hit here is number one, is one\nway in which we measure how good these models are is\nby putting them on LMArena, which is where developers\nand builders like yourself test out\nall these models and give it an ELO rating. And we're, of course, very\nproud that right now you have three Gemini models in\nthe top 10 in the LMArena. LUCIANO MARTINS:\nIncluding the top one. SHRESTHA BASU\nMALLICK: Including, how did I forget that? We are number one guys,\nand we're very proud of it. [CHEERING] Thank you. One of the things,\none of the use cases that has been really emerging\nin the last couple of months",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 2.2988767623901367
      },
      {
        "id": "2eaf072c-ca60-5cbb-8434-5fb172bea8aa",
        "doc_id": "gHHjDRDNUNU__chunk_003",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 364.16,
        "timestamp_end": 515.85,
        "text": "SHRESTHA BASU\nMALLICK: Including, how did I forget that? We are number one guys,\nand we're very proud of it. [CHEERING] Thank you. One of the things,\none of the use cases that has been really emerging\nin the last couple of months is, of course, app creation\nand live coding and going from 0 to 1. The best leaderboard right now\nfor that is the WebDev Arena. And we again, are number one\nwith 2.5 Pro on WebDev Arena. So if you are trying to build\n0 to 1 or few shot apps, Gemini 2.5 Pro is a\ngreat model for that. But in addition to\nuser preferences, we also like to measure\nhow well our models are doing based on rigorous\nacademic benchmarks. And so I'm not going to\ngo into a lot of details on these slides. These are available\nin blog posts. Reach out to me\nand Luciano if you want to know how exactly\nthese models are doing. But the TL;DR here is whether\nit's in highly complex domain-specific questions,\nwhether it's in coding, whether it's in\nmultimodal understanding, the Gemini 2.5 Pro is leading\non a lot of these academic benchmarks as well. Finally, performance is\none side of the coin. The other side of the\ncoin is, of course, price. This is a chart from Swyx that's\nbeen very popular on Twitter for the last few years-- on X, I'm sorry for\nthe last few months. But this really shows that\neven for price performance, the Gemini 2.5 models are\nreally at the frontier. Luciano? LUCIANO MARTINS:\nYeah, and I think it is worth\nmentioning, Shrestha, especially for those\nof you who watched Sundar's keynote yesterday and\nalso the developer's keynotes. Inside the DeepMind, we\nare doing this huge effort of trying to cover the\ndifferent aspects of experiences you may have. So we will talk a lot\nabout Gemini here. But also, we have\nthe model family that we are calling\nGenMedia, where you are able to generate high\nquality images, high quality videos. You also have-- and there is a\ncolleague from our team, Paul Ruiz, who is delivering\na talk this afternoon related to the Gemini Robotics,\nthe Gemini family of models",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 0.4453841745853424
      },
      {
        "id": "07bff983-b0de-5d12-9cb5-8b3d3c7df76c",
        "doc_id": "gHHjDRDNUNU__chunk_006",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 751.58,
        "timestamp_end": 904.85,
        "text": "mode called Deep Think. And the idea behind Deep\nThink is that the model 2.5 Pro can think through various\npossible answers to a problem before giving you\nthe best answer. So that is available\nin trusted testers, but will be rolled\nout more widely soon. And then we also have\nGemini Diffusion, which is the first time\nwe are testing a diffusion architecture as opposed to an\nautoregressive architecture. And these models are faster\nthan the fastest model out today and for almost\nsimilar, the performance. So try out both when you have\naccess to both Gemini Deep Diffusion and Deep Think. LUCIANO MARTINS: Perfect. SHRESTHA BASU MALLICK: All\nright, so the Gemini API. LUCIANO MARTINS: So yeah, if\nyou start thinking, especially after the keynote\nspeakers today, there are many surfaces where\nyou can interact with the Gemini models. So you have the Gemini app. You have Gemini available\non Google Cloud as the Code Assist, the assistant-- the\nCloud Assist inside the Console. You have Gemini on Workspace. So when you are talking\nabout developer experiences, we have the one\nspecific Gemini API with similar experiences as you\nhave on the no-code UI that we call AI Studio, where you\ncan start experimenting and developing your solutions\nusing the Gemini models, right? SHRESTHA BASU\nMALLICK: That's right. LUCIANO MARTINS: So\nbasically, the Gemini API is a very low barrier\nentry for having Gemini programmatic experiences. You have access to\nall public models there from the Gen Media ones,\nGemma, all the Gemini models, variants. You have a very generous\nfree of charge tier, where you can start\nexperimenting just after the session\nwithout concerns about credit cards, costs,\nbilling, anything of that. We keep developing\nmore SDKs for this API. So for now, you have available\none SDK for Python, JavaScript, Go, and Java. We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -0.2761947512626648
      },
      {
        "id": "b3910d5f-a24c-56c2-8f8e-90d35889ff56",
        "doc_id": "gHHjDRDNUNU__chunk_017",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2130.98,
        "timestamp_end": 2271.49,
        "text": "If you're building\na research agent, maybe you want to\nuse the Chat API. If you're building\na gaming agent, maybe you want to use the real\ntime API or a customer support agent. Of course, you can build\nresearch agents also on the real time API. But in general, you now\nhave these two APIs. And then we've talked\nabout tools a lot. So all I'll say is\ngive us feedback in terms of what are some\nof the other tools that you would see Google make available\nthrough the Gemini API. LUCIANO MARTINS: Yeah. So as you just said,\nthe Gemini 2.5 Pro-- 2.5 models are thinking\ncapable, so they can do more advanced and\nmore complex reasoning. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS:\nAnd maybe we could highlight that now,\nspecifically for the 2.5 Flash, we have the ability of\nusing what we just mentioned, the thinking budgets, where\nyou can calibrate how deep and how much the model\nwill do reasoning cycles. And also for both models,\nwe can have access to what we are calling\nthought summaries. SHRESTHA BASU MALLICK: Yes. LUCIANO MARTINS: So\nbasically you are not only counting on the model\nability to reason, but also you can see on your\nresponse which key steps or which key reasoning\nthoughts the model had to get to our conclusion. So basically, that's a\npretty basic Gemini API code using the Python SDK. For those of you who\nnever used the SDK before, it's pretty straightforward. So three key blocks. You need to import the SDK,\nyou instantiate the client, and then you interact\nwith the model you want. Here we are interacting\nwith the Gemini 2.5 Flash. You send your prompts. And then you have\none specific bit called ThinkingConfig,\nwhere, in this code, you are asking the API\nto include the thought summaries on your\nresponse, and that's to include thoughts\nequals true, and also the amount of tokens you want\nto use during the reasoning process. On your response,\nyou're going to have part of the response will\nbe the thought summaries. So it's separated from the\nfinal answer by intention. If you want to keep\nthose on your backend,",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -1.1429872512817383
      },
      {
        "id": "bcce6feb-c12e-5253-92d4-926629505824",
        "doc_id": "gHHjDRDNUNU__chunk_022",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2746.57,
        "timestamp_end": 2767.29,
        "text": "more of the other features and\npossibilities with the models. And the last one is the Gemini\nCookbook, a GitHub repository that our team curates and keep\nupdating with a lot of sample experience for you. Thank you so much, Shrestha. Thank you so much you all. SHRESTHA BASU MALLICK: Thank you\nall for coming out to hear us. [CHEERING] [MUSIC PLAYING]",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -7.341142654418945
      }
    ],
    "relevant_doc_ids": [
      "gHHjDRDNUNU__chunk_002"
    ]
  },
  {
    "query": "what are the best Gemini models right now for on-device tasks",
    "reranked_docs": [
      {
        "id": "5818d7d2-3012-5c4b-a76a-e93a3f646453",
        "doc_id": "4TE-KFXvhAk__chunk_002",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 230.74,
        "timestamp_end": 380.44,
        "text": "it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it because of its\nefficiency and speed, and it's now even better at\nalmost every single dimension. So we improved\nall the benchmarks across reasoning, coding,\nmultimodality, and also long context. Then we have our Gemini 2.0\nFlash, which is fast and cheap, works fine, and our\nGemini Nano, which is optimized for on-device tasks. And as you've heard, we've\nbeen shipping relentlessly. And I would like to give\nyou just a quick highlight of everything that we've\nbeen shipping in AI Studio and the Gemini API. There's a talk\ntomorrow that I would like to invite you to attend,\nwhich is by Shrestha Basu Mallick, a group product\nmanager on Gemini API, and Luciano Martins, our\ntechnical lead for Gemini API from DevRel. And they're going to do a deep\ndive into everything that's new within the Gemini\nAPI, so you definitely don't want to miss\nthat session tomorrow. But for now, just a glimpse to\nget you excited about what's new in AI Studio. We've built a new tab\nthat is called Build that instantly generates web apps. And it's really cool, because it\nenables developers and builders alike to prototype very\nquickly with natural language. We have a new generative media\nexperience in AI Studio as well, and I'm going to\ndemo all of this so you can see how\nit actually works. And we are always\nlistening to the community. We listen to your\nfeedback, and we always build with developers in mind. And that's why that\nsome of these features were actually\nrequested by community, and that's what happened with\nthe built-in new dashboard. You requested. We built it. And we also have some\nnew Native Audio and TTS support in AI Studio. On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 4.296703338623047
      },
      {
        "id": "b0ee6a81-6918-5bbf-8b1e-baa4084f5b62",
        "doc_id": "gHHjDRDNUNU__chunk_002",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 254.53,
        "timestamp_end": 382.64,
        "text": "when we think about the key\nstandard benchmarks, Shrestha? SHRESTHA BASU\nMALLICK: Yeah, we'll go to the benchmarks\nin a second. But the message\nthat I want everyone to take away from this\nslide is whether it's a super complex task, or whether\nit's on-device processing, you now have a Gemini model\nthat you can use for it. And it's pretty powerful. And it's pretty price effective. LUCIANO MARTINS: Yeah,\nmaybe it is also worth mentioning, Shrestha,\nthat some folks here may be developing with\nGemini since it was released. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS: So using the\nAI Studio, using the Gemini API, the older versions\nof Gemini like 2.0, 1.5 are still available. But we really\nencourage everybody to start experimenting\nwith the newer ones with better capabilities\nand better performance. SHRESTHA BASU MALLICK:\nThat's correct. So 2.0 Flash continues to be one\nof our most widely-used models, just again, because of how good\nit is for the price it's at. And we have a lot of people\nwho are still using it. And then we also have some\npeople on the 1.5 models. But we're encouraging\npeople now, we're two generations\nahead, to start using the 2.5 series of\nmodels and give us feedback. LUCIANO MARTINS: Nice. So please tell us a little bit\nabout the benchmarks, Shrestha. SHRESTHA BASU MALLICK:\nYeah, next slide. Yeah, so we're not going\nto spend a lot of time on the benchmarks. But the two key points\nthat I'd like to hit here is number one, is one\nway in which we measure how good these models are is\nby putting them on LMArena, which is where developers\nand builders like yourself test out\nall these models and give it an ELO rating. And we're, of course, very\nproud that right now you have three Gemini models in\nthe top 10 in the LMArena. LUCIANO MARTINS:\nIncluding the top one. SHRESTHA BASU\nMALLICK: Including, how did I forget that? We are number one guys,\nand we're very proud of it. [CHEERING] Thank you. One of the things,\none of the use cases that has been really emerging\nin the last couple of months",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 4.016523838043213
      },
      {
        "id": "07bff983-b0de-5d12-9cb5-8b3d3c7df76c",
        "doc_id": "gHHjDRDNUNU__chunk_006",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 751.58,
        "timestamp_end": 904.85,
        "text": "mode called Deep Think. And the idea behind Deep\nThink is that the model 2.5 Pro can think through various\npossible answers to a problem before giving you\nthe best answer. So that is available\nin trusted testers, but will be rolled\nout more widely soon. And then we also have\nGemini Diffusion, which is the first time\nwe are testing a diffusion architecture as opposed to an\nautoregressive architecture. And these models are faster\nthan the fastest model out today and for almost\nsimilar, the performance. So try out both when you have\naccess to both Gemini Deep Diffusion and Deep Think. LUCIANO MARTINS: Perfect. SHRESTHA BASU MALLICK: All\nright, so the Gemini API. LUCIANO MARTINS: So yeah, if\nyou start thinking, especially after the keynote\nspeakers today, there are many surfaces where\nyou can interact with the Gemini models. So you have the Gemini app. You have Gemini available\non Google Cloud as the Code Assist, the assistant-- the\nCloud Assist inside the Console. You have Gemini on Workspace. So when you are talking\nabout developer experiences, we have the one\nspecific Gemini API with similar experiences as you\nhave on the no-code UI that we call AI Studio, where you\ncan start experimenting and developing your solutions\nusing the Gemini models, right? SHRESTHA BASU\nMALLICK: That's right. LUCIANO MARTINS: So\nbasically, the Gemini API is a very low barrier\nentry for having Gemini programmatic experiences. You have access to\nall public models there from the Gen Media ones,\nGemma, all the Gemini models, variants. You have a very generous\nfree of charge tier, where you can start\nexperimenting just after the session\nwithout concerns about credit cards, costs,\nbilling, anything of that. We keep developing\nmore SDKs for this API. So for now, you have available\none SDK for Python, JavaScript, Go, and Java. We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 1.1958656311035156
      },
      {
        "id": "b3910d5f-a24c-56c2-8f8e-90d35889ff56",
        "doc_id": "gHHjDRDNUNU__chunk_017",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2130.98,
        "timestamp_end": 2271.49,
        "text": "If you're building\na research agent, maybe you want to\nuse the Chat API. If you're building\na gaming agent, maybe you want to use the real\ntime API or a customer support agent. Of course, you can build\nresearch agents also on the real time API. But in general, you now\nhave these two APIs. And then we've talked\nabout tools a lot. So all I'll say is\ngive us feedback in terms of what are some\nof the other tools that you would see Google make available\nthrough the Gemini API. LUCIANO MARTINS: Yeah. So as you just said,\nthe Gemini 2.5 Pro-- 2.5 models are thinking\ncapable, so they can do more advanced and\nmore complex reasoning. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS:\nAnd maybe we could highlight that now,\nspecifically for the 2.5 Flash, we have the ability of\nusing what we just mentioned, the thinking budgets, where\nyou can calibrate how deep and how much the model\nwill do reasoning cycles. And also for both models,\nwe can have access to what we are calling\nthought summaries. SHRESTHA BASU MALLICK: Yes. LUCIANO MARTINS: So\nbasically you are not only counting on the model\nability to reason, but also you can see on your\nresponse which key steps or which key reasoning\nthoughts the model had to get to our conclusion. So basically, that's a\npretty basic Gemini API code using the Python SDK. For those of you who\nnever used the SDK before, it's pretty straightforward. So three key blocks. You need to import the SDK,\nyou instantiate the client, and then you interact\nwith the model you want. Here we are interacting\nwith the Gemini 2.5 Flash. You send your prompts. And then you have\none specific bit called ThinkingConfig,\nwhere, in this code, you are asking the API\nto include the thought summaries on your\nresponse, and that's to include thoughts\nequals true, and also the amount of tokens you want\nto use during the reasoning process. On your response,\nyou're going to have part of the response will\nbe the thought summaries. So it's separated from the\nfinal answer by intention. If you want to keep\nthose on your backend,",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -2.8782622814178467
      },
      {
        "id": "bcce6feb-c12e-5253-92d4-926629505824",
        "doc_id": "gHHjDRDNUNU__chunk_022",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2746.57,
        "timestamp_end": 2767.29,
        "text": "more of the other features and\npossibilities with the models. And the last one is the Gemini\nCookbook, a GitHub repository that our team curates and keep\nupdating with a lot of sample experience for you. Thank you so much, Shrestha. Thank you so much you all. SHRESTHA BASU MALLICK: Thank you\nall for coming out to hear us. [CHEERING] [MUSIC PLAYING]",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -3.4163975715637207
      }
    ],
    "relevant_doc_ids": [
      "gHHjDRDNUNU__chunk_002"
    ]
  },
  {
    "query": "where can I test AI models with LMArena and see their Elo ratings",
    "reranked_docs": [
      {
        "id": "c8159692-2741-5292-9539-6b2eace2052d",
        "doc_id": "o7Bv4r08FBM__chunk_007",
        "video_id": "o7Bv4r08FBM",
        "title": "Google I/O 2025 \u2013 o7Bv4r08FBM",
        "timestamp_start": 888.38,
        "timestamp_end": 1038.17,
        "text": "the most basic system prompt,\nit will talk like a pirate during the conversation. So we don't have\na role for system prompts, but you can add to the\nfirst message, and it will work. You can add at any moment\nduring a conversation, and it will, from there,\ncontinue with your instructions. And people love also the\npersonality of Gemma. Personality is how\nthe model replies. When you ask a question,\nthe way it answers. People like its warm-- it has a strong personality. It's approachable. And that's why we have\na LMArena score so high. And as you can see here-- I guess those\nnumbers are updated. Those are the LM scores for our\nthree larger models, the 27 12, and 4B. And what's so great about this? First of all, this\nis from LMArena. This is a benchmark that people\ngo and blind vote on prompts. So you have a prompt. It gives you two answers, and\nyou say which one you prefer. So this is from the voice of\nthe people, let's say like that. There are, of course, criticism. But this is one of the\nbest benchmarks we have. It's people's choice. On this benchmark, you can see\nthe 27B is one of the top ones. And it's an open--\nremember, open model. Anyone can download\nand use right now. We are one of the best\nopen models, period. And the other thing is our\n27B needs 1 GPU to run, while the other open models that\nare large-- that are better, let's say like that, they\nneed, like, 30 GPUs, 10 GPUs. Do you have 10 GPUs to run now? Do you have access? So that's one of the\nimportant things. You can see the small\ndots over there. And you can see the\n12B, the middle kid. The 12B is really close. It's super strong. And you can see\nthe 4B over there. I'm not going to\nsay who is after. That doesn't matter. But the idea is that our 4B is\nreally, really strong for 4B, 4 billion. Can run basically\nmost of your devices. I'm not saying phones. And here, we see\nour capabilities in terms of specific languages. So if you look for\nGemma over there, it's right there at the bottom. You can see we were top\nin French, top in Spanish,",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -1.7761505842208862
      },
      {
        "id": "70fa7882-e039-5867-bcf4-c3155f758d7a",
        "doc_id": "4TE-KFXvhAk__chunk_017",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 1920.12,
        "timestamp_end": 2074.09,
        "text": "and it runs it on a fleet of\nreal devices of different sizes just to verify that\nit works really well. So if you're interested\nin mobile development, check out Google AI Edge. Google AI Edge,\nit's really cool. And with that, I'll\nhand it over to Joana to talk about what's next. JOANA CARRASQUEIRA: Awesome. Thank you, Josh. And you've heard it in the\nkeynotes in the previous session with Demis and Sergey,\nwe're pushing the boundaries of what's possible to build with\nAI here at Google and Google DeepMind. And we're really excited to\nbring all this innovation and put it in the\nhands of developers, in the hands of the community. And it's never been a better\ntime to build and co-create together. So we really believe\nin a future where AI is changing various fields\nacross scientific discovery, health care, and so many more. And we're going to achieve\nthis radical abundance in a safe and\nresponsible way, and we want to get there with\nyou, with the community. So let's have a look\nat some of the domains that we believe that have a\nhuge potential for developers and humanity at scale. AlphaEvolve, a\nGemini-powered coding agent for designing\nadvanced algorithms. A self-improving coding agent. And we all know that\nlarge language models can summarize documents. They can generate code. You can even\nbrainstorm with them. But with AlphaEvolve,\nwe're really expanding these\ncapabilities, and we are targeting fundamental\nand highly complex problems on\nmathematics and coding. AlphaEvolve leverages\nGemini Flash and Pro, and it's one of the big\npromises for the future. Another one-- and I'm really\nexcited about AI co-scientist. It's another\nscientific breakthrough that we're seeing, especially\nin the medical, in medicine, and research fields. And our goal is to accelerate\nthe speed up discovery and drug development. And with AI\nco-scientist, a scientist can give a research goal to\nthe agent in natural language, and then the AI\nco-scientist is designed to give you an overview, and a\nhypothesis, and a methodology.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -7.46599006652832
      },
      {
        "id": "2eaf072c-ca60-5cbb-8434-5fb172bea8aa",
        "doc_id": "gHHjDRDNUNU__chunk_003",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 364.16,
        "timestamp_end": 515.85,
        "text": "SHRESTHA BASU\nMALLICK: Including, how did I forget that? We are number one guys,\nand we're very proud of it. [CHEERING] Thank you. One of the things,\none of the use cases that has been really emerging\nin the last couple of months is, of course, app creation\nand live coding and going from 0 to 1. The best leaderboard right now\nfor that is the WebDev Arena. And we again, are number one\nwith 2.5 Pro on WebDev Arena. So if you are trying to build\n0 to 1 or few shot apps, Gemini 2.5 Pro is a\ngreat model for that. But in addition to\nuser preferences, we also like to measure\nhow well our models are doing based on rigorous\nacademic benchmarks. And so I'm not going to\ngo into a lot of details on these slides. These are available\nin blog posts. Reach out to me\nand Luciano if you want to know how exactly\nthese models are doing. But the TL;DR here is whether\nit's in highly complex domain-specific questions,\nwhether it's in coding, whether it's in\nmultimodal understanding, the Gemini 2.5 Pro is leading\non a lot of these academic benchmarks as well. Finally, performance is\none side of the coin. The other side of the\ncoin is, of course, price. This is a chart from Swyx that's\nbeen very popular on Twitter for the last few years-- on X, I'm sorry for\nthe last few months. But this really shows that\neven for price performance, the Gemini 2.5 models are\nreally at the frontier. Luciano? LUCIANO MARTINS:\nYeah, and I think it is worth\nmentioning, Shrestha, especially for those\nof you who watched Sundar's keynote yesterday and\nalso the developer's keynotes. Inside the DeepMind, we\nare doing this huge effort of trying to cover the\ndifferent aspects of experiences you may have. So we will talk a lot\nabout Gemini here. But also, we have\nthe model family that we are calling\nGenMedia, where you are able to generate high\nquality images, high quality videos. You also have-- and there is a\ncolleague from our team, Paul Ruiz, who is delivering\na talk this afternoon related to the Gemini Robotics,\nthe Gemini family of models",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -9.831453323364258
      },
      {
        "id": "6f4106b2-3ab0-5acf-bc57-f654b5e6ca8a",
        "doc_id": "4TE-KFXvhAk__chunk_001",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 127.49,
        "timestamp_end": 248.54,
        "text": "JOANA CARRASQUEIRA: We'll\nalso touch upon some developer tools for all\ntypes of experience from beginners to advanced. JOSH GORDON: Then we'll talk a\nlittle bit about infrastructure. And this talk is about\nsoftware not hardware. Our hardware infrastructure\nis TPUs, which you've probably heard a lot of. But in this talk,\nI'll briefly talk about XLA, which is a\nmachine learning compiler. And I'll talk about\nsome of the work we're doing for\ninference-- so making it possible to serve\nmodels at scale super efficiently with really\ncool new things with XLA for JAX and PyTorch. JOANA CARRASQUEIRA: OK. JOSH GORDON: Oh, and then\none more thing to mention. I went too fast. So sorry about that. So a lot of this talk is about\nthese huge foundation models. Towards the end of the talk,\nI'll talk about Google AI Edge. And we'll talk about\ndeploying small models on device, which is also super\nimportant for many reasons. JOANA CARRASQUEIRA: Awesome. OK. Let's start by exploring\nour core intelligence within our stack. We'll start with\nour Gemini models, which are our most capable\nand versatile model family. And our core philosophy here at\nGoogle is to provide developers with state-of-the-art models and\ntools that you can use to build powerful applications\nall throughout. And our Gemini models, they\nare known for being multimodal, have a long context window, and\nhaving very powerful reasoning, but we've built a variety of\nmodels for different use cases. So depending on what\nyou're trying to build, Google will have a model that\nis tailored for your use case. And I would like to just\ngive you a quick walkthrough of these models. I know you've heard\nit during the keynote, but, just very quickly,\nGemini 2.5 Pro, which is our most advanced\nmodel yet, especially for high complex tasks that\nbenefit from deep reasoning, it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -10.108715057373047
      },
      {
        "id": "f3c01d44-ad16-5c73-8bab-7224e626b701",
        "doc_id": "4TE-KFXvhAk__chunk_019",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 2201.15,
        "timestamp_end": 2283.1,
        "text": "So in order to learn\nmore, Josh, what do our developers have to do? JOSH GORDON: We have\njust a few links for you. So no problem. But we talked about a lot of\ndifferent tools in the stack. JOANA CARRASQUEIRA: We did. JOSH GORDON: So I don't\nwant to read the slide, but let me just point you\nto a couple highlights. ai.google.dev is the\nbest place to go to get started with Gemini and Gemma. We have a cookbook for Gemini. We have a cookbook for Gemma. Google AI Studio is\naistudio.google.com. If you're interested in JAX\nand Keras, there are the links. If you happen to be\ninterested in XLA, please check it\nout-- openxla.org. Google AI Edge is\nat the very bottom if you're a mobile\ndeveloper and you're interested in mobile deployment. And just to be clear, there's\nso many amazing things in the Google AI stack we didn't\nhave time to talk about today. Vertex has really amazing tools\nfor enterprise developers. But please, start\nhere, have fun, and, yeah, we're\naround after the talk. JOANA CARRASQUEIRA:\nYes, absolutely. And the Developer Relations\nteam is just outside. We have some really\ncool demo stations that you can experience. Engage with the team. Check out the sessions\ntomorrow, especially on the Gemini API,\nGemmaverse, and robotics. We have a lot of\ncool stuff that we want to put in the\nhands of developers. Many early access\nprograms as well. Stay in touch, stay\nengaged, and let's co-create the future of AI together. Thank you so much. JOSH GORDON: Thanks a lot. Thanks [APPLAUSE] [MUSIC PLAYING]",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -10.234939575195312
      }
    ],
    "relevant_doc_ids": [
      "gHHjDRDNUNU__chunk_002"
    ]
  },
  {
    "query": "how do Gemini models perform on coding and multimodal tasks",
    "reranked_docs": [
      {
        "id": "2b78c20e-37f7-560e-866d-949eb6ae6e03",
        "doc_id": "gHHjDRDNUNU__chunk_009",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 1130.2,
        "timestamp_end": 1280.26,
        "text": "But Luciano and I thought we\nwould hit some of the highlights that we really want people\nto know more about and use. So one of the areas of feedback\nthat we used to get a lot is you can, of course, upload\nfiles to the Gemini API. You can also, if you're\nless than 20 megabytes, you can also pass some of\nthese media information inline. But now, you can also-- and we've had this feature\nout for a few weeks. You can also pass\na YouTube link. And the Gemini API can\nanalyze that information. When you pass media files\nlike videos, depending on how much you want to fit\ninto the context window, you have now, a choice between\nthree resolution settings. At the lowest setting,\nyou can process up to six hours of video. But then you also have more\nhigh resolution settings that you can use. We support dynamic frame rate\nper second in the Gemini API. You can read that more about\nthat in our documentation, or come talk to me and Luciano. We support video clipping. That's a new feature. And we support\nimage segmentation. One other point I want to make\non multimodal understanding is even in the days\nof 1.5 and 2.0, Gemini models were\nsome of the best models out there for multimodal\nunderstanding. The example I like to give\nhere is I was in Costa Rica, and my guide showed\nme-- this is night. And my guide showed me\noh, there's a glass frog somewhere there on a branch. I took a photo, but I did not\nsee the glass frog in real life or in the photo. But I passed it to Gemini. And Gemini not\nonly saw the frog, it identified the\nspecies correctly. So that's how good multimodal\nunderstanding is on the Gemini. Please try it out. We also support long context. And then we, along with long-- long context is we have some\nof the largest context windows out there. So like the equivalent\nof depending on whether you're using a model\nwith 1 million context or 2 million context, you can read\nthe equivalent of eight novels, have the model read it\nand/or entire code bases. But what can sometimes\nhappen with long context",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 1.7633079290390015
      },
      {
        "id": "07bff983-b0de-5d12-9cb5-8b3d3c7df76c",
        "doc_id": "gHHjDRDNUNU__chunk_006",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 751.58,
        "timestamp_end": 904.85,
        "text": "mode called Deep Think. And the idea behind Deep\nThink is that the model 2.5 Pro can think through various\npossible answers to a problem before giving you\nthe best answer. So that is available\nin trusted testers, but will be rolled\nout more widely soon. And then we also have\nGemini Diffusion, which is the first time\nwe are testing a diffusion architecture as opposed to an\nautoregressive architecture. And these models are faster\nthan the fastest model out today and for almost\nsimilar, the performance. So try out both when you have\naccess to both Gemini Deep Diffusion and Deep Think. LUCIANO MARTINS: Perfect. SHRESTHA BASU MALLICK: All\nright, so the Gemini API. LUCIANO MARTINS: So yeah, if\nyou start thinking, especially after the keynote\nspeakers today, there are many surfaces where\nyou can interact with the Gemini models. So you have the Gemini app. You have Gemini available\non Google Cloud as the Code Assist, the assistant-- the\nCloud Assist inside the Console. You have Gemini on Workspace. So when you are talking\nabout developer experiences, we have the one\nspecific Gemini API with similar experiences as you\nhave on the no-code UI that we call AI Studio, where you\ncan start experimenting and developing your solutions\nusing the Gemini models, right? SHRESTHA BASU\nMALLICK: That's right. LUCIANO MARTINS: So\nbasically, the Gemini API is a very low barrier\nentry for having Gemini programmatic experiences. You have access to\nall public models there from the Gen Media ones,\nGemma, all the Gemini models, variants. You have a very generous\nfree of charge tier, where you can start\nexperimenting just after the session\nwithout concerns about credit cards, costs,\nbilling, anything of that. We keep developing\nmore SDKs for this API. So for now, you have available\none SDK for Python, JavaScript, Go, and Java. We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 0.2949119210243225
      },
      {
        "id": "0d53f966-23ba-5e29-8cd7-78087ea10f94",
        "doc_id": "gHHjDRDNUNU__chunk_004",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 501.0,
        "timestamp_end": 641.8,
        "text": "you are able to generate high\nquality images, high quality videos. You also have-- and there is a\ncolleague from our team, Paul Ruiz, who is delivering\na talk this afternoon related to the Gemini Robotics,\nthe Gemini family of models related to people\napplying robotics, developing robotic\nsolution with applied AI. And also, just\nafter the session, we have one another one\nwith Omar and Gus Martins to talk about Gemma 3,\nwhich is the open models family for from\nGoogle DeepMind, also developed with all\nthe technologies we use to develop Gemini. SHRESTHA BASU MALLICK:\nAnd some really good benchmark performance-- LUCIANO MARTINS: Absolutely. SHRESTHA BASU\nMALLICK: --in Gemma 3 as well, as Omar and\nGus will tell you. LUCIANO MARTINS:\nSo still talking about the different\nexperiences you may have, we heard a lot of feedbacks\nfrom you folks around the world. And one of the most asked\nfeatures we always heard were related to letting Gemini\nto create high quality audio. So yesterday we launched what\nwe are calling the Gemini TTS model, where you can from text\ngenerate high quality audio. But the coolest\nthing about the TTS is that you are not\njust creating audio. You have abilities to customize\nemotions on the audio. You may have multiple voices. You can create the audios\nwith different languages, like Brazilian Portuguese. And you can even create\nmulti-speaker interactions. So if, for example, you want\nto deliver an experience where your users will be thoughts\nabout something on a podcast, like formats, like the one\nyou have on the NotebookLM that people love. Who here is using\nNotebookLM for anything? Amazing. So you can have something\nlike the AI Overviews that are developed and delivered\nby the Gemini TTS model using the TTS, right Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: And I think it\nis not the only thing related to audio. We brought that to the live\ninteractions as well, right? SHRESTHA BASU\nMALLICK: That's right. So the TTS models,\nlike Luciano said, are available via chat endpoint.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -1.0250968933105469
      },
      {
        "id": "b3910d5f-a24c-56c2-8f8e-90d35889ff56",
        "doc_id": "gHHjDRDNUNU__chunk_017",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2130.98,
        "timestamp_end": 2271.49,
        "text": "If you're building\na research agent, maybe you want to\nuse the Chat API. If you're building\na gaming agent, maybe you want to use the real\ntime API or a customer support agent. Of course, you can build\nresearch agents also on the real time API. But in general, you now\nhave these two APIs. And then we've talked\nabout tools a lot. So all I'll say is\ngive us feedback in terms of what are some\nof the other tools that you would see Google make available\nthrough the Gemini API. LUCIANO MARTINS: Yeah. So as you just said,\nthe Gemini 2.5 Pro-- 2.5 models are thinking\ncapable, so they can do more advanced and\nmore complex reasoning. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS:\nAnd maybe we could highlight that now,\nspecifically for the 2.5 Flash, we have the ability of\nusing what we just mentioned, the thinking budgets, where\nyou can calibrate how deep and how much the model\nwill do reasoning cycles. And also for both models,\nwe can have access to what we are calling\nthought summaries. SHRESTHA BASU MALLICK: Yes. LUCIANO MARTINS: So\nbasically you are not only counting on the model\nability to reason, but also you can see on your\nresponse which key steps or which key reasoning\nthoughts the model had to get to our conclusion. So basically, that's a\npretty basic Gemini API code using the Python SDK. For those of you who\nnever used the SDK before, it's pretty straightforward. So three key blocks. You need to import the SDK,\nyou instantiate the client, and then you interact\nwith the model you want. Here we are interacting\nwith the Gemini 2.5 Flash. You send your prompts. And then you have\none specific bit called ThinkingConfig,\nwhere, in this code, you are asking the API\nto include the thought summaries on your\nresponse, and that's to include thoughts\nequals true, and also the amount of tokens you want\nto use during the reasoning process. On your response,\nyou're going to have part of the response will\nbe the thought summaries. So it's separated from the\nfinal answer by intention. If you want to keep\nthose on your backend,",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -1.3104503154754639
      },
      {
        "id": "bcce6feb-c12e-5253-92d4-926629505824",
        "doc_id": "gHHjDRDNUNU__chunk_022",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2746.57,
        "timestamp_end": 2767.29,
        "text": "more of the other features and\npossibilities with the models. And the last one is the Gemini\nCookbook, a GitHub repository that our team curates and keep\nupdating with a lot of sample experience for you. Thank you so much, Shrestha. Thank you so much you all. SHRESTHA BASU MALLICK: Thank you\nall for coming out to hear us. [CHEERING] [MUSIC PLAYING]",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -5.289290904998779
      }
    ],
    "relevant_doc_ids": [
      "gHHjDRDNUNU__chunk_003"
    ]
  },
  {
    "query": "what is Gemini 2.5 Pro and what can it do for app building",
    "reranked_docs": [
      {
        "id": "b3910d5f-a24c-56c2-8f8e-90d35889ff56",
        "doc_id": "gHHjDRDNUNU__chunk_017",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2130.98,
        "timestamp_end": 2271.49,
        "text": "If you're building\na research agent, maybe you want to\nuse the Chat API. If you're building\na gaming agent, maybe you want to use the real\ntime API or a customer support agent. Of course, you can build\nresearch agents also on the real time API. But in general, you now\nhave these two APIs. And then we've talked\nabout tools a lot. So all I'll say is\ngive us feedback in terms of what are some\nof the other tools that you would see Google make available\nthrough the Gemini API. LUCIANO MARTINS: Yeah. So as you just said,\nthe Gemini 2.5 Pro-- 2.5 models are thinking\ncapable, so they can do more advanced and\nmore complex reasoning. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS:\nAnd maybe we could highlight that now,\nspecifically for the 2.5 Flash, we have the ability of\nusing what we just mentioned, the thinking budgets, where\nyou can calibrate how deep and how much the model\nwill do reasoning cycles. And also for both models,\nwe can have access to what we are calling\nthought summaries. SHRESTHA BASU MALLICK: Yes. LUCIANO MARTINS: So\nbasically you are not only counting on the model\nability to reason, but also you can see on your\nresponse which key steps or which key reasoning\nthoughts the model had to get to our conclusion. So basically, that's a\npretty basic Gemini API code using the Python SDK. For those of you who\nnever used the SDK before, it's pretty straightforward. So three key blocks. You need to import the SDK,\nyou instantiate the client, and then you interact\nwith the model you want. Here we are interacting\nwith the Gemini 2.5 Flash. You send your prompts. And then you have\none specific bit called ThinkingConfig,\nwhere, in this code, you are asking the API\nto include the thought summaries on your\nresponse, and that's to include thoughts\nequals true, and also the amount of tokens you want\nto use during the reasoning process. On your response,\nyou're going to have part of the response will\nbe the thought summaries. So it's separated from the\nfinal answer by intention. If you want to keep\nthose on your backend,",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 4.478099822998047
      },
      {
        "id": "5818d7d2-3012-5c4b-a76a-e93a3f646453",
        "doc_id": "4TE-KFXvhAk__chunk_002",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 230.74,
        "timestamp_end": 380.44,
        "text": "it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it because of its\nefficiency and speed, and it's now even better at\nalmost every single dimension. So we improved\nall the benchmarks across reasoning, coding,\nmultimodality, and also long context. Then we have our Gemini 2.0\nFlash, which is fast and cheap, works fine, and our\nGemini Nano, which is optimized for on-device tasks. And as you've heard, we've\nbeen shipping relentlessly. And I would like to give\nyou just a quick highlight of everything that we've\nbeen shipping in AI Studio and the Gemini API. There's a talk\ntomorrow that I would like to invite you to attend,\nwhich is by Shrestha Basu Mallick, a group product\nmanager on Gemini API, and Luciano Martins, our\ntechnical lead for Gemini API from DevRel. And they're going to do a deep\ndive into everything that's new within the Gemini\nAPI, so you definitely don't want to miss\nthat session tomorrow. But for now, just a glimpse to\nget you excited about what's new in AI Studio. We've built a new tab\nthat is called Build that instantly generates web apps. And it's really cool, because it\nenables developers and builders alike to prototype very\nquickly with natural language. We have a new generative media\nexperience in AI Studio as well, and I'm going to\ndemo all of this so you can see how\nit actually works. And we are always\nlistening to the community. We listen to your\nfeedback, and we always build with developers in mind. And that's why that\nsome of these features were actually\nrequested by community, and that's what happened with\nthe built-in new dashboard. You requested. We built it. And we also have some\nnew Native Audio and TTS support in AI Studio. On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 4.109065055847168
      },
      {
        "id": "2eaf072c-ca60-5cbb-8434-5fb172bea8aa",
        "doc_id": "gHHjDRDNUNU__chunk_003",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 364.16,
        "timestamp_end": 515.85,
        "text": "SHRESTHA BASU\nMALLICK: Including, how did I forget that? We are number one guys,\nand we're very proud of it. [CHEERING] Thank you. One of the things,\none of the use cases that has been really emerging\nin the last couple of months is, of course, app creation\nand live coding and going from 0 to 1. The best leaderboard right now\nfor that is the WebDev Arena. And we again, are number one\nwith 2.5 Pro on WebDev Arena. So if you are trying to build\n0 to 1 or few shot apps, Gemini 2.5 Pro is a\ngreat model for that. But in addition to\nuser preferences, we also like to measure\nhow well our models are doing based on rigorous\nacademic benchmarks. And so I'm not going to\ngo into a lot of details on these slides. These are available\nin blog posts. Reach out to me\nand Luciano if you want to know how exactly\nthese models are doing. But the TL;DR here is whether\nit's in highly complex domain-specific questions,\nwhether it's in coding, whether it's in\nmultimodal understanding, the Gemini 2.5 Pro is leading\non a lot of these academic benchmarks as well. Finally, performance is\none side of the coin. The other side of the\ncoin is, of course, price. This is a chart from Swyx that's\nbeen very popular on Twitter for the last few years-- on X, I'm sorry for\nthe last few months. But this really shows that\neven for price performance, the Gemini 2.5 models are\nreally at the frontier. Luciano? LUCIANO MARTINS:\nYeah, and I think it is worth\nmentioning, Shrestha, especially for those\nof you who watched Sundar's keynote yesterday and\nalso the developer's keynotes. Inside the DeepMind, we\nare doing this huge effort of trying to cover the\ndifferent aspects of experiences you may have. So we will talk a lot\nabout Gemini here. But also, we have\nthe model family that we are calling\nGenMedia, where you are able to generate high\nquality images, high quality videos. You also have-- and there is a\ncolleague from our team, Paul Ruiz, who is delivering\na talk this afternoon related to the Gemini Robotics,\nthe Gemini family of models",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 3.517812967300415
      },
      {
        "id": "07bff983-b0de-5d12-9cb5-8b3d3c7df76c",
        "doc_id": "gHHjDRDNUNU__chunk_006",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 751.58,
        "timestamp_end": 904.85,
        "text": "mode called Deep Think. And the idea behind Deep\nThink is that the model 2.5 Pro can think through various\npossible answers to a problem before giving you\nthe best answer. So that is available\nin trusted testers, but will be rolled\nout more widely soon. And then we also have\nGemini Diffusion, which is the first time\nwe are testing a diffusion architecture as opposed to an\nautoregressive architecture. And these models are faster\nthan the fastest model out today and for almost\nsimilar, the performance. So try out both when you have\naccess to both Gemini Deep Diffusion and Deep Think. LUCIANO MARTINS: Perfect. SHRESTHA BASU MALLICK: All\nright, so the Gemini API. LUCIANO MARTINS: So yeah, if\nyou start thinking, especially after the keynote\nspeakers today, there are many surfaces where\nyou can interact with the Gemini models. So you have the Gemini app. You have Gemini available\non Google Cloud as the Code Assist, the assistant-- the\nCloud Assist inside the Console. You have Gemini on Workspace. So when you are talking\nabout developer experiences, we have the one\nspecific Gemini API with similar experiences as you\nhave on the no-code UI that we call AI Studio, where you\ncan start experimenting and developing your solutions\nusing the Gemini models, right? SHRESTHA BASU\nMALLICK: That's right. LUCIANO MARTINS: So\nbasically, the Gemini API is a very low barrier\nentry for having Gemini programmatic experiences. You have access to\nall public models there from the Gen Media ones,\nGemma, all the Gemini models, variants. You have a very generous\nfree of charge tier, where you can start\nexperimenting just after the session\nwithout concerns about credit cards, costs,\nbilling, anything of that. We keep developing\nmore SDKs for this API. So for now, you have available\none SDK for Python, JavaScript, Go, and Java. We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 3.357247829437256
      },
      {
        "id": "b0ee6a81-6918-5bbf-8b1e-baa4084f5b62",
        "doc_id": "gHHjDRDNUNU__chunk_002",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 254.53,
        "timestamp_end": 382.64,
        "text": "when we think about the key\nstandard benchmarks, Shrestha? SHRESTHA BASU\nMALLICK: Yeah, we'll go to the benchmarks\nin a second. But the message\nthat I want everyone to take away from this\nslide is whether it's a super complex task, or whether\nit's on-device processing, you now have a Gemini model\nthat you can use for it. And it's pretty powerful. And it's pretty price effective. LUCIANO MARTINS: Yeah,\nmaybe it is also worth mentioning, Shrestha,\nthat some folks here may be developing with\nGemini since it was released. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS: So using the\nAI Studio, using the Gemini API, the older versions\nof Gemini like 2.0, 1.5 are still available. But we really\nencourage everybody to start experimenting\nwith the newer ones with better capabilities\nand better performance. SHRESTHA BASU MALLICK:\nThat's correct. So 2.0 Flash continues to be one\nof our most widely-used models, just again, because of how good\nit is for the price it's at. And we have a lot of people\nwho are still using it. And then we also have some\npeople on the 1.5 models. But we're encouraging\npeople now, we're two generations\nahead, to start using the 2.5 series of\nmodels and give us feedback. LUCIANO MARTINS: Nice. So please tell us a little bit\nabout the benchmarks, Shrestha. SHRESTHA BASU MALLICK:\nYeah, next slide. Yeah, so we're not going\nto spend a lot of time on the benchmarks. But the two key points\nthat I'd like to hit here is number one, is one\nway in which we measure how good these models are is\nby putting them on LMArena, which is where developers\nand builders like yourself test out\nall these models and give it an ELO rating. And we're, of course, very\nproud that right now you have three Gemini models in\nthe top 10 in the LMArena. LUCIANO MARTINS:\nIncluding the top one. SHRESTHA BASU\nMALLICK: Including, how did I forget that? We are number one guys,\nand we're very proud of it. [CHEERING] Thank you. One of the things,\none of the use cases that has been really emerging\nin the last couple of months",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -3.2213993072509766
      }
    ],
    "relevant_doc_ids": [
      "gHHjDRDNUNU__chunk_003"
    ]
  },
  {
    "query": "where can I learn about Gemini GenMedia and robotic models",
    "reranked_docs": [
      {
        "id": "0d53f966-23ba-5e29-8cd7-78087ea10f94",
        "doc_id": "gHHjDRDNUNU__chunk_004",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 501.0,
        "timestamp_end": 641.8,
        "text": "you are able to generate high\nquality images, high quality videos. You also have-- and there is a\ncolleague from our team, Paul Ruiz, who is delivering\na talk this afternoon related to the Gemini Robotics,\nthe Gemini family of models related to people\napplying robotics, developing robotic\nsolution with applied AI. And also, just\nafter the session, we have one another one\nwith Omar and Gus Martins to talk about Gemma 3,\nwhich is the open models family for from\nGoogle DeepMind, also developed with all\nthe technologies we use to develop Gemini. SHRESTHA BASU MALLICK:\nAnd some really good benchmark performance-- LUCIANO MARTINS: Absolutely. SHRESTHA BASU\nMALLICK: --in Gemma 3 as well, as Omar and\nGus will tell you. LUCIANO MARTINS:\nSo still talking about the different\nexperiences you may have, we heard a lot of feedbacks\nfrom you folks around the world. And one of the most asked\nfeatures we always heard were related to letting Gemini\nto create high quality audio. So yesterday we launched what\nwe are calling the Gemini TTS model, where you can from text\ngenerate high quality audio. But the coolest\nthing about the TTS is that you are not\njust creating audio. You have abilities to customize\nemotions on the audio. You may have multiple voices. You can create the audios\nwith different languages, like Brazilian Portuguese. And you can even create\nmulti-speaker interactions. So if, for example, you want\nto deliver an experience where your users will be thoughts\nabout something on a podcast, like formats, like the one\nyou have on the NotebookLM that people love. Who here is using\nNotebookLM for anything? Amazing. So you can have something\nlike the AI Overviews that are developed and delivered\nby the Gemini TTS model using the TTS, right Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: And I think it\nis not the only thing related to audio. We brought that to the live\ninteractions as well, right? SHRESTHA BASU\nMALLICK: That's right. So the TTS models,\nlike Luciano said, are available via chat endpoint.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 3.3072316646575928
      },
      {
        "id": "51a07f69-109c-5809-b618-2760e83de9e5",
        "doc_id": "gHHjDRDNUNU__chunk_007",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 890.57,
        "timestamp_end": 1021.23,
        "text": "We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there. If you are a Google\nColab user, you have ways to interact with\nthe Gemini models as well. So the key idea is to\nmake it easier for you. No matter where you are having\nyour development experience, We are trying to bring\nGemini API closer to you. SHRESTHA BASU MALLICK:\nThat's correct. And I think this is\nalso a good place to make a plug for Google\nAI Studio, which of course, everyone knows about and loves. It is the place where\na lot of our developers first test out the\ncapabilities of the API before committing to\nbuilding applications at scale with the API. And Google AI Studio now\nhas code generation also. LUCIANO MARTINS: Absolutely. SHRESTHA BASU MALLICK: So this\nis a quick, high level overview of all the components that are\navailable through the Gemini API. It's pretty standard. You send in your prompt, you get\na response back from the model. If there's a tool\ncall needed, we do support a set of\nfirst party Google tools, as well as function calling. So in terms of the first party\nGoogle tools that we support, we, of course, support\nGoogle Search as a tool. The best search engine\nout there is now available as a tool for the\nmodel to call when it needs to. Now in addition to the\ninformation that you get out of Search, if you want to\nretrieve more in-depth content from web pages for\napplications such as you're building your own version\nof a research agent, so you pull some\ninformation from Search. You pull a set of\nURLs, but if you want to extract a little\nmore in depth content, we just released a new tool\nat I/O called URL Context. And then, of course, you can\ntake all that information. And if you want to create\nbeautiful charts out of it or run some analysis,\nwe make code execution available as a tool to\nyou via the API as well. Then, of course, you\nhave function calling.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -1.920060634613037
      },
      {
        "id": "1bf79af9-fc9e-5248-b56a-1c7adf068aff",
        "doc_id": "4TE-KFXvhAk__chunk_007",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 880.26,
        "timestamp_end": 979.77,
        "text": "So it's just really\nquick and easy to do. OK, we can go back\nto the slides. So very, very\nquickly, as a recap, Gemini Developer API is the\neasiest way to get started. It's super lightweight,\nit's fast to install, and you can get up and running\nhonestly in about a minute. OK? I will use the clicker. This is the flow to get\nstarted with Google AI Studio. Go to Google AI Studio. Get your key. Try one of the code\nexamples on ai.google.dev or in the cookbook. I see people taking pictures. That makes me happy. Please try this. We spent so much time\non making this easy, and I hope it works for you. If not, please file an\nissue and we'll get on it. This is the Gen AI SDK\nfor the Gemini API, and this is something\nwe've been rolling out. It's our latest SDK. We've been rolling\nit out gradually over the course of\nthe last six months. It's super user friendly. It's really easy to use. Really, the only point\nI want to make here, because I don't want\nto read the code examples or the\ndocumentation to you, you can call the API\nin a few lines of code. Basically add your key, select\na model, write a prompt, you can go ahead and call it. You can also get access\nto advanced functionality in one line of code. So if you'd like to get the\nthinking summaries that Joana showed you, you can just\nadd a ThinkingConfig, say include the thoughts. Now, you've got the\nthinking summaries. And a good use\ncase for this could be any time you need to\nexplain the model's reasoning. Particularly, you can\nimagine, if you're building an education\napp or a tutoring app, you can get the\nthinking summaries. In addition to\nreally cool things that you can do with\na single line of code, there's some more advanced stuff\nthat you can do with the SDK as well. So I know there's a lot\nof code on this slide, but we've talked a lot\nabout building agents and agentic experiences. In this example,\nyou could imagine that you have a Python\nfunction on your laptop called weather_function. And maybe that calls\nyour own weather server to get the weather.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -3.4760100841522217
      },
      {
        "id": "b3910d5f-a24c-56c2-8f8e-90d35889ff56",
        "doc_id": "gHHjDRDNUNU__chunk_017",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2130.98,
        "timestamp_end": 2271.49,
        "text": "If you're building\na research agent, maybe you want to\nuse the Chat API. If you're building\na gaming agent, maybe you want to use the real\ntime API or a customer support agent. Of course, you can build\nresearch agents also on the real time API. But in general, you now\nhave these two APIs. And then we've talked\nabout tools a lot. So all I'll say is\ngive us feedback in terms of what are some\nof the other tools that you would see Google make available\nthrough the Gemini API. LUCIANO MARTINS: Yeah. So as you just said,\nthe Gemini 2.5 Pro-- 2.5 models are thinking\ncapable, so they can do more advanced and\nmore complex reasoning. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS:\nAnd maybe we could highlight that now,\nspecifically for the 2.5 Flash, we have the ability of\nusing what we just mentioned, the thinking budgets, where\nyou can calibrate how deep and how much the model\nwill do reasoning cycles. And also for both models,\nwe can have access to what we are calling\nthought summaries. SHRESTHA BASU MALLICK: Yes. LUCIANO MARTINS: So\nbasically you are not only counting on the model\nability to reason, but also you can see on your\nresponse which key steps or which key reasoning\nthoughts the model had to get to our conclusion. So basically, that's a\npretty basic Gemini API code using the Python SDK. For those of you who\nnever used the SDK before, it's pretty straightforward. So three key blocks. You need to import the SDK,\nyou instantiate the client, and then you interact\nwith the model you want. Here we are interacting\nwith the Gemini 2.5 Flash. You send your prompts. And then you have\none specific bit called ThinkingConfig,\nwhere, in this code, you are asking the API\nto include the thought summaries on your\nresponse, and that's to include thoughts\nequals true, and also the amount of tokens you want\nto use during the reasoning process. On your response,\nyou're going to have part of the response will\nbe the thought summaries. So it's separated from the\nfinal answer by intention. If you want to keep\nthose on your backend,",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -3.881500482559204
      },
      {
        "id": "bcce6feb-c12e-5253-92d4-926629505824",
        "doc_id": "gHHjDRDNUNU__chunk_022",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2746.57,
        "timestamp_end": 2767.29,
        "text": "more of the other features and\npossibilities with the models. And the last one is the Gemini\nCookbook, a GitHub repository that our team curates and keep\nupdating with a lot of sample experience for you. Thank you so much, Shrestha. Thank you so much you all. SHRESTHA BASU MALLICK: Thank you\nall for coming out to hear us. [CHEERING] [MUSIC PLAYING]",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -4.224860191345215
      }
    ],
    "relevant_doc_ids": [
      "gHHjDRDNUNU__chunk_003"
    ]
  },
  {
    "query": "differences between Gemini models for robotics, chat, and audio generation tools",
    "reranked_docs": [
      {
        "id": "0d53f966-23ba-5e29-8cd7-78087ea10f94",
        "doc_id": "gHHjDRDNUNU__chunk_004",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 501.0,
        "timestamp_end": 641.8,
        "text": "you are able to generate high\nquality images, high quality videos. You also have-- and there is a\ncolleague from our team, Paul Ruiz, who is delivering\na talk this afternoon related to the Gemini Robotics,\nthe Gemini family of models related to people\napplying robotics, developing robotic\nsolution with applied AI. And also, just\nafter the session, we have one another one\nwith Omar and Gus Martins to talk about Gemma 3,\nwhich is the open models family for from\nGoogle DeepMind, also developed with all\nthe technologies we use to develop Gemini. SHRESTHA BASU MALLICK:\nAnd some really good benchmark performance-- LUCIANO MARTINS: Absolutely. SHRESTHA BASU\nMALLICK: --in Gemma 3 as well, as Omar and\nGus will tell you. LUCIANO MARTINS:\nSo still talking about the different\nexperiences you may have, we heard a lot of feedbacks\nfrom you folks around the world. And one of the most asked\nfeatures we always heard were related to letting Gemini\nto create high quality audio. So yesterday we launched what\nwe are calling the Gemini TTS model, where you can from text\ngenerate high quality audio. But the coolest\nthing about the TTS is that you are not\njust creating audio. You have abilities to customize\nemotions on the audio. You may have multiple voices. You can create the audios\nwith different languages, like Brazilian Portuguese. And you can even create\nmulti-speaker interactions. So if, for example, you want\nto deliver an experience where your users will be thoughts\nabout something on a podcast, like formats, like the one\nyou have on the NotebookLM that people love. Who here is using\nNotebookLM for anything? Amazing. So you can have something\nlike the AI Overviews that are developed and delivered\nby the Gemini TTS model using the TTS, right Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: And I think it\nis not the only thing related to audio. We brought that to the live\ninteractions as well, right? SHRESTHA BASU\nMALLICK: That's right. So the TTS models,\nlike Luciano said, are available via chat endpoint.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 2.68087100982666
      },
      {
        "id": "cc910b44-8924-521e-9890-82d926ac6eb6",
        "doc_id": "4TE-KFXvhAk__chunk_003",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 363.97,
        "timestamp_end": 521.659,
        "text": "On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API and also on [INAUDIBLE] API\nfor real generating audio. And some of the use\ncases here that we had in mind when\nwe built this were more dynamic audiobooks,\nsome more engaging podcasts, or, even for those of\nyou in customer support, producing more natural\nvoices into your workflows. We also have enhanced\ntooling, which is really cool, because now you\ncan use Grounding with Google Search, also with Code Execution\ntogether in just one API call, and URL context, which you\nheard during the keynote, which provides the model with\ndepth content from web pages. And since you can chain\nit to other tools, it's actually really powerful\nto build search agents. So really cool stuff. Lastly, just to call out that\nwe now offer Gemini SDK support for MCP, which reduces a\nlot of developer friction and simplifies building\nagentic capabilities. So you don't want to\nmiss tomorrow's talk to learn more about this. And Google AI Studio. Who here uses Google AI Studio? Can I see some hands? OK, awesome. Awesome. We have a lot to chat\nafter this session then. Google AI Studio is the\nperfect place for anyone to start developing with AI. It's the simplest way to test\nthe latest DeepMind models. We typically bring them\nto Google AI Studio so you can start prototyping\nand playing with the models. And you don't need Google\nCloud knowledge in order to set up your environment. It is free of charge, and you\ncan create, and test, and save your prompts. And there's also starter\napps that will inspire you. And that's exactly what I'm\ngoing to demo here today for you. I am going to show\nyou some of the work that the team has been doing. So if we go to Google AI Studio\nhere, if we go into Build, and Mumble Jumble is\nsomething that we've literally just created, and\nit's one of my favorite apps. So Mumble Jumble is one\nof those applications that",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 0.6535670161247253
      },
      {
        "id": "b3910d5f-a24c-56c2-8f8e-90d35889ff56",
        "doc_id": "gHHjDRDNUNU__chunk_017",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2130.98,
        "timestamp_end": 2271.49,
        "text": "If you're building\na research agent, maybe you want to\nuse the Chat API. If you're building\na gaming agent, maybe you want to use the real\ntime API or a customer support agent. Of course, you can build\nresearch agents also on the real time API. But in general, you now\nhave these two APIs. And then we've talked\nabout tools a lot. So all I'll say is\ngive us feedback in terms of what are some\nof the other tools that you would see Google make available\nthrough the Gemini API. LUCIANO MARTINS: Yeah. So as you just said,\nthe Gemini 2.5 Pro-- 2.5 models are thinking\ncapable, so they can do more advanced and\nmore complex reasoning. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS:\nAnd maybe we could highlight that now,\nspecifically for the 2.5 Flash, we have the ability of\nusing what we just mentioned, the thinking budgets, where\nyou can calibrate how deep and how much the model\nwill do reasoning cycles. And also for both models,\nwe can have access to what we are calling\nthought summaries. SHRESTHA BASU MALLICK: Yes. LUCIANO MARTINS: So\nbasically you are not only counting on the model\nability to reason, but also you can see on your\nresponse which key steps or which key reasoning\nthoughts the model had to get to our conclusion. So basically, that's a\npretty basic Gemini API code using the Python SDK. For those of you who\nnever used the SDK before, it's pretty straightforward. So three key blocks. You need to import the SDK,\nyou instantiate the client, and then you interact\nwith the model you want. Here we are interacting\nwith the Gemini 2.5 Flash. You send your prompts. And then you have\none specific bit called ThinkingConfig,\nwhere, in this code, you are asking the API\nto include the thought summaries on your\nresponse, and that's to include thoughts\nequals true, and also the amount of tokens you want\nto use during the reasoning process. On your response,\nyou're going to have part of the response will\nbe the thought summaries. So it's separated from the\nfinal answer by intention. If you want to keep\nthose on your backend,",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -0.1672186255455017
      },
      {
        "id": "8c096a3a-3e99-5203-b8c9-91d5e3adc2b2",
        "doc_id": "4TE-KFXvhAk__chunk_009",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 1066.46,
        "timestamp_end": 1237.45,
        "text": "And I would also like to\ndemo one of these new apps that we have in AI Studio. So I'm going back to\nthe laptop, and I'm going to show you something\nthat the team also just created. So AI Studio also got a facelift\nand has some new features. And the chat interface\nis still the same, but you've seen the talk to\nGemini live during the keynote. We have the new\nGenerative Media Console, which allows you to\ncreate and interact with our most creative models. And then we have\nthe Build, which is where all these new\napps are coming to. So I just wanted to show\nyou very quickly this one. There we go. And then we basically\ncan choose here what are the sounds\nthat we want. And this is all powered by\nLyria, our music generation model. [GENERATED MUSIC PLAYING] And just for the\ninterest of time, I'm not going to\nkeep playing it. But you can see some of the\ncapabilities of these models that we're bringing\nto AI Studio. We'll continue to the slides. In the Console,\nas you could see, you have access to\nour image generation, our video generation, and music\ngeneration models with applets to get you started. And so that's a really\ncool thing for you to play with after this session. Some of our videos,\nvery realistic images with a really good understanding\nof real-world physics and dynamics, improved quality,\nand more and more capabilities coming to these models. And this is the example\nthat I just showed. We've made Lyria RealTime our\ninteractive music generation model, which powers MusicFX DJ. It's available in the\nAPI and AI Studio, and you can check\nour API documentation for more information. This also allows\neveryone to interact, to create, and to perform\ngenerative music in real time. It's really cool. You might remember that in the\nshow before the first keynote, you might have\nseen this console. That's exactly why I wanted to\nshow you this particular app in this session. But there's a lot more that\nyou can try afterwards. And shifting the\ngears towards Gemma, earlier this year we\nreleased Gemma 3, which",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -0.43464669585227966
      },
      {
        "id": "bcce6feb-c12e-5253-92d4-926629505824",
        "doc_id": "gHHjDRDNUNU__chunk_022",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2746.57,
        "timestamp_end": 2767.29,
        "text": "more of the other features and\npossibilities with the models. And the last one is the Gemini\nCookbook, a GitHub repository that our team curates and keep\nupdating with a lot of sample experience for you. Thank you so much, Shrestha. Thank you so much you all. SHRESTHA BASU MALLICK: Thank you\nall for coming out to hear us. [CHEERING] [MUSIC PLAYING]",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -6.645432949066162
      }
    ],
    "relevant_doc_ids": [
      "gHHjDRDNUNU__chunk_004"
    ]
  },
  {
    "query": "how can I generate natural-sounding speech from text with Gemini TTS",
    "reranked_docs": [
      {
        "id": "cc910b44-8924-521e-9890-82d926ac6eb6",
        "doc_id": "4TE-KFXvhAk__chunk_003",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 363.97,
        "timestamp_end": 521.659,
        "text": "On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API and also on [INAUDIBLE] API\nfor real generating audio. And some of the use\ncases here that we had in mind when\nwe built this were more dynamic audiobooks,\nsome more engaging podcasts, or, even for those of\nyou in customer support, producing more natural\nvoices into your workflows. We also have enhanced\ntooling, which is really cool, because now you\ncan use Grounding with Google Search, also with Code Execution\ntogether in just one API call, and URL context, which you\nheard during the keynote, which provides the model with\ndepth content from web pages. And since you can chain\nit to other tools, it's actually really powerful\nto build search agents. So really cool stuff. Lastly, just to call out that\nwe now offer Gemini SDK support for MCP, which reduces a\nlot of developer friction and simplifies building\nagentic capabilities. So you don't want to\nmiss tomorrow's talk to learn more about this. And Google AI Studio. Who here uses Google AI Studio? Can I see some hands? OK, awesome. Awesome. We have a lot to chat\nafter this session then. Google AI Studio is the\nperfect place for anyone to start developing with AI. It's the simplest way to test\nthe latest DeepMind models. We typically bring them\nto Google AI Studio so you can start prototyping\nand playing with the models. And you don't need Google\nCloud knowledge in order to set up your environment. It is free of charge, and you\ncan create, and test, and save your prompts. And there's also starter\napps that will inspire you. And that's exactly what I'm\ngoing to demo here today for you. I am going to show\nyou some of the work that the team has been doing. So if we go to Google AI Studio\nhere, if we go into Build, and Mumble Jumble is\nsomething that we've literally just created, and\nit's one of my favorite apps. So Mumble Jumble is one\nof those applications that",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 3.5988523960113525
      },
      {
        "id": "0d53f966-23ba-5e29-8cd7-78087ea10f94",
        "doc_id": "gHHjDRDNUNU__chunk_004",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 501.0,
        "timestamp_end": 641.8,
        "text": "you are able to generate high\nquality images, high quality videos. You also have-- and there is a\ncolleague from our team, Paul Ruiz, who is delivering\na talk this afternoon related to the Gemini Robotics,\nthe Gemini family of models related to people\napplying robotics, developing robotic\nsolution with applied AI. And also, just\nafter the session, we have one another one\nwith Omar and Gus Martins to talk about Gemma 3,\nwhich is the open models family for from\nGoogle DeepMind, also developed with all\nthe technologies we use to develop Gemini. SHRESTHA BASU MALLICK:\nAnd some really good benchmark performance-- LUCIANO MARTINS: Absolutely. SHRESTHA BASU\nMALLICK: --in Gemma 3 as well, as Omar and\nGus will tell you. LUCIANO MARTINS:\nSo still talking about the different\nexperiences you may have, we heard a lot of feedbacks\nfrom you folks around the world. And one of the most asked\nfeatures we always heard were related to letting Gemini\nto create high quality audio. So yesterday we launched what\nwe are calling the Gemini TTS model, where you can from text\ngenerate high quality audio. But the coolest\nthing about the TTS is that you are not\njust creating audio. You have abilities to customize\nemotions on the audio. You may have multiple voices. You can create the audios\nwith different languages, like Brazilian Portuguese. And you can even create\nmulti-speaker interactions. So if, for example, you want\nto deliver an experience where your users will be thoughts\nabout something on a podcast, like formats, like the one\nyou have on the NotebookLM that people love. Who here is using\nNotebookLM for anything? Amazing. So you can have something\nlike the AI Overviews that are developed and delivered\nby the Gemini TTS model using the TTS, right Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: And I think it\nis not the only thing related to audio. We brought that to the live\ninteractions as well, right? SHRESTHA BASU\nMALLICK: That's right. So the TTS models,\nlike Luciano said, are available via chat endpoint.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 2.1866719722747803
      },
      {
        "id": "9b8e21a0-8c7a-5b78-9a21-66c03d49e1f5",
        "doc_id": "gHHjDRDNUNU__chunk_005",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 628.55,
        "timestamp_end": 769.3299999999999,
        "text": "LUCIANO MARTINS: And I think it\nis not the only thing related to audio. We brought that to the live\ninteractions as well, right? SHRESTHA BASU\nMALLICK: That's right. So the TTS models,\nlike Luciano said, are available via chat endpoint. And then on our Live API,\nyesterday, we also rolled out the native audio output models. So this is now\nGoogle's first release of an audio-to-audio\narchitecture. And there are two variants of\nthis model that you have access through the Gemini Live API,\nwhich is our real time API. So you have the Native\nAudio Dialogue model, which is the real benefit of\nthe native audio output models are that the voices\nthat come out of it are much more natural\nand much more compelling. The native audio-to-audio\narchitecture has better contextual\nunderstanding of what humans say to the AI. It can seamlessly transition\nbetween different languages, so maybe from Brazilian\nPortuguese to Bengali, if you want to switch\nin the same language, in the same sentence. And it's available on\nour Gemini real time API. We also have a version\nof this model available, again on the real time\nAPI with thinking enabled. So for more complex use\ncases, so one use case that comes to mind is let's say\nyou're building a gaming agent to play a strategy game. And you want that agent to be\nable to think, but also to talk to you with relatively\nlow latency. We have a thinking version\nof the native audio model that's also now available\nfrom the Live API. So please try it out\nand give us feedback. LUCIANO MARTINS: Perfect. So how people can start\nusing those models, Shrestha? SHRESTHA BASU MALLICK:\nThrough the API. Of course. But before we go there,\nthere are two other things that we should mention. It's hot off the news. You've probably\nall heard about it. We've also enabled\nan advanced reasoning mode called Deep Think. And the idea behind Deep\nThink is that the model 2.5 Pro can think through various\npossible answers to a problem before giving you\nthe best answer. So that is available\nin trusted testers,",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -0.14453887939453125
      },
      {
        "id": "8c096a3a-3e99-5203-b8c9-91d5e3adc2b2",
        "doc_id": "4TE-KFXvhAk__chunk_009",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 1066.46,
        "timestamp_end": 1237.45,
        "text": "And I would also like to\ndemo one of these new apps that we have in AI Studio. So I'm going back to\nthe laptop, and I'm going to show you something\nthat the team also just created. So AI Studio also got a facelift\nand has some new features. And the chat interface\nis still the same, but you've seen the talk to\nGemini live during the keynote. We have the new\nGenerative Media Console, which allows you to\ncreate and interact with our most creative models. And then we have\nthe Build, which is where all these new\napps are coming to. So I just wanted to show\nyou very quickly this one. There we go. And then we basically\ncan choose here what are the sounds\nthat we want. And this is all powered by\nLyria, our music generation model. [GENERATED MUSIC PLAYING] And just for the\ninterest of time, I'm not going to\nkeep playing it. But you can see some of the\ncapabilities of these models that we're bringing\nto AI Studio. We'll continue to the slides. In the Console,\nas you could see, you have access to\nour image generation, our video generation, and music\ngeneration models with applets to get you started. And so that's a really\ncool thing for you to play with after this session. Some of our videos,\nvery realistic images with a really good understanding\nof real-world physics and dynamics, improved quality,\nand more and more capabilities coming to these models. And this is the example\nthat I just showed. We've made Lyria RealTime our\ninteractive music generation model, which powers MusicFX DJ. It's available in the\nAPI and AI Studio, and you can check\nour API documentation for more information. This also allows\neveryone to interact, to create, and to perform\ngenerative music in real time. It's really cool. You might remember that in the\nshow before the first keynote, you might have\nseen this console. That's exactly why I wanted to\nshow you this particular app in this session. But there's a lot more that\nyou can try afterwards. And shifting the\ngears towards Gemma, earlier this year we\nreleased Gemma 3, which",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -6.9337897300720215
      },
      {
        "id": "32c91b78-6b96-5e75-8447-67d6f676ed2c",
        "doc_id": "4TE-KFXvhAk__chunk_011",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 1363.027,
        "timestamp_end": 1474.62,
        "text": "JOSH GORDON: Thanks. OK, so we've talked a lot\nabout foundation models, Gemini and Gemma. Now, let's talk a little\nbit about the frameworks that Google and the\ncommunity use to build them. So a lot of cool stuff to cover. Let's start with the\neasiest possible way to get started to fine tune a model. So in the developer keynote,\nGus showed a version of Gemma that speaks emoji. And this is a language that\nhe came up with his daughter. One way to do that is you\ncould just prompt the model to speak emoji. And in a lot of cases, you\ncan get away with a prompt. But if you have a very\nlarge amount of data, or maybe you're building a\nreally serious application like something in health or\nmedicine, what you can do is you can fine tune\nthe model to work even better with your data. And a really, really\ngreat thing about this is the truth is it\nsounds complicated, but it's not in practice. All you really need is\na two-column CSV file. And here, what you're\nlooking at is something with a prompt and a response. And if you've got\na couple thousand rows using our framework Keras-- and Keras is my favorite way by\nfar of doing just applied AI. That means using AI in practice. You can tell I care a\nlot about-- both of us care a lot about\nhealth and medicine, so there's a lot of wonderful,\nmore than you could ever count, opportunities to do\ngood in the world in those fields using\ntechnologies like this. You can train the model to\ndo something really useful. So we have a really great\ntutorial about this. It's honestly about\nfive key lines of code. You import a model of\nGemma from Keras hub. This model is already\ninstruction tuned. You can prompt it\nin a line of code, and you can also\ndo LoRA fine-tuning in about a line of code, which\nalso sounds fancy, but it's not. So Keras is great\nfor Applied AI. If you're doing research,\nwe have a really wonderful framework called JAX. JAX is a Python machine\nlearning library, and I guess I have two\nthings to say about it. One is that at the\nhighest scales, JAX is the best place to go.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -7.425118446350098
      }
    ],
    "relevant_doc_ids": [
      "gHHjDRDNUNU__chunk_004"
    ]
  },
  {
    "query": "what are the customization options for voices and emotions in Gemini TTS",
    "reranked_docs": [
      {
        "id": "0d53f966-23ba-5e29-8cd7-78087ea10f94",
        "doc_id": "gHHjDRDNUNU__chunk_004",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 501.0,
        "timestamp_end": 641.8,
        "text": "you are able to generate high\nquality images, high quality videos. You also have-- and there is a\ncolleague from our team, Paul Ruiz, who is delivering\na talk this afternoon related to the Gemini Robotics,\nthe Gemini family of models related to people\napplying robotics, developing robotic\nsolution with applied AI. And also, just\nafter the session, we have one another one\nwith Omar and Gus Martins to talk about Gemma 3,\nwhich is the open models family for from\nGoogle DeepMind, also developed with all\nthe technologies we use to develop Gemini. SHRESTHA BASU MALLICK:\nAnd some really good benchmark performance-- LUCIANO MARTINS: Absolutely. SHRESTHA BASU\nMALLICK: --in Gemma 3 as well, as Omar and\nGus will tell you. LUCIANO MARTINS:\nSo still talking about the different\nexperiences you may have, we heard a lot of feedbacks\nfrom you folks around the world. And one of the most asked\nfeatures we always heard were related to letting Gemini\nto create high quality audio. So yesterday we launched what\nwe are calling the Gemini TTS model, where you can from text\ngenerate high quality audio. But the coolest\nthing about the TTS is that you are not\njust creating audio. You have abilities to customize\nemotions on the audio. You may have multiple voices. You can create the audios\nwith different languages, like Brazilian Portuguese. And you can even create\nmulti-speaker interactions. So if, for example, you want\nto deliver an experience where your users will be thoughts\nabout something on a podcast, like formats, like the one\nyou have on the NotebookLM that people love. Who here is using\nNotebookLM for anything? Amazing. So you can have something\nlike the AI Overviews that are developed and delivered\nby the Gemini TTS model using the TTS, right Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: And I think it\nis not the only thing related to audio. We brought that to the live\ninteractions as well, right? SHRESTHA BASU\nMALLICK: That's right. So the TTS models,\nlike Luciano said, are available via chat endpoint.",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": 0.763524055480957
      },
      {
        "id": "cc910b44-8924-521e-9890-82d926ac6eb6",
        "doc_id": "4TE-KFXvhAk__chunk_003",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 363.97,
        "timestamp_end": 521.659,
        "text": "On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API and also on [INAUDIBLE] API\nfor real generating audio. And some of the use\ncases here that we had in mind when\nwe built this were more dynamic audiobooks,\nsome more engaging podcasts, or, even for those of\nyou in customer support, producing more natural\nvoices into your workflows. We also have enhanced\ntooling, which is really cool, because now you\ncan use Grounding with Google Search, also with Code Execution\ntogether in just one API call, and URL context, which you\nheard during the keynote, which provides the model with\ndepth content from web pages. And since you can chain\nit to other tools, it's actually really powerful\nto build search agents. So really cool stuff. Lastly, just to call out that\nwe now offer Gemini SDK support for MCP, which reduces a\nlot of developer friction and simplifies building\nagentic capabilities. So you don't want to\nmiss tomorrow's talk to learn more about this. And Google AI Studio. Who here uses Google AI Studio? Can I see some hands? OK, awesome. Awesome. We have a lot to chat\nafter this session then. Google AI Studio is the\nperfect place for anyone to start developing with AI. It's the simplest way to test\nthe latest DeepMind models. We typically bring them\nto Google AI Studio so you can start prototyping\nand playing with the models. And you don't need Google\nCloud knowledge in order to set up your environment. It is free of charge, and you\ncan create, and test, and save your prompts. And there's also starter\napps that will inspire you. And that's exactly what I'm\ngoing to demo here today for you. I am going to show\nyou some of the work that the team has been doing. So if we go to Google AI Studio\nhere, if we go into Build, and Mumble Jumble is\nsomething that we've literally just created, and\nit's one of my favorite apps. So Mumble Jumble is one\nof those applications that",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -0.25326281785964966
      },
      {
        "id": "9b8e21a0-8c7a-5b78-9a21-66c03d49e1f5",
        "doc_id": "gHHjDRDNUNU__chunk_005",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 628.55,
        "timestamp_end": 769.3299999999999,
        "text": "LUCIANO MARTINS: And I think it\nis not the only thing related to audio. We brought that to the live\ninteractions as well, right? SHRESTHA BASU\nMALLICK: That's right. So the TTS models,\nlike Luciano said, are available via chat endpoint. And then on our Live API,\nyesterday, we also rolled out the native audio output models. So this is now\nGoogle's first release of an audio-to-audio\narchitecture. And there are two variants of\nthis model that you have access through the Gemini Live API,\nwhich is our real time API. So you have the Native\nAudio Dialogue model, which is the real benefit of\nthe native audio output models are that the voices\nthat come out of it are much more natural\nand much more compelling. The native audio-to-audio\narchitecture has better contextual\nunderstanding of what humans say to the AI. It can seamlessly transition\nbetween different languages, so maybe from Brazilian\nPortuguese to Bengali, if you want to switch\nin the same language, in the same sentence. And it's available on\nour Gemini real time API. We also have a version\nof this model available, again on the real time\nAPI with thinking enabled. So for more complex use\ncases, so one use case that comes to mind is let's say\nyou're building a gaming agent to play a strategy game. And you want that agent to be\nable to think, but also to talk to you with relatively\nlow latency. We have a thinking version\nof the native audio model that's also now available\nfrom the Live API. So please try it out\nand give us feedback. LUCIANO MARTINS: Perfect. So how people can start\nusing those models, Shrestha? SHRESTHA BASU MALLICK:\nThrough the API. Of course. But before we go there,\nthere are two other things that we should mention. It's hot off the news. You've probably\nall heard about it. We've also enabled\nan advanced reasoning mode called Deep Think. And the idea behind Deep\nThink is that the model 2.5 Pro can think through various\npossible answers to a problem before giving you\nthe best answer. So that is available\nin trusted testers,",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -2.827732801437378
      },
      {
        "id": "8c096a3a-3e99-5203-b8c9-91d5e3adc2b2",
        "doc_id": "4TE-KFXvhAk__chunk_009",
        "video_id": "4TE-KFXvhAk",
        "title": "Google I/O 2025 \u2013 4TE-KFXvhAk",
        "timestamp_start": 1066.46,
        "timestamp_end": 1237.45,
        "text": "And I would also like to\ndemo one of these new apps that we have in AI Studio. So I'm going back to\nthe laptop, and I'm going to show you something\nthat the team also just created. So AI Studio also got a facelift\nand has some new features. And the chat interface\nis still the same, but you've seen the talk to\nGemini live during the keynote. We have the new\nGenerative Media Console, which allows you to\ncreate and interact with our most creative models. And then we have\nthe Build, which is where all these new\napps are coming to. So I just wanted to show\nyou very quickly this one. There we go. And then we basically\ncan choose here what are the sounds\nthat we want. And this is all powered by\nLyria, our music generation model. [GENERATED MUSIC PLAYING] And just for the\ninterest of time, I'm not going to\nkeep playing it. But you can see some of the\ncapabilities of these models that we're bringing\nto AI Studio. We'll continue to the slides. In the Console,\nas you could see, you have access to\nour image generation, our video generation, and music\ngeneration models with applets to get you started. And so that's a really\ncool thing for you to play with after this session. Some of our videos,\nvery realistic images with a really good understanding\nof real-world physics and dynamics, improved quality,\nand more and more capabilities coming to these models. And this is the example\nthat I just showed. We've made Lyria RealTime our\ninteractive music generation model, which powers MusicFX DJ. It's available in the\nAPI and AI Studio, and you can check\nour API documentation for more information. This also allows\neveryone to interact, to create, and to perform\ngenerative music in real time. It's really cool. You might remember that in the\nshow before the first keynote, you might have\nseen this console. That's exactly why I wanted to\nshow you this particular app in this session. But there's a lot more that\nyou can try afterwards. And shifting the\ngears towards Gemma, earlier this year we\nreleased Gemma 3, which",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -8.697874069213867
      },
      {
        "id": "bcce6feb-c12e-5253-92d4-926629505824",
        "doc_id": "gHHjDRDNUNU__chunk_022",
        "video_id": "gHHjDRDNUNU",
        "title": "Google I/O 2025 \u2013 gHHjDRDNUNU",
        "timestamp_start": 2746.57,
        "timestamp_end": 2767.29,
        "text": "more of the other features and\npossibilities with the models. And the last one is the Gemini\nCookbook, a GitHub repository that our team curates and keep\nupdating with a lot of sample experience for you. Thank you so much, Shrestha. Thank you so much you all. SHRESTHA BASU MALLICK: Thank you\nall for coming out to hear us. [CHEERING] [MUSIC PLAYING]",
        "source": "youtube",
        "speaker": "unknown",
        "rerank_score": -9.40207290649414
      }
    ],
    "relevant_doc_ids": [
      "gHHjDRDNUNU__chunk_004"
    ]
  }
]
[
  {
    "query": "What were the main topics and components discussed in Google's AI Stack for Developers session at Google I/O, including foundation models and the developer frameworks mentioned?",
    "retrieved_doc_ids": [
      "gHHjDRDNUNU__chunk_007",
      "4TE-KFXvhAk__chunk_017",
      "4TE-KFXvhAk__chunk_001",
      "4TE-KFXvhAk__chunk_019",
      "4TE-KFXvhAk__chunk_000"
    ],
    "recall_at_5": 1,
    "per_doc_cosine": [],
    "avg_cosine": 0.0,
    "reciprocal_rank": 1.0,
    "llm_score": null,
    "retrieved_answers": [
      {
        "doc_id": "4TE-KFXvhAk__chunk_000",
        "text": "[MUSIC PLAYING] JOANA CARRASQUEIRA:\nHello, everyone. My name is Joana Carrasqueira,\nand I lead Developer Relations at Google DeepMind. JOSH GORDON: Hi, everyone. I'm Josh. JOANA CARRASQUEIRA:\nAnd we're very excited to welcome you to\nour session, Google's AI Stack for Developers. We'll start by giving you a\nquick overview of Google's AI stack. Who's at I/O for the first time? Can I see some hands up? Oh, OK. Welcome to Google\nI/O. It's a pleasure to have you with us today. So we'll start by giving\nyou an overview of Google's end-to-end ecosystem of AI. And as you know, we've\nbeen leading the way in AI for decades, since we\nopen-sourced TensorFlow in 2015, from when we published our\nfield-defining research with transformers\nin 2017, to Gemini. And we are now in\nthe Gemini era. So we've been releasing a lot. Relentlessly, as it's\nbeen called today, we've been shipping many\nfeatures, many new products. And in our talk,\nwe're actually going to give you an overview\nof everything that's new for developers\nthroughout the AI stack. Our mission is to empower every\ndeveloper and organization to harness the power of AI. And Google's stack is so good\nand flexible because it combines very robust infrastructure\nwith state-of-the-art research. And all of this enables\nreal-world applications come to life that change\nentire fields, industries, and companies. We'll start by discussing\nfoundation models, touching upon our\nGemini, Gemma, and some of our domain-specific models. JOSH GORDON: After\nfoundation models, we'll take a look\nat AI frameworks that we use to build them. So we'll talk about JAX, which\nis really great for researchers. We'll talk about Keras, which is\nreally amazing for applied AI. Later on, we'll even\ntalk a little bit about the work we're\ndoing with PyTorch. JOANA CARRASQUEIRA: We'll\nalso touch upon some developer tools for all\ntypes of experience from beginners to advanced. JOSH GORDON: Then we'll talk a\nlittle bit about infrastructure. And this talk is about\nsoftware not hardware.",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_017",
        "text": "and it runs it on a fleet of\nreal devices of different sizes just to verify that\nit works really well. So if you're interested\nin mobile development, check out Google AI Edge. Google AI Edge,\nit's really cool. And with that, I'll\nhand it over to Joana to talk about what's next. JOANA CARRASQUEIRA: Awesome. Thank you, Josh. And you've heard it in the\nkeynotes in the previous session with Demis and Sergey,\nwe're pushing the boundaries of what's possible to build with\nAI here at Google and Google DeepMind. And we're really excited to\nbring all this innovation and put it in the\nhands of developers, in the hands of the community. And it's never been a better\ntime to build and co-create together. So we really believe\nin a future where AI is changing various fields\nacross scientific discovery, health care, and so many more. And we're going to achieve\nthis radical abundance in a safe and\nresponsible way, and we want to get there with\nyou, with the community. So let's have a look\nat some of the domains that we believe that have a\nhuge potential for developers and humanity at scale. AlphaEvolve, a\nGemini-powered coding agent for designing\nadvanced algorithms. A self-improving coding agent. And we all know that\nlarge language models can summarize documents. They can generate code. You can even\nbrainstorm with them. But with AlphaEvolve,\nwe're really expanding these\ncapabilities, and we are targeting fundamental\nand highly complex problems on\nmathematics and coding. AlphaEvolve leverages\nGemini Flash and Pro, and it's one of the big\npromises for the future. Another one-- and I'm really\nexcited about AI co-scientist. It's another\nscientific breakthrough that we're seeing, especially\nin the medical, in medicine, and research fields. And our goal is to accelerate\nthe speed up discovery and drug development. And with AI\nco-scientist, a scientist can give a research goal to\nthe agent in natural language, and then the AI\nco-scientist is designed to give you an overview, and a\nhypothesis, and a methodology.",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_019",
        "text": "So in order to learn\nmore, Josh, what do our developers have to do? JOSH GORDON: We have\njust a few links for you. So no problem. But we talked about a lot of\ndifferent tools in the stack. JOANA CARRASQUEIRA: We did. JOSH GORDON: So I don't\nwant to read the slide, but let me just point you\nto a couple highlights. ai.google.dev is the\nbest place to go to get started with Gemini and Gemma. We have a cookbook for Gemini. We have a cookbook for Gemma. Google AI Studio is\naistudio.google.com. If you're interested in JAX\nand Keras, there are the links. If you happen to be\ninterested in XLA, please check it\nout-- openxla.org. Google AI Edge is\nat the very bottom if you're a mobile\ndeveloper and you're interested in mobile deployment. And just to be clear, there's\nso many amazing things in the Google AI stack we didn't\nhave time to talk about today. Vertex has really amazing tools\nfor enterprise developers. But please, start\nhere, have fun, and, yeah, we're\naround after the talk. JOANA CARRASQUEIRA:\nYes, absolutely. And the Developer Relations\nteam is just outside. We have some really\ncool demo stations that you can experience. Engage with the team. Check out the sessions\ntomorrow, especially on the Gemini API,\nGemmaverse, and robotics. We have a lot of\ncool stuff that we want to put in the\nhands of developers. Many early access\nprograms as well. Stay in touch, stay\nengaged, and let's co-create the future of AI together. Thank you so much. JOSH GORDON: Thanks a lot. Thanks [APPLAUSE] [MUSIC PLAYING]",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_001",
        "text": "JOANA CARRASQUEIRA: We'll\nalso touch upon some developer tools for all\ntypes of experience from beginners to advanced. JOSH GORDON: Then we'll talk a\nlittle bit about infrastructure. And this talk is about\nsoftware not hardware. Our hardware infrastructure\nis TPUs, which you've probably heard a lot of. But in this talk,\nI'll briefly talk about XLA, which is a\nmachine learning compiler. And I'll talk about\nsome of the work we're doing for\ninference-- so making it possible to serve\nmodels at scale super efficiently with really\ncool new things with XLA for JAX and PyTorch. JOANA CARRASQUEIRA: OK. JOSH GORDON: Oh, and then\none more thing to mention. I went too fast. So sorry about that. So a lot of this talk is about\nthese huge foundation models. Towards the end of the talk,\nI'll talk about Google AI Edge. And we'll talk about\ndeploying small models on device, which is also super\nimportant for many reasons. JOANA CARRASQUEIRA: Awesome. OK. Let's start by exploring\nour core intelligence within our stack. We'll start with\nour Gemini models, which are our most capable\nand versatile model family. And our core philosophy here at\nGoogle is to provide developers with state-of-the-art models and\ntools that you can use to build powerful applications\nall throughout. And our Gemini models, they\nare known for being multimodal, have a long context window, and\nhaving very powerful reasoning, but we've built a variety of\nmodels for different use cases. So depending on what\nyou're trying to build, Google will have a model that\nis tailored for your use case. And I would like to just\ngive you a quick walkthrough of these models. I know you've heard\nit during the keynote, but, just very quickly,\nGemini 2.5 Pro, which is our most advanced\nmodel yet, especially for high complex tasks that\nbenefit from deep reasoning, it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it",
        "cosine": null
      },
      {
        "doc_id": "gHHjDRDNUNU__chunk_007",
        "text": "We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there. If you are a Google\nColab user, you have ways to interact with\nthe Gemini models as well. So the key idea is to\nmake it easier for you. No matter where you are having\nyour development experience, We are trying to bring\nGemini API closer to you. SHRESTHA BASU MALLICK:\nThat's correct. And I think this is\nalso a good place to make a plug for Google\nAI Studio, which of course, everyone knows about and loves. It is the place where\na lot of our developers first test out the\ncapabilities of the API before committing to\nbuilding applications at scale with the API. And Google AI Studio now\nhas code generation also. LUCIANO MARTINS: Absolutely. SHRESTHA BASU MALLICK: So this\nis a quick, high level overview of all the components that are\navailable through the Gemini API. It's pretty standard. You send in your prompt, you get\na response back from the model. If there's a tool\ncall needed, we do support a set of\nfirst party Google tools, as well as function calling. So in terms of the first party\nGoogle tools that we support, we, of course, support\nGoogle Search as a tool. The best search engine\nout there is now available as a tool for the\nmodel to call when it needs to. Now in addition to the\ninformation that you get out of Search, if you want to\nretrieve more in-depth content from web pages for\napplications such as you're building your own version\nof a research agent, so you pull some\ninformation from Search. You pull a set of\nURLs, but if you want to extract a little\nmore in depth content, we just released a new tool\nat I/O called URL Context. And then, of course, you can\ntake all that information. And if you want to create\nbeautiful charts out of it or run some analysis,\nwe make code execution available as a tool to\nyou via the API as well. Then, of course, you\nhave function calling.",
        "cosine": null
      }
    ],
    "true_documents": []
  },
  {
    "query": "Who were the presenters introducing Google's AI stack at I/O, and what did they say about the end-to-end ecosystem, infrastructure, and real-world applications?",
    "retrieved_doc_ids": [
      "4TE-KFXvhAk__chunk_017",
      "Uh-7YX8tkxI__chunk_000",
      "4TE-KFXvhAk__chunk_001",
      "4TE-KFXvhAk__chunk_019",
      "4TE-KFXvhAk__chunk_000"
    ],
    "recall_at_5": 1,
    "per_doc_cosine": [],
    "avg_cosine": 0.0,
    "reciprocal_rank": 1.0,
    "llm_score": null,
    "retrieved_answers": [
      {
        "doc_id": "4TE-KFXvhAk__chunk_000",
        "text": "[MUSIC PLAYING] JOANA CARRASQUEIRA:\nHello, everyone. My name is Joana Carrasqueira,\nand I lead Developer Relations at Google DeepMind. JOSH GORDON: Hi, everyone. I'm Josh. JOANA CARRASQUEIRA:\nAnd we're very excited to welcome you to\nour session, Google's AI Stack for Developers. We'll start by giving you a\nquick overview of Google's AI stack. Who's at I/O for the first time? Can I see some hands up? Oh, OK. Welcome to Google\nI/O. It's a pleasure to have you with us today. So we'll start by giving\nyou an overview of Google's end-to-end ecosystem of AI. And as you know, we've\nbeen leading the way in AI for decades, since we\nopen-sourced TensorFlow in 2015, from when we published our\nfield-defining research with transformers\nin 2017, to Gemini. And we are now in\nthe Gemini era. So we've been releasing a lot. Relentlessly, as it's\nbeen called today, we've been shipping many\nfeatures, many new products. And in our talk,\nwe're actually going to give you an overview\nof everything that's new for developers\nthroughout the AI stack. Our mission is to empower every\ndeveloper and organization to harness the power of AI. And Google's stack is so good\nand flexible because it combines very robust infrastructure\nwith state-of-the-art research. And all of this enables\nreal-world applications come to life that change\nentire fields, industries, and companies. We'll start by discussing\nfoundation models, touching upon our\nGemini, Gemma, and some of our domain-specific models. JOSH GORDON: After\nfoundation models, we'll take a look\nat AI frameworks that we use to build them. So we'll talk about JAX, which\nis really great for researchers. We'll talk about Keras, which is\nreally amazing for applied AI. Later on, we'll even\ntalk a little bit about the work we're\ndoing with PyTorch. JOANA CARRASQUEIRA: We'll\nalso touch upon some developer tools for all\ntypes of experience from beginners to advanced. JOSH GORDON: Then we'll talk a\nlittle bit about infrastructure. And this talk is about\nsoftware not hardware.",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_017",
        "text": "and it runs it on a fleet of\nreal devices of different sizes just to verify that\nit works really well. So if you're interested\nin mobile development, check out Google AI Edge. Google AI Edge,\nit's really cool. And with that, I'll\nhand it over to Joana to talk about what's next. JOANA CARRASQUEIRA: Awesome. Thank you, Josh. And you've heard it in the\nkeynotes in the previous session with Demis and Sergey,\nwe're pushing the boundaries of what's possible to build with\nAI here at Google and Google DeepMind. And we're really excited to\nbring all this innovation and put it in the\nhands of developers, in the hands of the community. And it's never been a better\ntime to build and co-create together. So we really believe\nin a future where AI is changing various fields\nacross scientific discovery, health care, and so many more. And we're going to achieve\nthis radical abundance in a safe and\nresponsible way, and we want to get there with\nyou, with the community. So let's have a look\nat some of the domains that we believe that have a\nhuge potential for developers and humanity at scale. AlphaEvolve, a\nGemini-powered coding agent for designing\nadvanced algorithms. A self-improving coding agent. And we all know that\nlarge language models can summarize documents. They can generate code. You can even\nbrainstorm with them. But with AlphaEvolve,\nwe're really expanding these\ncapabilities, and we are targeting fundamental\nand highly complex problems on\nmathematics and coding. AlphaEvolve leverages\nGemini Flash and Pro, and it's one of the big\npromises for the future. Another one-- and I'm really\nexcited about AI co-scientist. It's another\nscientific breakthrough that we're seeing, especially\nin the medical, in medicine, and research fields. And our goal is to accelerate\nthe speed up discovery and drug development. And with AI\nco-scientist, a scientist can give a research goal to\nthe agent in natural language, and then the AI\nco-scientist is designed to give you an overview, and a\nhypothesis, and a methodology.",
        "cosine": null
      },
      {
        "doc_id": "Uh-7YX8tkxI__chunk_000",
        "text": "[MUSIC PLAYING] ALEX KANTROWITZ: All\nright, everybody, we have an amazing\ncrowd here today. We're going to be\nlive-streaming this. So let's hear you make\nsome noise so everybody can hear that you're here. Let's go. [CHEERING] Not bad. I'm Alex Kantrowitz I'm the host\nof \"Big Technology\" podcast, and I'm here to speak with you\nabout the frontiers of AI with two amazing guests. Demis Hassabis, the CEO\nof DeepMind, is here. Google DeepMind. Good to see you, Demis. DEMIS HASSABIS:\nGood to see you too. ALEX KANTROWITZ: And we\nhave a special guest. Sergey Brin, the co-founder\nof Google, is also here. [CHEERS, APPLAUSE] All right. So this is going to be fun. Let's start with\nthe frontier models. Demis, this is for you. With what we know today\nabout frontier models, how much improvement is\nthere left to be unlocked, and why do you think so\nmany smart people are saying that the gains\nare about to level off? DEMIS HASSABIS: I think we're\nseeing incredible progress. You've all seen it today, all\nthe amazing stuff we showed in the [INAUDIBLE] keynote. So I think we're\nseeing incredible gains with the existing techniques,\npushing them to the limit. But we're also inventing new\nthings all the time as well. And I think to get all\nthe way to something like AGI may require one or\ntwo more new breakthroughs. And I think we have\nlots of promising ideas that we're cooking\nup and we hope to bring into the main\nbranch of the Gemini branch. ALEX KANTROWITZ: All right. And so there's been this\ndiscussion about scale. Does solve all problems,\nor does it not? So I want to ask you, in terms\nof the improvement that's available today, is\nscale still the star, or is it a supporting actor? DEMIS HASSABIS: I think I've\nalways been of the opinion that you need both. You need to scale to the\nmaximum the techniques that you know about. You want to exploit them to\nthe limit, whether that's data or compute scale. And at the same time, you want\nto spend a bunch of effort on what's coming next maybe six\nmonths, a year down the line,",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_019",
        "text": "So in order to learn\nmore, Josh, what do our developers have to do? JOSH GORDON: We have\njust a few links for you. So no problem. But we talked about a lot of\ndifferent tools in the stack. JOANA CARRASQUEIRA: We did. JOSH GORDON: So I don't\nwant to read the slide, but let me just point you\nto a couple highlights. ai.google.dev is the\nbest place to go to get started with Gemini and Gemma. We have a cookbook for Gemini. We have a cookbook for Gemma. Google AI Studio is\naistudio.google.com. If you're interested in JAX\nand Keras, there are the links. If you happen to be\ninterested in XLA, please check it\nout-- openxla.org. Google AI Edge is\nat the very bottom if you're a mobile\ndeveloper and you're interested in mobile deployment. And just to be clear, there's\nso many amazing things in the Google AI stack we didn't\nhave time to talk about today. Vertex has really amazing tools\nfor enterprise developers. But please, start\nhere, have fun, and, yeah, we're\naround after the talk. JOANA CARRASQUEIRA:\nYes, absolutely. And the Developer Relations\nteam is just outside. We have some really\ncool demo stations that you can experience. Engage with the team. Check out the sessions\ntomorrow, especially on the Gemini API,\nGemmaverse, and robotics. We have a lot of\ncool stuff that we want to put in the\nhands of developers. Many early access\nprograms as well. Stay in touch, stay\nengaged, and let's co-create the future of AI together. Thank you so much. JOSH GORDON: Thanks a lot. Thanks [APPLAUSE] [MUSIC PLAYING]",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_001",
        "text": "JOANA CARRASQUEIRA: We'll\nalso touch upon some developer tools for all\ntypes of experience from beginners to advanced. JOSH GORDON: Then we'll talk a\nlittle bit about infrastructure. And this talk is about\nsoftware not hardware. Our hardware infrastructure\nis TPUs, which you've probably heard a lot of. But in this talk,\nI'll briefly talk about XLA, which is a\nmachine learning compiler. And I'll talk about\nsome of the work we're doing for\ninference-- so making it possible to serve\nmodels at scale super efficiently with really\ncool new things with XLA for JAX and PyTorch. JOANA CARRASQUEIRA: OK. JOSH GORDON: Oh, and then\none more thing to mention. I went too fast. So sorry about that. So a lot of this talk is about\nthese huge foundation models. Towards the end of the talk,\nI'll talk about Google AI Edge. And we'll talk about\ndeploying small models on device, which is also super\nimportant for many reasons. JOANA CARRASQUEIRA: Awesome. OK. Let's start by exploring\nour core intelligence within our stack. We'll start with\nour Gemini models, which are our most capable\nand versatile model family. And our core philosophy here at\nGoogle is to provide developers with state-of-the-art models and\ntools that you can use to build powerful applications\nall throughout. And our Gemini models, they\nare known for being multimodal, have a long context window, and\nhaving very powerful reasoning, but we've built a variety of\nmodels for different use cases. So depending on what\nyou're trying to build, Google will have a model that\nis tailored for your use case. And I would like to just\ngive you a quick walkthrough of these models. I know you've heard\nit during the keynote, but, just very quickly,\nGemini 2.5 Pro, which is our most advanced\nmodel yet, especially for high complex tasks that\nbenefit from deep reasoning, it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it",
        "cosine": null
      }
    ],
    "true_documents": []
  },
  {
    "query": "What are Google Gemini models and how do Gemini 2.5 Pro and Gemini 2.5 Flash differ in capabilities like coding and reasoning?",
    "retrieved_doc_ids": [
      "gHHjDRDNUNU__chunk_006",
      "gHHjDRDNUNU__chunk_003",
      "4TE-KFXvhAk__chunk_002",
      "gHHjDRDNUNU__chunk_002",
      "gHHjDRDNUNU__chunk_017"
    ],
    "recall_at_5": 0,
    "per_doc_cosine": [],
    "avg_cosine": 0.0,
    "reciprocal_rank": 0.0,
    "llm_score": null,
    "retrieved_answers": [
      {
        "doc_id": "gHHjDRDNUNU__chunk_017",
        "text": "If you're building\na research agent, maybe you want to\nuse the Chat API. If you're building\na gaming agent, maybe you want to use the real\ntime API or a customer support agent. Of course, you can build\nresearch agents also on the real time API. But in general, you now\nhave these two APIs. And then we've talked\nabout tools a lot. So all I'll say is\ngive us feedback in terms of what are some\nof the other tools that you would see Google make available\nthrough the Gemini API. LUCIANO MARTINS: Yeah. So as you just said,\nthe Gemini 2.5 Pro-- 2.5 models are thinking\ncapable, so they can do more advanced and\nmore complex reasoning. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS:\nAnd maybe we could highlight that now,\nspecifically for the 2.5 Flash, we have the ability of\nusing what we just mentioned, the thinking budgets, where\nyou can calibrate how deep and how much the model\nwill do reasoning cycles. And also for both models,\nwe can have access to what we are calling\nthought summaries. SHRESTHA BASU MALLICK: Yes. LUCIANO MARTINS: So\nbasically you are not only counting on the model\nability to reason, but also you can see on your\nresponse which key steps or which key reasoning\nthoughts the model had to get to our conclusion. So basically, that's a\npretty basic Gemini API code using the Python SDK. For those of you who\nnever used the SDK before, it's pretty straightforward. So three key blocks. You need to import the SDK,\nyou instantiate the client, and then you interact\nwith the model you want. Here we are interacting\nwith the Gemini 2.5 Flash. You send your prompts. And then you have\none specific bit called ThinkingConfig,\nwhere, in this code, you are asking the API\nto include the thought summaries on your\nresponse, and that's to include thoughts\nequals true, and also the amount of tokens you want\nto use during the reasoning process. On your response,\nyou're going to have part of the response will\nbe the thought summaries. So it's separated from the\nfinal answer by intention. If you want to keep\nthose on your backend,",
        "cosine": null
      },
      {
        "doc_id": "gHHjDRDNUNU__chunk_002",
        "text": "when we think about the key\nstandard benchmarks, Shrestha? SHRESTHA BASU\nMALLICK: Yeah, we'll go to the benchmarks\nin a second. But the message\nthat I want everyone to take away from this\nslide is whether it's a super complex task, or whether\nit's on-device processing, you now have a Gemini model\nthat you can use for it. And it's pretty powerful. And it's pretty price effective. LUCIANO MARTINS: Yeah,\nmaybe it is also worth mentioning, Shrestha,\nthat some folks here may be developing with\nGemini since it was released. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS: So using the\nAI Studio, using the Gemini API, the older versions\nof Gemini like 2.0, 1.5 are still available. But we really\nencourage everybody to start experimenting\nwith the newer ones with better capabilities\nand better performance. SHRESTHA BASU MALLICK:\nThat's correct. So 2.0 Flash continues to be one\nof our most widely-used models, just again, because of how good\nit is for the price it's at. And we have a lot of people\nwho are still using it. And then we also have some\npeople on the 1.5 models. But we're encouraging\npeople now, we're two generations\nahead, to start using the 2.5 series of\nmodels and give us feedback. LUCIANO MARTINS: Nice. So please tell us a little bit\nabout the benchmarks, Shrestha. SHRESTHA BASU MALLICK:\nYeah, next slide. Yeah, so we're not going\nto spend a lot of time on the benchmarks. But the two key points\nthat I'd like to hit here is number one, is one\nway in which we measure how good these models are is\nby putting them on LMArena, which is where developers\nand builders like yourself test out\nall these models and give it an ELO rating. And we're, of course, very\nproud that right now you have three Gemini models in\nthe top 10 in the LMArena. LUCIANO MARTINS:\nIncluding the top one. SHRESTHA BASU\nMALLICK: Including, how did I forget that? We are number one guys,\nand we're very proud of it. [CHEERING] Thank you. One of the things,\none of the use cases that has been really emerging\nin the last couple of months",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_002",
        "text": "it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it because of its\nefficiency and speed, and it's now even better at\nalmost every single dimension. So we improved\nall the benchmarks across reasoning, coding,\nmultimodality, and also long context. Then we have our Gemini 2.0\nFlash, which is fast and cheap, works fine, and our\nGemini Nano, which is optimized for on-device tasks. And as you've heard, we've\nbeen shipping relentlessly. And I would like to give\nyou just a quick highlight of everything that we've\nbeen shipping in AI Studio and the Gemini API. There's a talk\ntomorrow that I would like to invite you to attend,\nwhich is by Shrestha Basu Mallick, a group product\nmanager on Gemini API, and Luciano Martins, our\ntechnical lead for Gemini API from DevRel. And they're going to do a deep\ndive into everything that's new within the Gemini\nAPI, so you definitely don't want to miss\nthat session tomorrow. But for now, just a glimpse to\nget you excited about what's new in AI Studio. We've built a new tab\nthat is called Build that instantly generates web apps. And it's really cool, because it\nenables developers and builders alike to prototype very\nquickly with natural language. We have a new generative media\nexperience in AI Studio as well, and I'm going to\ndemo all of this so you can see how\nit actually works. And we are always\nlistening to the community. We listen to your\nfeedback, and we always build with developers in mind. And that's why that\nsome of these features were actually\nrequested by community, and that's what happened with\nthe built-in new dashboard. You requested. We built it. And we also have some\nnew Native Audio and TTS support in AI Studio. On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API",
        "cosine": null
      },
      {
        "doc_id": "gHHjDRDNUNU__chunk_006",
        "text": "mode called Deep Think. And the idea behind Deep\nThink is that the model 2.5 Pro can think through various\npossible answers to a problem before giving you\nthe best answer. So that is available\nin trusted testers, but will be rolled\nout more widely soon. And then we also have\nGemini Diffusion, which is the first time\nwe are testing a diffusion architecture as opposed to an\nautoregressive architecture. And these models are faster\nthan the fastest model out today and for almost\nsimilar, the performance. So try out both when you have\naccess to both Gemini Deep Diffusion and Deep Think. LUCIANO MARTINS: Perfect. SHRESTHA BASU MALLICK: All\nright, so the Gemini API. LUCIANO MARTINS: So yeah, if\nyou start thinking, especially after the keynote\nspeakers today, there are many surfaces where\nyou can interact with the Gemini models. So you have the Gemini app. You have Gemini available\non Google Cloud as the Code Assist, the assistant-- the\nCloud Assist inside the Console. You have Gemini on Workspace. So when you are talking\nabout developer experiences, we have the one\nspecific Gemini API with similar experiences as you\nhave on the no-code UI that we call AI Studio, where you\ncan start experimenting and developing your solutions\nusing the Gemini models, right? SHRESTHA BASU\nMALLICK: That's right. LUCIANO MARTINS: So\nbasically, the Gemini API is a very low barrier\nentry for having Gemini programmatic experiences. You have access to\nall public models there from the Gen Media ones,\nGemma, all the Gemini models, variants. You have a very generous\nfree of charge tier, where you can start\nexperimenting just after the session\nwithout concerns about credit cards, costs,\nbilling, anything of that. We keep developing\nmore SDKs for this API. So for now, you have available\none SDK for Python, JavaScript, Go, and Java. We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there.",
        "cosine": null
      },
      {
        "doc_id": "gHHjDRDNUNU__chunk_003",
        "text": "SHRESTHA BASU\nMALLICK: Including, how did I forget that? We are number one guys,\nand we're very proud of it. [CHEERING] Thank you. One of the things,\none of the use cases that has been really emerging\nin the last couple of months is, of course, app creation\nand live coding and going from 0 to 1. The best leaderboard right now\nfor that is the WebDev Arena. And we again, are number one\nwith 2.5 Pro on WebDev Arena. So if you are trying to build\n0 to 1 or few shot apps, Gemini 2.5 Pro is a\ngreat model for that. But in addition to\nuser preferences, we also like to measure\nhow well our models are doing based on rigorous\nacademic benchmarks. And so I'm not going to\ngo into a lot of details on these slides. These are available\nin blog posts. Reach out to me\nand Luciano if you want to know how exactly\nthese models are doing. But the TL;DR here is whether\nit's in highly complex domain-specific questions,\nwhether it's in coding, whether it's in\nmultimodal understanding, the Gemini 2.5 Pro is leading\non a lot of these academic benchmarks as well. Finally, performance is\none side of the coin. The other side of the\ncoin is, of course, price. This is a chart from Swyx that's\nbeen very popular on Twitter for the last few years-- on X, I'm sorry for\nthe last few months. But this really shows that\neven for price performance, the Gemini 2.5 models are\nreally at the frontier. Luciano? LUCIANO MARTINS:\nYeah, and I think it is worth\nmentioning, Shrestha, especially for those\nof you who watched Sundar's keynote yesterday and\nalso the developer's keynotes. Inside the DeepMind, we\nare doing this huge effort of trying to cover the\ndifferent aspects of experiences you may have. So we will talk a lot\nabout Gemini here. But also, we have\nthe model family that we are calling\nGenMedia, where you are able to generate high\nquality images, high quality videos. You also have-- and there is a\ncolleague from our team, Paul Ruiz, who is delivering\na talk this afternoon related to the Gemini Robotics,\nthe Gemini family of models",
        "cosine": null
      }
    ],
    "true_documents": []
  },
  {
    "query": "What does Google describe about tooling and infrastructure for AI deployment, including XLA, TPUs, and on-device inference via Google AI Edge?",
    "retrieved_doc_ids": [
      "4TE-KFXvhAk__chunk_017",
      "4TE-KFXvhAk__chunk_001",
      "4TE-KFXvhAk__chunk_019",
      "4TE-KFXvhAk__chunk_016",
      "4TE-KFXvhAk__chunk_000"
    ],
    "recall_at_5": 1,
    "per_doc_cosine": [],
    "avg_cosine": 0.0,
    "reciprocal_rank": 0.5,
    "llm_score": null,
    "retrieved_answers": [
      {
        "doc_id": "4TE-KFXvhAk__chunk_017",
        "text": "and it runs it on a fleet of\nreal devices of different sizes just to verify that\nit works really well. So if you're interested\nin mobile development, check out Google AI Edge. Google AI Edge,\nit's really cool. And with that, I'll\nhand it over to Joana to talk about what's next. JOANA CARRASQUEIRA: Awesome. Thank you, Josh. And you've heard it in the\nkeynotes in the previous session with Demis and Sergey,\nwe're pushing the boundaries of what's possible to build with\nAI here at Google and Google DeepMind. And we're really excited to\nbring all this innovation and put it in the\nhands of developers, in the hands of the community. And it's never been a better\ntime to build and co-create together. So we really believe\nin a future where AI is changing various fields\nacross scientific discovery, health care, and so many more. And we're going to achieve\nthis radical abundance in a safe and\nresponsible way, and we want to get there with\nyou, with the community. So let's have a look\nat some of the domains that we believe that have a\nhuge potential for developers and humanity at scale. AlphaEvolve, a\nGemini-powered coding agent for designing\nadvanced algorithms. A self-improving coding agent. And we all know that\nlarge language models can summarize documents. They can generate code. You can even\nbrainstorm with them. But with AlphaEvolve,\nwe're really expanding these\ncapabilities, and we are targeting fundamental\nand highly complex problems on\nmathematics and coding. AlphaEvolve leverages\nGemini Flash and Pro, and it's one of the big\npromises for the future. Another one-- and I'm really\nexcited about AI co-scientist. It's another\nscientific breakthrough that we're seeing, especially\nin the medical, in medicine, and research fields. And our goal is to accelerate\nthe speed up discovery and drug development. And with AI\nco-scientist, a scientist can give a research goal to\nthe agent in natural language, and then the AI\nco-scientist is designed to give you an overview, and a\nhypothesis, and a methodology.",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_001",
        "text": "JOANA CARRASQUEIRA: We'll\nalso touch upon some developer tools for all\ntypes of experience from beginners to advanced. JOSH GORDON: Then we'll talk a\nlittle bit about infrastructure. And this talk is about\nsoftware not hardware. Our hardware infrastructure\nis TPUs, which you've probably heard a lot of. But in this talk,\nI'll briefly talk about XLA, which is a\nmachine learning compiler. And I'll talk about\nsome of the work we're doing for\ninference-- so making it possible to serve\nmodels at scale super efficiently with really\ncool new things with XLA for JAX and PyTorch. JOANA CARRASQUEIRA: OK. JOSH GORDON: Oh, and then\none more thing to mention. I went too fast. So sorry about that. So a lot of this talk is about\nthese huge foundation models. Towards the end of the talk,\nI'll talk about Google AI Edge. And we'll talk about\ndeploying small models on device, which is also super\nimportant for many reasons. JOANA CARRASQUEIRA: Awesome. OK. Let's start by exploring\nour core intelligence within our stack. We'll start with\nour Gemini models, which are our most capable\nand versatile model family. And our core philosophy here at\nGoogle is to provide developers with state-of-the-art models and\ntools that you can use to build powerful applications\nall throughout. And our Gemini models, they\nare known for being multimodal, have a long context window, and\nhaving very powerful reasoning, but we've built a variety of\nmodels for different use cases. So depending on what\nyou're trying to build, Google will have a model that\nis tailored for your use case. And I would like to just\ngive you a quick walkthrough of these models. I know you've heard\nit during the keynote, but, just very quickly,\nGemini 2.5 Pro, which is our most advanced\nmodel yet, especially for high complex tasks that\nbenefit from deep reasoning, it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_019",
        "text": "So in order to learn\nmore, Josh, what do our developers have to do? JOSH GORDON: We have\njust a few links for you. So no problem. But we talked about a lot of\ndifferent tools in the stack. JOANA CARRASQUEIRA: We did. JOSH GORDON: So I don't\nwant to read the slide, but let me just point you\nto a couple highlights. ai.google.dev is the\nbest place to go to get started with Gemini and Gemma. We have a cookbook for Gemini. We have a cookbook for Gemma. Google AI Studio is\naistudio.google.com. If you're interested in JAX\nand Keras, there are the links. If you happen to be\ninterested in XLA, please check it\nout-- openxla.org. Google AI Edge is\nat the very bottom if you're a mobile\ndeveloper and you're interested in mobile deployment. And just to be clear, there's\nso many amazing things in the Google AI stack we didn't\nhave time to talk about today. Vertex has really amazing tools\nfor enterprise developers. But please, start\nhere, have fun, and, yeah, we're\naround after the talk. JOANA CARRASQUEIRA:\nYes, absolutely. And the Developer Relations\nteam is just outside. We have some really\ncool demo stations that you can experience. Engage with the team. Check out the sessions\ntomorrow, especially on the Gemini API,\nGemmaverse, and robotics. We have a lot of\ncool stuff that we want to put in the\nhands of developers. Many early access\nprograms as well. Stay in touch, stay\nengaged, and let's co-create the future of AI together. Thank you so much. JOSH GORDON: Thanks a lot. Thanks [APPLAUSE] [MUSIC PLAYING]",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_000",
        "text": "[MUSIC PLAYING] JOANA CARRASQUEIRA:\nHello, everyone. My name is Joana Carrasqueira,\nand I lead Developer Relations at Google DeepMind. JOSH GORDON: Hi, everyone. I'm Josh. JOANA CARRASQUEIRA:\nAnd we're very excited to welcome you to\nour session, Google's AI Stack for Developers. We'll start by giving you a\nquick overview of Google's AI stack. Who's at I/O for the first time? Can I see some hands up? Oh, OK. Welcome to Google\nI/O. It's a pleasure to have you with us today. So we'll start by giving\nyou an overview of Google's end-to-end ecosystem of AI. And as you know, we've\nbeen leading the way in AI for decades, since we\nopen-sourced TensorFlow in 2015, from when we published our\nfield-defining research with transformers\nin 2017, to Gemini. And we are now in\nthe Gemini era. So we've been releasing a lot. Relentlessly, as it's\nbeen called today, we've been shipping many\nfeatures, many new products. And in our talk,\nwe're actually going to give you an overview\nof everything that's new for developers\nthroughout the AI stack. Our mission is to empower every\ndeveloper and organization to harness the power of AI. And Google's stack is so good\nand flexible because it combines very robust infrastructure\nwith state-of-the-art research. And all of this enables\nreal-world applications come to life that change\nentire fields, industries, and companies. We'll start by discussing\nfoundation models, touching upon our\nGemini, Gemma, and some of our domain-specific models. JOSH GORDON: After\nfoundation models, we'll take a look\nat AI frameworks that we use to build them. So we'll talk about JAX, which\nis really great for researchers. We'll talk about Keras, which is\nreally amazing for applied AI. Later on, we'll even\ntalk a little bit about the work we're\ndoing with PyTorch. JOANA CARRASQUEIRA: We'll\nalso touch upon some developer tools for all\ntypes of experience from beginners to advanced. JOSH GORDON: Then we'll talk a\nlittle bit about infrastructure. And this talk is about\nsoftware not hardware.",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_016",
        "text": "So you can imagine, if\nyou're doing something like sign language recognition\nand maybe the user is holding up their hand and they're signing,\nyou don't want to drop frames. And if you're sending those\nframes to a server on the cloud, unless you happen to have\nthe world's fastest internet connection, you're probably\ngoing to drop frames. But if you have that gesture\nrecognition model running locally, you're not going to. So that's one huge advantage. Others, of course, are privacy. Data doesn't need\nto leave the device. I mean, a lot of this offline. I know this is obvious\nto mobile folks, but if you're working\non an airplane, maybe you want to run your\nmachine learning model there. Cost savings is a really\nimportant one too. So if you're serving a model\nto lots of users on the cloud, you might be paying\nfor the compute that you need to serve it. But of course, if it's\nrunning on the phone, the compute's happening\nlocally, so you don't need to bother with\nserving infrastructure. There's a lot of really cool\nnew stuff in Google AI Edge. On our side, we've added support\nfor things like the latest Gemma models. And by the way, this is for both\nclassical machine learning-- well, \"deep learning,\" which is\nnow suddenly becoming classical. Things like gesture recognition,\nwhich were state-of-the-art four years ago, now that's classical\nML because we're talking about large language models\nand generative AI. But you can run small large\nlanguage models on device. We have a new really awesome\ncommunity with Hugging Face, and there's a lot of really\nsmart people putting together models that are ready to\nrun pre-optimized on device. And we have a private\npreview-- this is coming soon-- for AI Edge Portal, which is\nbasically a testing service. So you submit your model\nto a cloud service, and it runs it on a fleet of\nreal devices of different sizes just to verify that\nit works really well. So if you're interested\nin mobile development, check out Google AI Edge. Google AI Edge,\nit's really cool.",
        "cosine": null
      }
    ],
    "true_documents": []
  },
  {
    "query": "What new AI Studio features and Gemini API capabilities were announced, such as the Build tab that generates web apps and the new generative media experience?",
    "retrieved_doc_ids": [
      "gHHjDRDNUNU__chunk_007",
      "4TE-KFXvhAk__chunk_017",
      "4TE-KFXvhAk__chunk_009",
      "4TE-KFXvhAk__chunk_002",
      "4TE-KFXvhAk__chunk_003"
    ],
    "recall_at_5": 1,
    "per_doc_cosine": [],
    "avg_cosine": 0.0,
    "reciprocal_rank": 0.5,
    "llm_score": null,
    "retrieved_answers": [
      {
        "doc_id": "4TE-KFXvhAk__chunk_003",
        "text": "On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API and also on [INAUDIBLE] API\nfor real generating audio. And some of the use\ncases here that we had in mind when\nwe built this were more dynamic audiobooks,\nsome more engaging podcasts, or, even for those of\nyou in customer support, producing more natural\nvoices into your workflows. We also have enhanced\ntooling, which is really cool, because now you\ncan use Grounding with Google Search, also with Code Execution\ntogether in just one API call, and URL context, which you\nheard during the keynote, which provides the model with\ndepth content from web pages. And since you can chain\nit to other tools, it's actually really powerful\nto build search agents. So really cool stuff. Lastly, just to call out that\nwe now offer Gemini SDK support for MCP, which reduces a\nlot of developer friction and simplifies building\nagentic capabilities. So you don't want to\nmiss tomorrow's talk to learn more about this. And Google AI Studio. Who here uses Google AI Studio? Can I see some hands? OK, awesome. Awesome. We have a lot to chat\nafter this session then. Google AI Studio is the\nperfect place for anyone to start developing with AI. It's the simplest way to test\nthe latest DeepMind models. We typically bring them\nto Google AI Studio so you can start prototyping\nand playing with the models. And you don't need Google\nCloud knowledge in order to set up your environment. It is free of charge, and you\ncan create, and test, and save your prompts. And there's also starter\napps that will inspire you. And that's exactly what I'm\ngoing to demo here today for you. I am going to show\nyou some of the work that the team has been doing. So if we go to Google AI Studio\nhere, if we go into Build, and Mumble Jumble is\nsomething that we've literally just created, and\nit's one of my favorite apps. So Mumble Jumble is one\nof those applications that",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_002",
        "text": "it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it because of its\nefficiency and speed, and it's now even better at\nalmost every single dimension. So we improved\nall the benchmarks across reasoning, coding,\nmultimodality, and also long context. Then we have our Gemini 2.0\nFlash, which is fast and cheap, works fine, and our\nGemini Nano, which is optimized for on-device tasks. And as you've heard, we've\nbeen shipping relentlessly. And I would like to give\nyou just a quick highlight of everything that we've\nbeen shipping in AI Studio and the Gemini API. There's a talk\ntomorrow that I would like to invite you to attend,\nwhich is by Shrestha Basu Mallick, a group product\nmanager on Gemini API, and Luciano Martins, our\ntechnical lead for Gemini API from DevRel. And they're going to do a deep\ndive into everything that's new within the Gemini\nAPI, so you definitely don't want to miss\nthat session tomorrow. But for now, just a glimpse to\nget you excited about what's new in AI Studio. We've built a new tab\nthat is called Build that instantly generates web apps. And it's really cool, because it\nenables developers and builders alike to prototype very\nquickly with natural language. We have a new generative media\nexperience in AI Studio as well, and I'm going to\ndemo all of this so you can see how\nit actually works. And we are always\nlistening to the community. We listen to your\nfeedback, and we always build with developers in mind. And that's why that\nsome of these features were actually\nrequested by community, and that's what happened with\nthe built-in new dashboard. You requested. We built it. And we also have some\nnew Native Audio and TTS support in AI Studio. On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_009",
        "text": "And I would also like to\ndemo one of these new apps that we have in AI Studio. So I'm going back to\nthe laptop, and I'm going to show you something\nthat the team also just created. So AI Studio also got a facelift\nand has some new features. And the chat interface\nis still the same, but you've seen the talk to\nGemini live during the keynote. We have the new\nGenerative Media Console, which allows you to\ncreate and interact with our most creative models. And then we have\nthe Build, which is where all these new\napps are coming to. So I just wanted to show\nyou very quickly this one. There we go. And then we basically\ncan choose here what are the sounds\nthat we want. And this is all powered by\nLyria, our music generation model. [GENERATED MUSIC PLAYING] And just for the\ninterest of time, I'm not going to\nkeep playing it. But you can see some of the\ncapabilities of these models that we're bringing\nto AI Studio. We'll continue to the slides. In the Console,\nas you could see, you have access to\nour image generation, our video generation, and music\ngeneration models with applets to get you started. And so that's a really\ncool thing for you to play with after this session. Some of our videos,\nvery realistic images with a really good understanding\nof real-world physics and dynamics, improved quality,\nand more and more capabilities coming to these models. And this is the example\nthat I just showed. We've made Lyria RealTime our\ninteractive music generation model, which powers MusicFX DJ. It's available in the\nAPI and AI Studio, and you can check\nour API documentation for more information. This also allows\neveryone to interact, to create, and to perform\ngenerative music in real time. It's really cool. You might remember that in the\nshow before the first keynote, you might have\nseen this console. That's exactly why I wanted to\nshow you this particular app in this session. But there's a lot more that\nyou can try afterwards. And shifting the\ngears towards Gemma, earlier this year we\nreleased Gemma 3, which",
        "cosine": null
      },
      {
        "doc_id": "gHHjDRDNUNU__chunk_007",
        "text": "We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there. If you are a Google\nColab user, you have ways to interact with\nthe Gemini models as well. So the key idea is to\nmake it easier for you. No matter where you are having\nyour development experience, We are trying to bring\nGemini API closer to you. SHRESTHA BASU MALLICK:\nThat's correct. And I think this is\nalso a good place to make a plug for Google\nAI Studio, which of course, everyone knows about and loves. It is the place where\na lot of our developers first test out the\ncapabilities of the API before committing to\nbuilding applications at scale with the API. And Google AI Studio now\nhas code generation also. LUCIANO MARTINS: Absolutely. SHRESTHA BASU MALLICK: So this\nis a quick, high level overview of all the components that are\navailable through the Gemini API. It's pretty standard. You send in your prompt, you get\na response back from the model. If there's a tool\ncall needed, we do support a set of\nfirst party Google tools, as well as function calling. So in terms of the first party\nGoogle tools that we support, we, of course, support\nGoogle Search as a tool. The best search engine\nout there is now available as a tool for the\nmodel to call when it needs to. Now in addition to the\ninformation that you get out of Search, if you want to\nretrieve more in-depth content from web pages for\napplications such as you're building your own version\nof a research agent, so you pull some\ninformation from Search. You pull a set of\nURLs, but if you want to extract a little\nmore in depth content, we just released a new tool\nat I/O called URL Context. And then, of course, you can\ntake all that information. And if you want to create\nbeautiful charts out of it or run some analysis,\nwe make code execution available as a tool to\nyou via the API as well. Then, of course, you\nhave function calling.",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_017",
        "text": "and it runs it on a fleet of\nreal devices of different sizes just to verify that\nit works really well. So if you're interested\nin mobile development, check out Google AI Edge. Google AI Edge,\nit's really cool. And with that, I'll\nhand it over to Joana to talk about what's next. JOANA CARRASQUEIRA: Awesome. Thank you, Josh. And you've heard it in the\nkeynotes in the previous session with Demis and Sergey,\nwe're pushing the boundaries of what's possible to build with\nAI here at Google and Google DeepMind. And we're really excited to\nbring all this innovation and put it in the\nhands of developers, in the hands of the community. And it's never been a better\ntime to build and co-create together. So we really believe\nin a future where AI is changing various fields\nacross scientific discovery, health care, and so many more. And we're going to achieve\nthis radical abundance in a safe and\nresponsible way, and we want to get there with\nyou, with the community. So let's have a look\nat some of the domains that we believe that have a\nhuge potential for developers and humanity at scale. AlphaEvolve, a\nGemini-powered coding agent for designing\nadvanced algorithms. A self-improving coding agent. And we all know that\nlarge language models can summarize documents. They can generate code. You can even\nbrainstorm with them. But with AlphaEvolve,\nwe're really expanding these\ncapabilities, and we are targeting fundamental\nand highly complex problems on\nmathematics and coding. AlphaEvolve leverages\nGemini Flash and Pro, and it's one of the big\npromises for the future. Another one-- and I'm really\nexcited about AI co-scientist. It's another\nscientific breakthrough that we're seeing, especially\nin the medical, in medicine, and research fields. And our goal is to accelerate\nthe speed up discovery and drug development. And with AI\nco-scientist, a scientist can give a research goal to\nthe agent in natural language, and then the AI\nco-scientist is designed to give you an overview, and a\nhypothesis, and a methodology.",
        "cosine": null
      }
    ],
    "true_documents": []
  },
  {
    "query": "What updates were shared about the Gemini model lineup (2.5 Flash, 2.0 Flash, Nano) and the new text-to-speech features, including emotion and style control in the API?",
    "retrieved_doc_ids": [
      "gHHjDRDNUNU__chunk_001",
      "gHHjDRDNUNU__chunk_022",
      "4TE-KFXvhAk__chunk_002",
      "gHHjDRDNUNU__chunk_002",
      "4TE-KFXvhAk__chunk_003"
    ],
    "recall_at_5": 1,
    "per_doc_cosine": [],
    "avg_cosine": 0.0,
    "reciprocal_rank": 0.25,
    "llm_score": null,
    "retrieved_answers": [
      {
        "doc_id": "gHHjDRDNUNU__chunk_022",
        "text": "more of the other features and\npossibilities with the models. And the last one is the Gemini\nCookbook, a GitHub repository that our team curates and keep\nupdating with a lot of sample experience for you. Thank you so much, Shrestha. Thank you so much you all. SHRESTHA BASU MALLICK: Thank you\nall for coming out to hear us. [CHEERING] [MUSIC PLAYING]",
        "cosine": null
      },
      {
        "doc_id": "gHHjDRDNUNU__chunk_002",
        "text": "when we think about the key\nstandard benchmarks, Shrestha? SHRESTHA BASU\nMALLICK: Yeah, we'll go to the benchmarks\nin a second. But the message\nthat I want everyone to take away from this\nslide is whether it's a super complex task, or whether\nit's on-device processing, you now have a Gemini model\nthat you can use for it. And it's pretty powerful. And it's pretty price effective. LUCIANO MARTINS: Yeah,\nmaybe it is also worth mentioning, Shrestha,\nthat some folks here may be developing with\nGemini since it was released. SHRESTHA BASU MALLICK: Yeah. LUCIANO MARTINS: So using the\nAI Studio, using the Gemini API, the older versions\nof Gemini like 2.0, 1.5 are still available. But we really\nencourage everybody to start experimenting\nwith the newer ones with better capabilities\nand better performance. SHRESTHA BASU MALLICK:\nThat's correct. So 2.0 Flash continues to be one\nof our most widely-used models, just again, because of how good\nit is for the price it's at. And we have a lot of people\nwho are still using it. And then we also have some\npeople on the 1.5 models. But we're encouraging\npeople now, we're two generations\nahead, to start using the 2.5 series of\nmodels and give us feedback. LUCIANO MARTINS: Nice. So please tell us a little bit\nabout the benchmarks, Shrestha. SHRESTHA BASU MALLICK:\nYeah, next slide. Yeah, so we're not going\nto spend a lot of time on the benchmarks. But the two key points\nthat I'd like to hit here is number one, is one\nway in which we measure how good these models are is\nby putting them on LMArena, which is where developers\nand builders like yourself test out\nall these models and give it an ELO rating. And we're, of course, very\nproud that right now you have three Gemini models in\nthe top 10 in the LMArena. LUCIANO MARTINS:\nIncluding the top one. SHRESTHA BASU\nMALLICK: Including, how did I forget that? We are number one guys,\nand we're very proud of it. [CHEERING] Thank you. One of the things,\none of the use cases that has been really emerging\nin the last couple of months",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_003",
        "text": "On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API and also on [INAUDIBLE] API\nfor real generating audio. And some of the use\ncases here that we had in mind when\nwe built this were more dynamic audiobooks,\nsome more engaging podcasts, or, even for those of\nyou in customer support, producing more natural\nvoices into your workflows. We also have enhanced\ntooling, which is really cool, because now you\ncan use Grounding with Google Search, also with Code Execution\ntogether in just one API call, and URL context, which you\nheard during the keynote, which provides the model with\ndepth content from web pages. And since you can chain\nit to other tools, it's actually really powerful\nto build search agents. So really cool stuff. Lastly, just to call out that\nwe now offer Gemini SDK support for MCP, which reduces a\nlot of developer friction and simplifies building\nagentic capabilities. So you don't want to\nmiss tomorrow's talk to learn more about this. And Google AI Studio. Who here uses Google AI Studio? Can I see some hands? OK, awesome. Awesome. We have a lot to chat\nafter this session then. Google AI Studio is the\nperfect place for anyone to start developing with AI. It's the simplest way to test\nthe latest DeepMind models. We typically bring them\nto Google AI Studio so you can start prototyping\nand playing with the models. And you don't need Google\nCloud knowledge in order to set up your environment. It is free of charge, and you\ncan create, and test, and save your prompts. And there's also starter\napps that will inspire you. And that's exactly what I'm\ngoing to demo here today for you. I am going to show\nyou some of the work that the team has been doing. So if we go to Google AI Studio\nhere, if we go into Build, and Mumble Jumble is\nsomething that we've literally just created, and\nit's one of my favorite apps. So Mumble Jumble is one\nof those applications that",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_002",
        "text": "it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it because of its\nefficiency and speed, and it's now even better at\nalmost every single dimension. So we improved\nall the benchmarks across reasoning, coding,\nmultimodality, and also long context. Then we have our Gemini 2.0\nFlash, which is fast and cheap, works fine, and our\nGemini Nano, which is optimized for on-device tasks. And as you've heard, we've\nbeen shipping relentlessly. And I would like to give\nyou just a quick highlight of everything that we've\nbeen shipping in AI Studio and the Gemini API. There's a talk\ntomorrow that I would like to invite you to attend,\nwhich is by Shrestha Basu Mallick, a group product\nmanager on Gemini API, and Luciano Martins, our\ntechnical lead for Gemini API from DevRel. And they're going to do a deep\ndive into everything that's new within the Gemini\nAPI, so you definitely don't want to miss\nthat session tomorrow. But for now, just a glimpse to\nget you excited about what's new in AI Studio. We've built a new tab\nthat is called Build that instantly generates web apps. And it's really cool, because it\nenables developers and builders alike to prototype very\nquickly with natural language. We have a new generative media\nexperience in AI Studio as well, and I'm going to\ndemo all of this so you can see how\nit actually works. And we are always\nlistening to the community. We listen to your\nfeedback, and we always build with developers in mind. And that's why that\nsome of these features were actually\nrequested by community, and that's what happened with\nthe built-in new dashboard. You requested. We built it. And we also have some\nnew Native Audio and TTS support in AI Studio. On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API",
        "cosine": null
      },
      {
        "doc_id": "gHHjDRDNUNU__chunk_001",
        "text": "And as part of that,\nwe have a section where we do a deep dive on\nagentic capabilities in the API. LUCIANO MARTINS:\nThanks for the add. SHRESTHA BASU MALLICK: Yeah. So starting with what are\nthe families of models we have available? So the most performant and\npowerful model that we have is the Gemini 2.5 Pro. It's in preview right now. A few weeks ago, we released\na updated version of 2.5 Pro. And it's suitable for highly\ncomplex tasks that require a lot of deep reasoning. And coding has been, of course,\na standout use case of that. We also have 2.5 Flash,\nwhich this Google I/O we released a new preview\nversion just yesterday for 2.5 Flash. And 2.5 Flash is\nour model size that has probably one of the\nbest price performance ratios in the market today. LUCIANO MARTINS: That's right. SHRESTHA BASU MALLICK: We\nalso have 2.0 Flash-Lite. So this is a small,\nfast, cheap model that you can use for high\nvolume tasks like summarization. And then Luciano,\ndo you want to talk about the Nano and\nthe Embedding models? LUCIANO MARTINS:\nYeah, absolutely. We heard a lot of\nfeedback from you folks. And many of you are\ndeveloping mobile applications or applications that\nmust run on devices. So then you have\nalso available what we call the Gemini\nNano, which is a smaller version of the Gemini, which is\nable to run locally on devices like on Android devices if you\nare developing with the Android Studio using the AI Corps. And also, many of you are\ntrying to create applications that do some kind\nof semantic ranking or to organize information\nin large scale. So you have also one model that\nwe call the Gemini Embedding, which the key objective of this\nmodel is to let you ingest text. And then the model delivers you\nhigh quality multidimensional embeddings. But how those\nmodels are behaving when we think about the key\nstandard benchmarks, Shrestha? SHRESTHA BASU\nMALLICK: Yeah, we'll go to the benchmarks\nin a second. But the message\nthat I want everyone to take away from this\nslide is whether it's",
        "cosine": null
      }
    ],
    "true_documents": []
  },
  {
    "query": "What are the new Gemini API capabilities for text-to-speech, including controlling emotion and style, and how can I use Grounding with Google Search and Code Execution in a single API call?",
    "retrieved_doc_ids": [
      "gHHjDRDNUNU__chunk_007",
      "gHHjDRDNUNU__chunk_005",
      "gHHjDRDNUNU__chunk_013",
      "4TE-KFXvhAk__chunk_003",
      "4TE-KFXvhAk__chunk_007"
    ],
    "recall_at_5": 1,
    "per_doc_cosine": [],
    "avg_cosine": 0.0,
    "reciprocal_rank": 1.0,
    "llm_score": null,
    "retrieved_answers": [
      {
        "doc_id": "4TE-KFXvhAk__chunk_003",
        "text": "On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API and also on [INAUDIBLE] API\nfor real generating audio. And some of the use\ncases here that we had in mind when\nwe built this were more dynamic audiobooks,\nsome more engaging podcasts, or, even for those of\nyou in customer support, producing more natural\nvoices into your workflows. We also have enhanced\ntooling, which is really cool, because now you\ncan use Grounding with Google Search, also with Code Execution\ntogether in just one API call, and URL context, which you\nheard during the keynote, which provides the model with\ndepth content from web pages. And since you can chain\nit to other tools, it's actually really powerful\nto build search agents. So really cool stuff. Lastly, just to call out that\nwe now offer Gemini SDK support for MCP, which reduces a\nlot of developer friction and simplifies building\nagentic capabilities. So you don't want to\nmiss tomorrow's talk to learn more about this. And Google AI Studio. Who here uses Google AI Studio? Can I see some hands? OK, awesome. Awesome. We have a lot to chat\nafter this session then. Google AI Studio is the\nperfect place for anyone to start developing with AI. It's the simplest way to test\nthe latest DeepMind models. We typically bring them\nto Google AI Studio so you can start prototyping\nand playing with the models. And you don't need Google\nCloud knowledge in order to set up your environment. It is free of charge, and you\ncan create, and test, and save your prompts. And there's also starter\napps that will inspire you. And that's exactly what I'm\ngoing to demo here today for you. I am going to show\nyou some of the work that the team has been doing. So if we go to Google AI Studio\nhere, if we go into Build, and Mumble Jumble is\nsomething that we've literally just created, and\nit's one of my favorite apps. So Mumble Jumble is one\nof those applications that",
        "cosine": null
      },
      {
        "doc_id": "gHHjDRDNUNU__chunk_007",
        "text": "We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there. If you are a Google\nColab user, you have ways to interact with\nthe Gemini models as well. So the key idea is to\nmake it easier for you. No matter where you are having\nyour development experience, We are trying to bring\nGemini API closer to you. SHRESTHA BASU MALLICK:\nThat's correct. And I think this is\nalso a good place to make a plug for Google\nAI Studio, which of course, everyone knows about and loves. It is the place where\na lot of our developers first test out the\ncapabilities of the API before committing to\nbuilding applications at scale with the API. And Google AI Studio now\nhas code generation also. LUCIANO MARTINS: Absolutely. SHRESTHA BASU MALLICK: So this\nis a quick, high level overview of all the components that are\navailable through the Gemini API. It's pretty standard. You send in your prompt, you get\na response back from the model. If there's a tool\ncall needed, we do support a set of\nfirst party Google tools, as well as function calling. So in terms of the first party\nGoogle tools that we support, we, of course, support\nGoogle Search as a tool. The best search engine\nout there is now available as a tool for the\nmodel to call when it needs to. Now in addition to the\ninformation that you get out of Search, if you want to\nretrieve more in-depth content from web pages for\napplications such as you're building your own version\nof a research agent, so you pull some\ninformation from Search. You pull a set of\nURLs, but if you want to extract a little\nmore in depth content, we just released a new tool\nat I/O called URL Context. And then, of course, you can\ntake all that information. And if you want to create\nbeautiful charts out of it or run some analysis,\nwe make code execution available as a tool to\nyou via the API as well. Then, of course, you\nhave function calling.",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_007",
        "text": "So it's just really\nquick and easy to do. OK, we can go back\nto the slides. So very, very\nquickly, as a recap, Gemini Developer API is the\neasiest way to get started. It's super lightweight,\nit's fast to install, and you can get up and running\nhonestly in about a minute. OK? I will use the clicker. This is the flow to get\nstarted with Google AI Studio. Go to Google AI Studio. Get your key. Try one of the code\nexamples on ai.google.dev or in the cookbook. I see people taking pictures. That makes me happy. Please try this. We spent so much time\non making this easy, and I hope it works for you. If not, please file an\nissue and we'll get on it. This is the Gen AI SDK\nfor the Gemini API, and this is something\nwe've been rolling out. It's our latest SDK. We've been rolling\nit out gradually over the course of\nthe last six months. It's super user friendly. It's really easy to use. Really, the only point\nI want to make here, because I don't want\nto read the code examples or the\ndocumentation to you, you can call the API\nin a few lines of code. Basically add your key, select\na model, write a prompt, you can go ahead and call it. You can also get access\nto advanced functionality in one line of code. So if you'd like to get the\nthinking summaries that Joana showed you, you can just\nadd a ThinkingConfig, say include the thoughts. Now, you've got the\nthinking summaries. And a good use\ncase for this could be any time you need to\nexplain the model's reasoning. Particularly, you can\nimagine, if you're building an education\napp or a tutoring app, you can get the\nthinking summaries. In addition to\nreally cool things that you can do with\na single line of code, there's some more advanced stuff\nthat you can do with the SDK as well. So I know there's a lot\nof code on this slide, but we've talked a lot\nabout building agents and agentic experiences. In this example,\nyou could imagine that you have a Python\nfunction on your laptop called weather_function. And maybe that calls\nyour own weather server to get the weather.",
        "cosine": null
      },
      {
        "doc_id": "gHHjDRDNUNU__chunk_013",
        "text": "And this, of course, native\naudio output, as I already mentioned, gives you\nmuch more natural sounding voices\nand the ability-- you don't now have to\nspecify your language. You can seamlessly switch\nbetween languages and flow in and out. We support tool chaining\nin the Live API. We have supported\nthis since December. So all of the tools that\nI mentioned, Search, Code Execution, URL\ncontext, function calling, you can layer these\ntools in the same prompt and get much more\ncompelling results. Get data from Search, do some\nanalysis, get the output. We have voice\nactivity detection. Of course, we need\nit in the Live API. What we now provide for\nyou, though, is the ability to configure the\nthresholds, how much of time do you want after\nthe end of speech to decide that the\nuser's speech has ended. That's one of four\nthresholds that you can now set with the Live API. You can also disable our\nvoice activity detection model and bring your own. Session management. We have a lot of\nparameters out there. So in its most basic state,\nthe Live API currently supports about 20\nminutes of audio and about a few\nminutes of video. But we now have\nvarious techniques for you to increase\nyour session length, including sliding window, the\nability to change resolution on what video is\npassed, the ability to decide, do you want audio\nto be streamed only when-- do you want video to be streamed\nonly when audio is being spoken, or even when audio\nis not being spoken, and other parameters that you\ncan use through the Live API. Ephemeral tokens\nare coming soon, but that's one way to do\nauthorization into the Live API. And finally, with the native\naudio output, specifically the audio-to-audio\narchitecture, we are also releasing a couple\nof modes for you to try out. One of them is proactive audio. What this feature\ndoes today is it lets the AI decide\nwhen to respond to you and when whatever the human\nis saying is irrelevant. So imagine if I'm having a\nconversation with the AI, and Luciano comes to me and\nsays something unrelated.",
        "cosine": null
      },
      {
        "doc_id": "gHHjDRDNUNU__chunk_005",
        "text": "LUCIANO MARTINS: And I think it\nis not the only thing related to audio. We brought that to the live\ninteractions as well, right? SHRESTHA BASU\nMALLICK: That's right. So the TTS models,\nlike Luciano said, are available via chat endpoint. And then on our Live API,\nyesterday, we also rolled out the native audio output models. So this is now\nGoogle's first release of an audio-to-audio\narchitecture. And there are two variants of\nthis model that you have access through the Gemini Live API,\nwhich is our real time API. So you have the Native\nAudio Dialogue model, which is the real benefit of\nthe native audio output models are that the voices\nthat come out of it are much more natural\nand much more compelling. The native audio-to-audio\narchitecture has better contextual\nunderstanding of what humans say to the AI. It can seamlessly transition\nbetween different languages, so maybe from Brazilian\nPortuguese to Bengali, if you want to switch\nin the same language, in the same sentence. And it's available on\nour Gemini real time API. We also have a version\nof this model available, again on the real time\nAPI with thinking enabled. So for more complex use\ncases, so one use case that comes to mind is let's say\nyou're building a gaming agent to play a strategy game. And you want that agent to be\nable to think, but also to talk to you with relatively\nlow latency. We have a thinking version\nof the native audio model that's also now available\nfrom the Live API. So please try it out\nand give us feedback. LUCIANO MARTINS: Perfect. So how people can start\nusing those models, Shrestha? SHRESTHA BASU MALLICK:\nThrough the API. Of course. But before we go there,\nthere are two other things that we should mention. It's hot off the news. You've probably\nall heard about it. We've also enabled\nan advanced reasoning mode called Deep Think. And the idea behind Deep\nThink is that the model 2.5 Pro can think through various\npossible answers to a problem before giving you\nthe best answer. So that is available\nin trusted testers,",
        "cosine": null
      }
    ],
    "true_documents": []
  },
  {
    "query": "How does Google AI Studio help developers test DeepMind models without Google Cloud knowledge, and what starter apps (like Mumble Jumble) are available to prototype with?",
    "retrieved_doc_ids": [
      "gHHjDRDNUNU__chunk_007",
      "4TE-KFXvhAk__chunk_017",
      "4TE-KFXvhAk__chunk_001",
      "4TE-KFXvhAk__chunk_019",
      "4TE-KFXvhAk__chunk_006"
    ],
    "recall_at_5": 0,
    "per_doc_cosine": [],
    "avg_cosine": 0.0,
    "reciprocal_rank": 0.0,
    "llm_score": null,
    "retrieved_answers": [
      {
        "doc_id": "4TE-KFXvhAk__chunk_006",
        "text": "there's information about\nthe models, everything you need to get started. We also have the\nGemini API Cookbook. We have a link to\nthis at the end. It's basically goo.gle/cookbook. G-O-O.G-L-E/cookbook. And this will take you to\na whole slew of notebooks that the team has put together. And basically, all\nof these notebooks are end-to-end examples\nthat show you one thing that you might be interested\nin, like what's the best way to do code execution? What's the best way to\ndo function calling? You'll find that\nin the cookbook. I also very, very quickly\nwant to show you how easy it is to get started with the API. So basically, in\nGoogle AI Studio, you don't need a credit\ncard or anything like that, in about a minute you can\njust click Get API key. Create your key. Now, if you're doing\nthis for the first time, behind the scenes,\nthis will automatically create a Cloud project\nfor you, but that detail is not important. Basically, now I have an API key\nand I'm ready to install the SDK and call the model. If you open up any of the\nnotebooks in the cookbook, well, let's just say-- it's in a different\ndirectory here, but let's just say\nwe've opened up-- eh, we'll just say we\nopened up this one, which is in the quickstarts directory. And this shows you\nexactly what Joana showed, how to get the\nthinking summaries. You can add your API\nkey in Google Colab. If you zoom in, you\ncan hit Add new secret. And in this particular notebook,\nit's called Google API key. But you could call\nit whatever you like. So you would add\nGoogle API key there, you would paste your\nkey there, and now you're ready to run this. So if you do Runtime and Run\nall, you're calling the API and you're running\nall the examples. You can also, directly\nin Google Colab, we have this thing where you\ncan grab an API key straight inside Google Colab. So it's just really\nquick and easy to do. OK, we can go back\nto the slides. So very, very\nquickly, as a recap, Gemini Developer API is the\neasiest way to get started. It's super lightweight,\nit's fast to install,",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_017",
        "text": "and it runs it on a fleet of\nreal devices of different sizes just to verify that\nit works really well. So if you're interested\nin mobile development, check out Google AI Edge. Google AI Edge,\nit's really cool. And with that, I'll\nhand it over to Joana to talk about what's next. JOANA CARRASQUEIRA: Awesome. Thank you, Josh. And you've heard it in the\nkeynotes in the previous session with Demis and Sergey,\nwe're pushing the boundaries of what's possible to build with\nAI here at Google and Google DeepMind. And we're really excited to\nbring all this innovation and put it in the\nhands of developers, in the hands of the community. And it's never been a better\ntime to build and co-create together. So we really believe\nin a future where AI is changing various fields\nacross scientific discovery, health care, and so many more. And we're going to achieve\nthis radical abundance in a safe and\nresponsible way, and we want to get there with\nyou, with the community. So let's have a look\nat some of the domains that we believe that have a\nhuge potential for developers and humanity at scale. AlphaEvolve, a\nGemini-powered coding agent for designing\nadvanced algorithms. A self-improving coding agent. And we all know that\nlarge language models can summarize documents. They can generate code. You can even\nbrainstorm with them. But with AlphaEvolve,\nwe're really expanding these\ncapabilities, and we are targeting fundamental\nand highly complex problems on\nmathematics and coding. AlphaEvolve leverages\nGemini Flash and Pro, and it's one of the big\npromises for the future. Another one-- and I'm really\nexcited about AI co-scientist. It's another\nscientific breakthrough that we're seeing, especially\nin the medical, in medicine, and research fields. And our goal is to accelerate\nthe speed up discovery and drug development. And with AI\nco-scientist, a scientist can give a research goal to\nthe agent in natural language, and then the AI\nco-scientist is designed to give you an overview, and a\nhypothesis, and a methodology.",
        "cosine": null
      },
      {
        "doc_id": "gHHjDRDNUNU__chunk_007",
        "text": "We launched a Java during\nI/O, right, Shrestha? And also, you have\nthe ability to use the API on other developer\ntools you may be using. For example, if you\nuse Firebase Studio, the API is available there. If you are a Google\nColab user, you have ways to interact with\nthe Gemini models as well. So the key idea is to\nmake it easier for you. No matter where you are having\nyour development experience, We are trying to bring\nGemini API closer to you. SHRESTHA BASU MALLICK:\nThat's correct. And I think this is\nalso a good place to make a plug for Google\nAI Studio, which of course, everyone knows about and loves. It is the place where\na lot of our developers first test out the\ncapabilities of the API before committing to\nbuilding applications at scale with the API. And Google AI Studio now\nhas code generation also. LUCIANO MARTINS: Absolutely. SHRESTHA BASU MALLICK: So this\nis a quick, high level overview of all the components that are\navailable through the Gemini API. It's pretty standard. You send in your prompt, you get\na response back from the model. If there's a tool\ncall needed, we do support a set of\nfirst party Google tools, as well as function calling. So in terms of the first party\nGoogle tools that we support, we, of course, support\nGoogle Search as a tool. The best search engine\nout there is now available as a tool for the\nmodel to call when it needs to. Now in addition to the\ninformation that you get out of Search, if you want to\nretrieve more in-depth content from web pages for\napplications such as you're building your own version\nof a research agent, so you pull some\ninformation from Search. You pull a set of\nURLs, but if you want to extract a little\nmore in depth content, we just released a new tool\nat I/O called URL Context. And then, of course, you can\ntake all that information. And if you want to create\nbeautiful charts out of it or run some analysis,\nwe make code execution available as a tool to\nyou via the API as well. Then, of course, you\nhave function calling.",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_019",
        "text": "So in order to learn\nmore, Josh, what do our developers have to do? JOSH GORDON: We have\njust a few links for you. So no problem. But we talked about a lot of\ndifferent tools in the stack. JOANA CARRASQUEIRA: We did. JOSH GORDON: So I don't\nwant to read the slide, but let me just point you\nto a couple highlights. ai.google.dev is the\nbest place to go to get started with Gemini and Gemma. We have a cookbook for Gemini. We have a cookbook for Gemma. Google AI Studio is\naistudio.google.com. If you're interested in JAX\nand Keras, there are the links. If you happen to be\ninterested in XLA, please check it\nout-- openxla.org. Google AI Edge is\nat the very bottom if you're a mobile\ndeveloper and you're interested in mobile deployment. And just to be clear, there's\nso many amazing things in the Google AI stack we didn't\nhave time to talk about today. Vertex has really amazing tools\nfor enterprise developers. But please, start\nhere, have fun, and, yeah, we're\naround after the talk. JOANA CARRASQUEIRA:\nYes, absolutely. And the Developer Relations\nteam is just outside. We have some really\ncool demo stations that you can experience. Engage with the team. Check out the sessions\ntomorrow, especially on the Gemini API,\nGemmaverse, and robotics. We have a lot of\ncool stuff that we want to put in the\nhands of developers. Many early access\nprograms as well. Stay in touch, stay\nengaged, and let's co-create the future of AI together. Thank you so much. JOSH GORDON: Thanks a lot. Thanks [APPLAUSE] [MUSIC PLAYING]",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_001",
        "text": "JOANA CARRASQUEIRA: We'll\nalso touch upon some developer tools for all\ntypes of experience from beginners to advanced. JOSH GORDON: Then we'll talk a\nlittle bit about infrastructure. And this talk is about\nsoftware not hardware. Our hardware infrastructure\nis TPUs, which you've probably heard a lot of. But in this talk,\nI'll briefly talk about XLA, which is a\nmachine learning compiler. And I'll talk about\nsome of the work we're doing for\ninference-- so making it possible to serve\nmodels at scale super efficiently with really\ncool new things with XLA for JAX and PyTorch. JOANA CARRASQUEIRA: OK. JOSH GORDON: Oh, and then\none more thing to mention. I went too fast. So sorry about that. So a lot of this talk is about\nthese huge foundation models. Towards the end of the talk,\nI'll talk about Google AI Edge. And we'll talk about\ndeploying small models on device, which is also super\nimportant for many reasons. JOANA CARRASQUEIRA: Awesome. OK. Let's start by exploring\nour core intelligence within our stack. We'll start with\nour Gemini models, which are our most capable\nand versatile model family. And our core philosophy here at\nGoogle is to provide developers with state-of-the-art models and\ntools that you can use to build powerful applications\nall throughout. And our Gemini models, they\nare known for being multimodal, have a long context window, and\nhaving very powerful reasoning, but we've built a variety of\nmodels for different use cases. So depending on what\nyou're trying to build, Google will have a model that\nis tailored for your use case. And I would like to just\ngive you a quick walkthrough of these models. I know you've heard\nit during the keynote, but, just very quickly,\nGemini 2.5 Pro, which is our most advanced\nmodel yet, especially for high complex tasks that\nbenefit from deep reasoning, it's really good at coding\nand also more complex prompts. It leads coding\nbenchmarks including WebDev Arena leaderboard,\nand it's really our most powerful model. Gemini 2.5 Flash, which\ndevelopers love it",
        "cosine": null
      }
    ],
    "true_documents": []
  },
  {
    "query": "What is the Mumble Jumble app in Google AI Studio and how can you customize its voice and interruptibility settings?",
    "retrieved_doc_ids": [
      "4TE-KFXvhAk__chunk_004",
      "4TE-KFXvhAk__chunk_009",
      "gHHjDRDNUNU__chunk_013",
      "gHHjDRDNUNU__chunk_014",
      "4TE-KFXvhAk__chunk_003"
    ],
    "recall_at_5": 1,
    "per_doc_cosine": [],
    "avg_cosine": 0.0,
    "reciprocal_rank": 1.0,
    "llm_score": null,
    "retrieved_answers": [
      {
        "doc_id": "4TE-KFXvhAk__chunk_004",
        "text": "So if we go to Google AI Studio\nhere, if we go into Build, and Mumble Jumble is\nsomething that we've literally just created, and\nit's one of my favorite apps. So Mumble Jumble is one\nof those applications that with just natural\nlanguage allows you to interact with\nthe model so you can build those more dynamic and\ninteractive audio experiences. So let's have a look. So, first of all, it uses 2.5\npreview native audio dialog. You can choose if it's easy to\ninterrupt or hard to interrupt. And then you can come and\npick your voice, hi pitch. So there's multiple things\nthat you can play with and then you can customize. I really like the fox, so let's\ngo ahead and pick the fox. Let's get the cowboy. Happy because we're\nat Google I/O. We're all super\nhappy to be here. And let's get a microphone,\nbecause I have one. AI FOX: Howdy, partner. Finley Flicker Fox here ready\nto charm the boots off you. JOANA CARRASQUEIRA: Hello. What's the weather like\ntoday in Mountain View? AI FOX: Well now, it's a fine\nday in Mountain View, sunny and quite pleasant. Perfect for moseying\non out and about. JOANA CARRASQUEIRA: I agree. Are there any hiking trails\nnear the Google campus? AI FOX: Why sure as shooting. There are plenty of trails\nnear the Google campus, perfect for a grand\nadventure and some fresh air. JOANA CARRASQUEIRA:\n[SPEAKING PORTUGUESE] AI FOX: Well now, the best\nway to get to those trails is to head towards the foothills\nfollowing the marked paths. You'll find them\nright along the way. JOANA CARRASQUEIRA:\n[SPEAKING PORTUGUESE] AI FOX: You betcha. There are a few\ncozy cafes nearby, perfect for a spell\nof rest and a good cup of coffee after your hike. JOANA CARRASQUEIRA:\nThank you so much. AI FOX: It was my\npleasure, partner. Happy trails, and\nenjoy your coffee. JOANA CARRASQUEIRA: Thank you. So, as you could see, there's\nsome really cool experiences that we're bringing\ninto AI Studio. Audio is getting better; more\nnatural experiences with voice. And in case you\ndidn't notice, I even",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_003",
        "text": "On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API and also on [INAUDIBLE] API\nfor real generating audio. And some of the use\ncases here that we had in mind when\nwe built this were more dynamic audiobooks,\nsome more engaging podcasts, or, even for those of\nyou in customer support, producing more natural\nvoices into your workflows. We also have enhanced\ntooling, which is really cool, because now you\ncan use Grounding with Google Search, also with Code Execution\ntogether in just one API call, and URL context, which you\nheard during the keynote, which provides the model with\ndepth content from web pages. And since you can chain\nit to other tools, it's actually really powerful\nto build search agents. So really cool stuff. Lastly, just to call out that\nwe now offer Gemini SDK support for MCP, which reduces a\nlot of developer friction and simplifies building\nagentic capabilities. So you don't want to\nmiss tomorrow's talk to learn more about this. And Google AI Studio. Who here uses Google AI Studio? Can I see some hands? OK, awesome. Awesome. We have a lot to chat\nafter this session then. Google AI Studio is the\nperfect place for anyone to start developing with AI. It's the simplest way to test\nthe latest DeepMind models. We typically bring them\nto Google AI Studio so you can start prototyping\nand playing with the models. And you don't need Google\nCloud knowledge in order to set up your environment. It is free of charge, and you\ncan create, and test, and save your prompts. And there's also starter\napps that will inspire you. And that's exactly what I'm\ngoing to demo here today for you. I am going to show\nyou some of the work that the team has been doing. So if we go to Google AI Studio\nhere, if we go into Build, and Mumble Jumble is\nsomething that we've literally just created, and\nit's one of my favorite apps. So Mumble Jumble is one\nof those applications that",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_009",
        "text": "And I would also like to\ndemo one of these new apps that we have in AI Studio. So I'm going back to\nthe laptop, and I'm going to show you something\nthat the team also just created. So AI Studio also got a facelift\nand has some new features. And the chat interface\nis still the same, but you've seen the talk to\nGemini live during the keynote. We have the new\nGenerative Media Console, which allows you to\ncreate and interact with our most creative models. And then we have\nthe Build, which is where all these new\napps are coming to. So I just wanted to show\nyou very quickly this one. There we go. And then we basically\ncan choose here what are the sounds\nthat we want. And this is all powered by\nLyria, our music generation model. [GENERATED MUSIC PLAYING] And just for the\ninterest of time, I'm not going to\nkeep playing it. But you can see some of the\ncapabilities of these models that we're bringing\nto AI Studio. We'll continue to the slides. In the Console,\nas you could see, you have access to\nour image generation, our video generation, and music\ngeneration models with applets to get you started. And so that's a really\ncool thing for you to play with after this session. Some of our videos,\nvery realistic images with a really good understanding\nof real-world physics and dynamics, improved quality,\nand more and more capabilities coming to these models. And this is the example\nthat I just showed. We've made Lyria RealTime our\ninteractive music generation model, which powers MusicFX DJ. It's available in the\nAPI and AI Studio, and you can check\nour API documentation for more information. This also allows\neveryone to interact, to create, and to perform\ngenerative music in real time. It's really cool. You might remember that in the\nshow before the first keynote, you might have\nseen this console. That's exactly why I wanted to\nshow you this particular app in this session. But there's a lot more that\nyou can try afterwards. And shifting the\ngears towards Gemma, earlier this year we\nreleased Gemma 3, which",
        "cosine": null
      },
      {
        "doc_id": "gHHjDRDNUNU__chunk_013",
        "text": "And this, of course, native\naudio output, as I already mentioned, gives you\nmuch more natural sounding voices\nand the ability-- you don't now have to\nspecify your language. You can seamlessly switch\nbetween languages and flow in and out. We support tool chaining\nin the Live API. We have supported\nthis since December. So all of the tools that\nI mentioned, Search, Code Execution, URL\ncontext, function calling, you can layer these\ntools in the same prompt and get much more\ncompelling results. Get data from Search, do some\nanalysis, get the output. We have voice\nactivity detection. Of course, we need\nit in the Live API. What we now provide for\nyou, though, is the ability to configure the\nthresholds, how much of time do you want after\nthe end of speech to decide that the\nuser's speech has ended. That's one of four\nthresholds that you can now set with the Live API. You can also disable our\nvoice activity detection model and bring your own. Session management. We have a lot of\nparameters out there. So in its most basic state,\nthe Live API currently supports about 20\nminutes of audio and about a few\nminutes of video. But we now have\nvarious techniques for you to increase\nyour session length, including sliding window, the\nability to change resolution on what video is\npassed, the ability to decide, do you want audio\nto be streamed only when-- do you want video to be streamed\nonly when audio is being spoken, or even when audio\nis not being spoken, and other parameters that you\ncan use through the Live API. Ephemeral tokens\nare coming soon, but that's one way to do\nauthorization into the Live API. And finally, with the native\naudio output, specifically the audio-to-audio\narchitecture, we are also releasing a couple\nof modes for you to try out. One of them is proactive audio. What this feature\ndoes today is it lets the AI decide\nwhen to respond to you and when whatever the human\nis saying is irrelevant. So imagine if I'm having a\nconversation with the AI, and Luciano comes to me and\nsays something unrelated.",
        "cosine": null
      },
      {
        "doc_id": "gHHjDRDNUNU__chunk_014",
        "text": "What this feature\ndoes today is it lets the AI decide\nwhen to respond to you and when whatever the human\nis saying is irrelevant. So imagine if I'm having a\nconversation with the AI, and Luciano comes to me and\nsays something unrelated. The AI will know not to\nrespond to that audio output, so the AI proactively\ndecides not to respond. So we are calling\nit proactive audio, because we aim to\nbring much more proactivity to this feature. And then effective\ndialogue lets you pick up on the user's tone\nand sentiment and lets the AI respond appropriately. As I mentioned, you also\nhave thinking available with the Live API. All right. Time for agents. LUCIANO MARTINS: Excellent. So how many of you are\ntrying to experiment solving your computational\nproblems using agents or multi-agent solutions? Yay, everybody. SHRESTHA BASU MALLICK: All\nthe Brazilian contingent. LUCIANO MARTINS: OK. So with that in\nmind, we always try to develop the new tools and the\nnew capabilities of the model thinking how you\nfolks can use them in your projects to make agents\nbetter and more trustful. Right, Shrestha? SHRESTHA BASU MALLICK:\nThat's correct. LUCIANO MARTINS: So\nif you start thinking about how our regular\nagents architecture work, what do you have\nthere, Shrestha? So basically, we have\nthree main blocks. SHRESTHA BASU MALLICK: Yes. LUCIANO MARTINS: We normally\nsee one orchestration layer, one models layer,\nand one tools layer. Right? SHRESTHA BASU MALLICK: Yeah. May I build upon that? I think as we've been mentioning\nwith the 2.5 series models, these models are predominantly\ntrained to be really good at planning and reasoning,\nwhich, when you think about it, is a key part of what\nmakes an agent work. So the model layer is\nwhere a lot of the planning and reasoning happens. And then, of course,\nthere's the tools layer. We've already talked\na lot about we have a set of first party\nhosted tools from Google, Google Search, Code Execution,\nand URL context, as well as a few other tools. Some of you may have heard\nSundar mention the computer use",
        "cosine": null
      }
    ],
    "true_documents": []
  },
  {
    "query": "Where can I find a Google AI Studio demo featuring an AI FOX character, weather queries for Mountain View, and a Portuguese dialogue about nearby hiking trails?",
    "retrieved_doc_ids": [
      "4TE-KFXvhAk__chunk_004",
      "4TE-KFXvhAk__chunk_019",
      "4TE-KFXvhAk__chunk_000",
      "4TE-KFXvhAk__chunk_003",
      "4TE-KFXvhAk__chunk_007"
    ],
    "recall_at_5": 1,
    "per_doc_cosine": [],
    "avg_cosine": 0.0,
    "reciprocal_rank": 1.0,
    "llm_score": null,
    "retrieved_answers": [
      {
        "doc_id": "4TE-KFXvhAk__chunk_004",
        "text": "So if we go to Google AI Studio\nhere, if we go into Build, and Mumble Jumble is\nsomething that we've literally just created, and\nit's one of my favorite apps. So Mumble Jumble is one\nof those applications that with just natural\nlanguage allows you to interact with\nthe model so you can build those more dynamic and\ninteractive audio experiences. So let's have a look. So, first of all, it uses 2.5\npreview native audio dialog. You can choose if it's easy to\ninterrupt or hard to interrupt. And then you can come and\npick your voice, hi pitch. So there's multiple things\nthat you can play with and then you can customize. I really like the fox, so let's\ngo ahead and pick the fox. Let's get the cowboy. Happy because we're\nat Google I/O. We're all super\nhappy to be here. And let's get a microphone,\nbecause I have one. AI FOX: Howdy, partner. Finley Flicker Fox here ready\nto charm the boots off you. JOANA CARRASQUEIRA: Hello. What's the weather like\ntoday in Mountain View? AI FOX: Well now, it's a fine\nday in Mountain View, sunny and quite pleasant. Perfect for moseying\non out and about. JOANA CARRASQUEIRA: I agree. Are there any hiking trails\nnear the Google campus? AI FOX: Why sure as shooting. There are plenty of trails\nnear the Google campus, perfect for a grand\nadventure and some fresh air. JOANA CARRASQUEIRA:\n[SPEAKING PORTUGUESE] AI FOX: Well now, the best\nway to get to those trails is to head towards the foothills\nfollowing the marked paths. You'll find them\nright along the way. JOANA CARRASQUEIRA:\n[SPEAKING PORTUGUESE] AI FOX: You betcha. There are a few\ncozy cafes nearby, perfect for a spell\nof rest and a good cup of coffee after your hike. JOANA CARRASQUEIRA:\nThank you so much. AI FOX: It was my\npleasure, partner. Happy trails, and\nenjoy your coffee. JOANA CARRASQUEIRA: Thank you. So, as you could see, there's\nsome really cool experiences that we're bringing\ninto AI Studio. Audio is getting better; more\nnatural experiences with voice. And in case you\ndidn't notice, I even",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_019",
        "text": "So in order to learn\nmore, Josh, what do our developers have to do? JOSH GORDON: We have\njust a few links for you. So no problem. But we talked about a lot of\ndifferent tools in the stack. JOANA CARRASQUEIRA: We did. JOSH GORDON: So I don't\nwant to read the slide, but let me just point you\nto a couple highlights. ai.google.dev is the\nbest place to go to get started with Gemini and Gemma. We have a cookbook for Gemini. We have a cookbook for Gemma. Google AI Studio is\naistudio.google.com. If you're interested in JAX\nand Keras, there are the links. If you happen to be\ninterested in XLA, please check it\nout-- openxla.org. Google AI Edge is\nat the very bottom if you're a mobile\ndeveloper and you're interested in mobile deployment. And just to be clear, there's\nso many amazing things in the Google AI stack we didn't\nhave time to talk about today. Vertex has really amazing tools\nfor enterprise developers. But please, start\nhere, have fun, and, yeah, we're\naround after the talk. JOANA CARRASQUEIRA:\nYes, absolutely. And the Developer Relations\nteam is just outside. We have some really\ncool demo stations that you can experience. Engage with the team. Check out the sessions\ntomorrow, especially on the Gemini API,\nGemmaverse, and robotics. We have a lot of\ncool stuff that we want to put in the\nhands of developers. Many early access\nprograms as well. Stay in touch, stay\nengaged, and let's co-create the future of AI together. Thank you so much. JOSH GORDON: Thanks a lot. Thanks [APPLAUSE] [MUSIC PLAYING]",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_000",
        "text": "[MUSIC PLAYING] JOANA CARRASQUEIRA:\nHello, everyone. My name is Joana Carrasqueira,\nand I lead Developer Relations at Google DeepMind. JOSH GORDON: Hi, everyone. I'm Josh. JOANA CARRASQUEIRA:\nAnd we're very excited to welcome you to\nour session, Google's AI Stack for Developers. We'll start by giving you a\nquick overview of Google's AI stack. Who's at I/O for the first time? Can I see some hands up? Oh, OK. Welcome to Google\nI/O. It's a pleasure to have you with us today. So we'll start by giving\nyou an overview of Google's end-to-end ecosystem of AI. And as you know, we've\nbeen leading the way in AI for decades, since we\nopen-sourced TensorFlow in 2015, from when we published our\nfield-defining research with transformers\nin 2017, to Gemini. And we are now in\nthe Gemini era. So we've been releasing a lot. Relentlessly, as it's\nbeen called today, we've been shipping many\nfeatures, many new products. And in our talk,\nwe're actually going to give you an overview\nof everything that's new for developers\nthroughout the AI stack. Our mission is to empower every\ndeveloper and organization to harness the power of AI. And Google's stack is so good\nand flexible because it combines very robust infrastructure\nwith state-of-the-art research. And all of this enables\nreal-world applications come to life that change\nentire fields, industries, and companies. We'll start by discussing\nfoundation models, touching upon our\nGemini, Gemma, and some of our domain-specific models. JOSH GORDON: After\nfoundation models, we'll take a look\nat AI frameworks that we use to build them. So we'll talk about JAX, which\nis really great for researchers. We'll talk about Keras, which is\nreally amazing for applied AI. Later on, we'll even\ntalk a little bit about the work we're\ndoing with PyTorch. JOANA CARRASQUEIRA: We'll\nalso touch upon some developer tools for all\ntypes of experience from beginners to advanced. JOSH GORDON: Then we'll talk a\nlittle bit about infrastructure. And this talk is about\nsoftware not hardware.",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_003",
        "text": "On the Gemini API\nside of things, there are also new capabilities\nfor text-to-speech, allowing you to control\nemotion and style for a more expressive and dynamic audio. And it's both available\non the live API and also on [INAUDIBLE] API\nfor real generating audio. And some of the use\ncases here that we had in mind when\nwe built this were more dynamic audiobooks,\nsome more engaging podcasts, or, even for those of\nyou in customer support, producing more natural\nvoices into your workflows. We also have enhanced\ntooling, which is really cool, because now you\ncan use Grounding with Google Search, also with Code Execution\ntogether in just one API call, and URL context, which you\nheard during the keynote, which provides the model with\ndepth content from web pages. And since you can chain\nit to other tools, it's actually really powerful\nto build search agents. So really cool stuff. Lastly, just to call out that\nwe now offer Gemini SDK support for MCP, which reduces a\nlot of developer friction and simplifies building\nagentic capabilities. So you don't want to\nmiss tomorrow's talk to learn more about this. And Google AI Studio. Who here uses Google AI Studio? Can I see some hands? OK, awesome. Awesome. We have a lot to chat\nafter this session then. Google AI Studio is the\nperfect place for anyone to start developing with AI. It's the simplest way to test\nthe latest DeepMind models. We typically bring them\nto Google AI Studio so you can start prototyping\nand playing with the models. And you don't need Google\nCloud knowledge in order to set up your environment. It is free of charge, and you\ncan create, and test, and save your prompts. And there's also starter\napps that will inspire you. And that's exactly what I'm\ngoing to demo here today for you. I am going to show\nyou some of the work that the team has been doing. So if we go to Google AI Studio\nhere, if we go into Build, and Mumble Jumble is\nsomething that we've literally just created, and\nit's one of my favorite apps. So Mumble Jumble is one\nof those applications that",
        "cosine": null
      },
      {
        "doc_id": "4TE-KFXvhAk__chunk_007",
        "text": "So it's just really\nquick and easy to do. OK, we can go back\nto the slides. So very, very\nquickly, as a recap, Gemini Developer API is the\neasiest way to get started. It's super lightweight,\nit's fast to install, and you can get up and running\nhonestly in about a minute. OK? I will use the clicker. This is the flow to get\nstarted with Google AI Studio. Go to Google AI Studio. Get your key. Try one of the code\nexamples on ai.google.dev or in the cookbook. I see people taking pictures. That makes me happy. Please try this. We spent so much time\non making this easy, and I hope it works for you. If not, please file an\nissue and we'll get on it. This is the Gen AI SDK\nfor the Gemini API, and this is something\nwe've been rolling out. It's our latest SDK. We've been rolling\nit out gradually over the course of\nthe last six months. It's super user friendly. It's really easy to use. Really, the only point\nI want to make here, because I don't want\nto read the code examples or the\ndocumentation to you, you can call the API\nin a few lines of code. Basically add your key, select\na model, write a prompt, you can go ahead and call it. You can also get access\nto advanced functionality in one line of code. So if you'd like to get the\nthinking summaries that Joana showed you, you can just\nadd a ThinkingConfig, say include the thoughts. Now, you've got the\nthinking summaries. And a good use\ncase for this could be any time you need to\nexplain the model's reasoning. Particularly, you can\nimagine, if you're building an education\napp or a tutoring app, you can get the\nthinking summaries. In addition to\nreally cool things that you can do with\na single line of code, there's some more advanced stuff\nthat you can do with the SDK as well. So I know there's a lot\nof code on this slide, but we've talked a lot\nabout building agents and agentic experiences. In this example,\nyou could imagine that you have a Python\nfunction on your laptop called weather_function. And maybe that calls\nyour own weather server to get the weather.",
        "cosine": null
      }
    ],
    "true_documents": []
  }
]
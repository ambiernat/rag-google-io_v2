{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a77bfa7-b249-4a38-ab5b-f9b5b0863837",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c8eef3-3fa2-448c-bd99-014ef5030999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import SparseVector\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bccf657-a7da-4970-94f0-294ce786b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eedf0f6-be20-4118-ba98-3670acf05be6",
   "metadata": {},
   "source": [
    "# Connect to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bacd9d7-8a10-48b6-b760-dd3f0c53433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "DENSE_COLLECTION = \"google-io-transcripts\"  # If you have dense-only collection\n",
    "SPARSE_COLLECTION = \"sparse_collection\"      # If you have sparse-only collection\n",
    "HYBRID_COLLECTION = \"hybrid_collection\"      # Recommended for hybrid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f22ad8-5a6e-4ea5-ac92-f7104908917f",
   "metadata": {},
   "source": [
    "# Load Models (Dense + Sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ea21f-95d4-4c67-b622-11ba61526529",
   "metadata": {},
   "source": [
    "#### Dense Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42d1a9e-26ed-42e2-93c0-9cadd7ac9813",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90df4595-27ba-4d2e-a21e-ca327c35b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(text: str):\n",
    "    return embedding_model.encode(text).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc40aea6-0a6b-4ad4-9a7c-db503a241d7c",
   "metadata": {},
   "source": [
    "# Sparse Query Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930a594a-33bb-4c66-8275-68799605d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from retrieval.retrievers.BM25 import BM25Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3ceb8e7-6809-4134-a46d-8fd307ce8f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In run_index_sparse.py, run_index_hybrid.py, run_query_hybrid.py, and notebook:\n",
    "\n",
    "with open(\"../data/models/bm25_encoder.pkl\", \"rb\") as f:\n",
    "    bm25_encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ff5e1-2c58-4541-b370-3e45f812840e",
   "metadata": {},
   "source": [
    "#### Query Encoding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6646c31e-4b8f-4bbf-bce3-a247d305e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    \"\"\"Must match tokenization used during indexing\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "def bm25_query(query: str):\n",
    "    \"\"\"Encode query as sparse vector using BM25\"\"\"\n",
    "    tokens = tokenize(query)\n",
    "    return bm25_encoder.encode_query(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33588bcb-967e-41bc-9809-77e458cf973f",
   "metadata": {},
   "source": [
    "# Dense & Sparse Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11b64fae-95b5-4953-bdb7-47d9875bb4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_retrieve(query: str, k: int = 10):\n",
    "    \"\"\"Retrieve using dense embeddings only\"\"\"\n",
    "    vector = embed_query(query)\n",
    "    hits = q_client.query_points(\n",
    "        collection_name=DENSE_COLLECTION,\n",
    "        query=vector,\n",
    "        limit=k,\n",
    "    )\n",
    "    return hits.points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0f3e8e3-5eda-4b39-ba17-a021c25aaf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_retrieve(query: str, k: int = 10):\n",
    "    \"\"\"Retrieve using sparse BM25 vectors only\"\"\"\n",
    "    sparse_vec = bm25_query(query)\n",
    "    hits = q_client.query_points(\n",
    "        collection_name=SPARSE_COLLECTION,\n",
    "        query=sparse_vec,\n",
    "        using=\"text\",\n",
    "        limit=k,\n",
    "    )\n",
    "    return hits.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9993f179-e5fa-4438-aca5-ba29a7cde5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_retrieve_rrf(query: str, k_dense: int = 50, k_sparse: int = 50, k: int = 10):\n",
    "    \"\"\"\n",
    "    Hybrid retrieval using Reciprocal Rank Fusion (RRF)\n",
    "    Retrieves from separate dense and sparse collections, then fuses\n",
    "    \"\"\"\n",
    "    # Get results from both retrievers\n",
    "    dense_results = dense_retrieve(query, k=k_dense)\n",
    "    sparse_results = sparse_retrieve(query, k=k_sparse)\n",
    "    \n",
    "    # Apply RRF fusion\n",
    "    return rrf([dense_results, sparse_results], k_final=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bc08db7c-ac9d-46d5-8b9d-f144d88e9791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import Prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "57a82517-8f93-44cf-9e60-5913462907e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_retrieve(query: str, k: int = 10):\n",
    "    \"\"\"Native hybrid retrieval using Qdrant's built-in hybrid search\"\"\"\n",
    "    dense_vector = embed_query(query)\n",
    "    sparse_vector = bm25_query(query)\n",
    "    \n",
    "    response = q_client.query_points(\n",
    "        collection_name=HYBRID_COLLECTION,\n",
    "        prefetch=[\n",
    "            Prefetch(\n",
    "                query=sparse_vector,\n",
    "                using=\"text\",\n",
    "                limit=50,\n",
    "            )\n",
    "        ],\n",
    "        query=dense_vector,\n",
    "        using=\"dense\",\n",
    "        limit=k,\n",
    "    )\n",
    "    \n",
    "    # Convert to dictionary format\n",
    "    results = []\n",
    "    for point in response.points:\n",
    "        results.append({\n",
    "            \"id\": point.id,\n",
    "            \"score\": point.score,\n",
    "            \"payload\": point.payload\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Update the collection name\n",
    "HYBRID_COLLECTION = \"hybrid_collection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23748159-2c64-43dd-8d4d-0374bc1ddbce",
   "metadata": {},
   "source": [
    "# Reciprocal Rank Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "864ea3a3-5ff3-46bd-936d-7b97a830d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf(rankings: List, k_final: int = 10, k_rrf: int = 60):\n",
    "    \"\"\"\n",
    "    Reciprocal Rank Fusion algorithm\n",
    "    \n",
    "    Args:\n",
    "        rankings: List of result lists from different retrievers\n",
    "        k_final: Number of final results to return\n",
    "        k_rrf: RRF constant (typically 60)\n",
    "    \"\"\"\n",
    "    scores = defaultdict(float)\n",
    "    payloads = {}\n",
    "    \n",
    "    for ranking in rankings:\n",
    "        for rank, hit in enumerate(ranking):\n",
    "            # Use hit.id for Qdrant point ID\n",
    "            doc_id = hit.id\n",
    "            scores[doc_id] += 1.0 / (k_rrf + rank + 1)\n",
    "            payloads[doc_id] = hit.payload\n",
    "    \n",
    "    # Sort by score and return top k_final\n",
    "    fused_results = sorted(\n",
    "        [\n",
    "            {\"id\": doc_id, \"score\": score, \"payload\": payloads[doc_id]}\n",
    "            for doc_id, score in scores.items()\n",
    "        ],\n",
    "        key=lambda x: x[\"score\"],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    return fused_results[:k_final]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afca21-2b46-40b0-8995-ca0123f4d333",
   "metadata": {},
   "source": [
    "# Hybrid Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5657d9b4-e74e-4738-bded-b5b541e3998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose Your Hybrid Retrieval Method\n",
    "\n",
    "# Use this for RRF-based hybrid (works with separate collections)\n",
    "# hybrid_retrieve = hybrid_retrieve_rrf\n",
    "\n",
    "# OR use this for native Qdrant hybrid (requires hybrid_collection)\n",
    "hybrid_retrieve = hybrid_retrieve_native"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad6ceb-33ec-4f37-b307-514e0d26ddc2",
   "metadata": {},
   "source": [
    "### Manual Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e8c00f8-df87-4890-b64c-6e173430eca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available collections:\n",
      "  - google-io-transcripts\n",
      "  - hybrid_collection\n",
      "  - sparse_collection\n"
     ]
    }
   ],
   "source": [
    "# Check collections\n",
    "collections = q_client.get_collections()\n",
    "\n",
    "print(\"Available collections:\")\n",
    "for collection in collections.collections:\n",
    "    print(f\"  - {collection.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c87a95d2-0c8b-4cff-9cd3-0a933d3ec11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is Gemini?\n",
      "\n",
      "1. Score: 0.4081\n",
      "   Text: [MUSIC PLAYING] [CHEERING] [AUDIENCE SCREAMING] LUCIANO MARTINS: Hey, folks. Good morning. It's a pleasure to\n",
      "be here with you all. I'm Luciano Martins. I'm Brazilian. [CHEERING] I'm an AI Developer\n",
      "Advocate at Google DeepMind. And I'm here with\n",
      "my friend Shrestha. SHRESTHA BASU\n",
      "MALLICK: Thank you. Luciano Hi, everyone. I am Shreshta Basu Mallick. I'm the Product Lead for\n",
      "the Gemini Developer API. And it looks like I should have\n",
      "brought an Indian contingent here. [LAUGHING] Thank you. LUCIANO MA...\n",
      "\n",
      "2. Score: 0.3980\n",
      "   Text: when we think about the key\n",
      "standard benchmarks, Shrestha? SHRESTHA BASU\n",
      "MALLICK: Yeah, we'll go to the benchmarks\n",
      "in a second. But the message\n",
      "that I want everyone to take away from this\n",
      "slide is whether it's a super complex task, or whether\n",
      "it's on-device processing, you now have a Gemini model\n",
      "that you can use for it. And it's pretty powerful. And it's pretty price effective. LUCIANO MARTINS: Yeah,\n",
      "maybe it is also worth mentioning, Shrestha,\n",
      "that some folks here may be developing with\n",
      "Gemini...\n",
      "\n",
      "3. Score: 0.3932\n",
      "   Text: We launched a Java during\n",
      "I/O, right, Shrestha? And also, you have\n",
      "the ability to use the API on other developer\n",
      "tools you may be using. For example, if you\n",
      "use Firebase Studio, the API is available there. If you are a Google\n",
      "Colab user, you have ways to interact with\n",
      "the Gemini models as well. So the key idea is to\n",
      "make it easier for you. No matter where you are having\n",
      "your development experience, We are trying to bring\n",
      "Gemini API closer to you. SHRESTHA BASU MALLICK:\n",
      "That's correct. And I thin...\n",
      "\n",
      "4. Score: 0.3656\n",
      "   Text: but we've talked a lot\n",
      "about building agents and agentic experiences. In this example,\n",
      "you could imagine that you have a Python\n",
      "function on your laptop called weather_function. And maybe that calls\n",
      "your own weather server to get the weather. What you can do is you can pass\n",
      "the definition of that function to the Gemini API\n",
      "in JSON, including the function name and the\n",
      "parameters that it takes. Then what you can do is\n",
      "you can write a prompt. So here, the prompt happens\n",
      "to be \"What's the temperature...\n",
      "\n",
      "5. Score: 0.2798\n",
      "   Text: you are able to generate high\n",
      "quality images, high quality videos. You also have-- and there is a\n",
      "colleague from our team, Paul Ruiz, who is delivering\n",
      "a talk this afternoon related to the Gemini Robotics,\n",
      "the Gemini family of models related to people\n",
      "applying robotics, developing robotic\n",
      "solution with applied AI. And also, just\n",
      "after the session, we have one another one\n",
      "with Omar and Gus Martins to talk about Gemma 3,\n",
      "which is the open models family for from\n",
      "Google DeepMind, also developed with...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Gemini?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "results = hybrid_retrieve(query, k=5)\n",
    "\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. Score: {r['score']:.4f}\")\n",
    "    print(f\"   Text: {r['payload']['text'][:500]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85661af0-8b7e-46a6-a8b4-daa36aa092f8",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0ec34bb9-f0a2-4645-8d82-662b0ab09686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth\n",
    "with open(\"../data/eval/ground_truth_gpt5nano.json\") as f:\n",
    "    ground_truth = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4e28722a-bd85-401a-bf1f-c3f3bcb2716a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 evaluation queries\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(ground_truth)} evaluation queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea847bc5-616c-4435-bef6-83c2999814d1",
   "metadata": {},
   "source": [
    "#### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "198fd104-a960-4425-bbe9-ea92883a4de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First result payload:\n",
      "{'doc_id': 'gHHjDRDNUNU__chunk_000', 'text': \"[MUSIC PLAYING] [CHEERING] [AUDIENCE SCREAMING] LUCIANO MARTINS: Hey, folks. Good morning. It's a pleasure to\\nbe here with you all. I'm Luciano Martins. I'm Brazilian. [CHEERING] I'm an AI Developer\\nAdvocate at Google DeepMind. And I'm here with\\nmy friend Shrestha. SHRESTHA BASU\\nMALLICK: Thank you. Luciano Hi, everyone. I am Shreshta Basu Mallick. I'm the Product Lead for\\nthe Gemini Developer API. And it looks like I should have\\nbrought an Indian contingent here. [LAUGHING] Thank you. LUCIANO MARTINS: Yay! OK, so the idea of\\nthis conversation is we want to share with you\\nsome of the new things you have available to develop your\\nsolutions using Gemini models and the Gemini API. How many developers\\nyou have here? Amazing. OK, so we can start talking\\nabout the Gemini models universe. We started Gemini\\nby the end of 2023. And since then, we\\nhave done a lot of work together between many different\\nGoogle DeepMind teams. And by now, we are very\\nproud of the point we got in and all the stuff we\\nlaunched at during I/O. Just doing a quick recap,\\none of the key differentials of the Gemini is that it is\\nmulti-modal from scratch. Since when we started\\ndeveloping the model, it always handled and\\nunderstood multi-modal data. It means that you\\ncan work with Gemini with any format\\nof information you want, from text, image, video,\\naudio, code, anything, right Shrestha? SHRESTHA BASU MALLICK:\\nThat's correct. And just a quick\\noverview of what are all the types\\nof models, and what are all the families of\\nmodels we have available. So what we're going\\nto do today is Luciano and I split this\\ntalk into two parts. So the first part\\nof the talk will be talking about the models. And then the second\\npart of the talk will be talking about the API. What are the capabilities\\nand functionalities available through the API? And as part of that,\\nwe have a section where we do a deep dive on\\nagentic capabilities in the API. LUCIANO MARTINS:\\nThanks for the add. SHRESTHA BASU MALLICK: Yeah. So starting with what are\\nthe families of models we have available?\", 'video_id': 'gHHjDRDNUNU', 'title': 'Google I/O 2025 - Keynote', 'timestamp_start': 0.0, 'timestamp_end': 146.11, 'source': 'youtube', 'speaker': 'unknown'}\n",
      "\n",
      "Available keys:\n",
      "dict_keys(['doc_id', 'text', 'video_id', 'title', 'timestamp_start', 'timestamp_end', 'source', 'speaker'])\n"
     ]
    }
   ],
   "source": [
    "# Debug: see what's actually in the payload\n",
    "query = \"What is Gemini?\"\n",
    "results = hybrid_retrieve(query, k=5)\n",
    "\n",
    "print(\"First result payload:\")\n",
    "print(results[0][\"payload\"])\n",
    "print(\"\\nAvailable keys:\")\n",
    "print(results[0][\"payload\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cf7ef07a-1588-4375-ada6-58e5d713b677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of results: <class 'list'>\n",
      "Number of results: 5\n",
      "\n",
      "First result type: <class 'dict'>\n",
      "First result: {'id': 42, 'score': 0.01639344262295082, 'payload': {'doc_id': 'gHHjDRDNUNU__chunk_022', 'video_id': 'gHHjDRDNUNU', 'title': 'Google I/O 2025 - Keynote', 'timestamp_start': 2746.57, 'timestamp_end': 2767.29, 'text': 'more of the other features and\\npossibilities with the models. And the last one is the Gemini\\nCookbook, a GitHub repository that our team curates and keep\\nupdating with a lot of sample experience for you. Thank you so much, Shrestha. Thank you so much you all. SHRESTHA BASU MALLICK: Thank you\\nall for coming out to hear us. [CHEERING] [MUSIC PLAYING]', 'source': 'youtube', 'speaker': 'unknown'}}\n",
      "\n",
      "First result keys: dict_keys(['id', 'score', 'payload'])\n"
     ]
    }
   ],
   "source": [
    "# Debug the actual structure\n",
    "query = \"What is Gemini?\"\n",
    "results = hybrid_retrieve(query, k_final=5)\n",
    "\n",
    "print(f\"Type of results: {type(results)}\")\n",
    "print(f\"Number of results: {len(results)}\")\n",
    "print(f\"\\nFirst result type: {type(results[0])}\")\n",
    "print(f\"First result: {results[0]}\")\n",
    "print(f\"\\nFirst result keys: {results[0].keys() if isinstance(results[0], dict) else 'Not a dict'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62b73f11-f05e-4b6c-a87f-a52a2f901eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload keys: dict_keys(['doc_id', 'video_id', 'title', 'timestamp_start', 'timestamp_end', 'text', 'source', 'speaker'])\n",
      "Full payload: {'doc_id': 'gHHjDRDNUNU__chunk_022', 'video_id': 'gHHjDRDNUNU', 'title': 'Google I/O 2025 - Keynote', 'timestamp_start': 2746.57, 'timestamp_end': 2767.29, 'text': 'more of the other features and\\npossibilities with the models. And the last one is the Gemini\\nCookbook, a GitHub repository that our team curates and keep\\nupdating with a lot of sample experience for you. Thank you so much, Shrestha. Thank you so much you all. SHRESTHA BASU MALLICK: Thank you\\nall for coming out to hear us. [CHEERING] [MUSIC PLAYING]', 'source': 'youtube', 'speaker': 'unknown'}\n"
     ]
    }
   ],
   "source": [
    "# Check the payload structure\n",
    "print(f\"Payload keys: {results[0]['payload'].keys()}\")\n",
    "print(f\"Full payload: {results[0]['payload']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0577df7-7dd3-4a46-a18b-66a818d95fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_id(result):\n",
    "    \"\"\"Helper to get document ID\"\"\"\n",
    "    return result[\"payload\"][\"doc_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e0d29151-6e8f-459b-a4d9-d6c657bb26ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(results: List[Dict], relevant_ids: List[str], k: int) -> int:\n",
    "    \"\"\"Binary recall: 1 if any relevant doc in top-k, else 0\"\"\"\n",
    "    retrieved = [r[\"payload\"][\"doc_id\"] for r in results[:k]]\n",
    "    return int(any(rid in retrieved for rid in relevant_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "18db809e-7656-46c9-b553-3219e9e5e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(results: List[Dict], relevant_ids: List[str]) -> float:\n",
    "    \"\"\"Mean Reciprocal Rank: 1/rank of first relevant doc\"\"\"\n",
    "    for i, r in enumerate(results, start=1):\n",
    "        if r[\"payload\"][\"doc_id\"] in relevant_ids:\n",
    "            return 1.0 / i\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b9cb27a-b23a-49d8-b8de-aab925a49b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(results: List[Dict], relevant_ids: List[str], k: int) -> float:\n",
    "    \"\"\"Proportion of retrieved docs that are relevant\"\"\"\n",
    "    retrieved = [r[\"payload\"][\"doc_id\"] for r in results[:k]]\n",
    "    relevant_retrieved = sum(1 for rid in retrieved if rid in relevant_ids)\n",
    "    return relevant_retrieved / k if k > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd88149-8a02-4270-897b-b83b57fa177e",
   "metadata": {},
   "source": [
    "# Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "56120561-ef4b-47b4-84ee-d7912377f9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test query: What were the main topics and components discussed in Google's AI Stack for Developers session at Google I/O, including foundation models and the developer frameworks mentioned?\n",
      "\n",
      "Number of results: 5\n",
      "\n",
      "First result:\n",
      "  Type: <class 'dict'>\n",
      "  Keys: dict_keys(['id', 'score', 'payload'])\n",
      "  Full result: {'id': 0, 'score': 0.01639344262295082, 'payload': {'doc_id': '4TE-KFXvhAk__chunk_000', 'video_id': '4TE-KFXvhAk', 'title': \"Google I/O 2025 - What's New in AI\", 'timestamp_start': 0.0, 'timestamp_end': 139.04, 'text': \"[MUSIC PLAYING] JOANA CARRASQUEIRA:\\nHello, everyone. My name is Joana Carrasqueira,\\nand I lead Developer Relations at Google DeepMind. JOSH GORDON: Hi, everyone. I'm Josh. JOANA CARRASQUEIRA:\\nAnd we're very excited to welcome you to\\nour session, Google's AI Stack for Developers. We'll start by giving you a\\nquick overview of Google's AI stack. Who's at I/O for the first time? Can I see some hands up? Oh, OK. Welcome to Google\\nI/O. It's a pleasure to have you with us today. So we'll start by giving\\nyou an overview of Google's end-to-end ecosystem of AI. And as you know, we've\\nbeen leading the way in AI for decades, since we\\nopen-sourced TensorFlow in 2015, from when we published our\\nfield-defining research with transformers\\nin 2017, to Gemini. And we are now in\\nthe Gemini era. So we've been releasing a lot. Relentlessly, as it's\\nbeen called today, we've been shipping many\\nfeatures, many new products. And in our talk,\\nwe're actually going to give you an overview\\nof everything that's new for developers\\nthroughout the AI stack. Our mission is to empower every\\ndeveloper and organization to harness the power of AI. And Google's stack is so good\\nand flexible because it combines very robust infrastructure\\nwith state-of-the-art research. And all of this enables\\nreal-world applications come to life that change\\nentire fields, industries, and companies. We'll start by discussing\\nfoundation models, touching upon our\\nGemini, Gemma, and some of our domain-specific models. JOSH GORDON: After\\nfoundation models, we'll take a look\\nat AI frameworks that we use to build them. So we'll talk about JAX, which\\nis really great for researchers. We'll talk about Keras, which is\\nreally amazing for applied AI. Later on, we'll even\\ntalk a little bit about the work we're\\ndoing with PyTorch. JOANA CARRASQUEIRA: We'll\\nalso touch upon some developer tools for all\\ntypes of experience from beginners to advanced. JOSH GORDON: Then we'll talk a\\nlittle bit about infrastructure. And this talk is about\\nsoftware not hardware.\", 'source': 'youtube', 'speaker': 'unknown'}}\n",
      "\n",
      "Payload keys: dict_keys(['doc_id', 'video_id', 'title', 'timestamp_start', 'timestamp_end', 'text', 'source', 'speaker'])\n",
      "Full payload: {'doc_id': '4TE-KFXvhAk__chunk_000', 'video_id': '4TE-KFXvhAk', 'title': \"Google I/O 2025 - What's New in AI\", 'timestamp_start': 0.0, 'timestamp_end': 139.04, 'text': \"[MUSIC PLAYING] JOANA CARRASQUEIRA:\\nHello, everyone. My name is Joana Carrasqueira,\\nand I lead Developer Relations at Google DeepMind. JOSH GORDON: Hi, everyone. I'm Josh. JOANA CARRASQUEIRA:\\nAnd we're very excited to welcome you to\\nour session, Google's AI Stack for Developers. We'll start by giving you a\\nquick overview of Google's AI stack. Who's at I/O for the first time? Can I see some hands up? Oh, OK. Welcome to Google\\nI/O. It's a pleasure to have you with us today. So we'll start by giving\\nyou an overview of Google's end-to-end ecosystem of AI. And as you know, we've\\nbeen leading the way in AI for decades, since we\\nopen-sourced TensorFlow in 2015, from when we published our\\nfield-defining research with transformers\\nin 2017, to Gemini. And we are now in\\nthe Gemini era. So we've been releasing a lot. Relentlessly, as it's\\nbeen called today, we've been shipping many\\nfeatures, many new products. And in our talk,\\nwe're actually going to give you an overview\\nof everything that's new for developers\\nthroughout the AI stack. Our mission is to empower every\\ndeveloper and organization to harness the power of AI. And Google's stack is so good\\nand flexible because it combines very robust infrastructure\\nwith state-of-the-art research. And all of this enables\\nreal-world applications come to life that change\\nentire fields, industries, and companies. We'll start by discussing\\nfoundation models, touching upon our\\nGemini, Gemma, and some of our domain-specific models. JOSH GORDON: After\\nfoundation models, we'll take a look\\nat AI frameworks that we use to build them. So we'll talk about JAX, which\\nis really great for researchers. We'll talk about Keras, which is\\nreally amazing for applied AI. Later on, we'll even\\ntalk a little bit about the work we're\\ndoing with PyTorch. JOANA CARRASQUEIRA: We'll\\nalso touch upon some developer tools for all\\ntypes of experience from beginners to advanced. JOSH GORDON: Then we'll talk a\\nlittle bit about infrastructure. And this talk is about\\nsoftware not hardware.\", 'source': 'youtube', 'speaker': 'unknown'}\n"
     ]
    }
   ],
   "source": [
    "# # Debug: Check what's actually being returned\n",
    "# query = ground_truth[0][\"query\"]\n",
    "# print(f\"Test query: {query}\\n\")\n",
    "\n",
    "# results = hybrid_retrieve(query, k_final=5)\n",
    "\n",
    "# print(f\"Number of results: {len(results)}\")\n",
    "# print(f\"\\nFirst result:\")\n",
    "# print(f\"  Type: {type(results[0])}\")\n",
    "# print(f\"  Keys: {results[0].keys() if isinstance(results[0], dict) else 'Not a dict'}\")\n",
    "# print(f\"  Full result: {results[0]}\")\n",
    "\n",
    "# if isinstance(results[0], dict) and 'payload' in results[0]:\n",
    "#     print(f\"\\nPayload keys: {results[0]['payload'].keys()}\")\n",
    "#     print(f\"Full payload: {results[0]['payload']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cfb8e5b9-9787-4111-ad46-f2ebf7641cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "Queries evaluated: 10/10\n",
      "Recall@5:      0.6000\n",
      "Recall@10:     0.6000\n",
      "MRR:           0.3667\n",
      "Precision@5:   0.1200\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# In your notebook, run the evaluation\n",
    "recalls_5 = []\n",
    "recalls_10 = []\n",
    "mrrs = []\n",
    "precisions_5 = []\n",
    "\n",
    "for idx, item in enumerate(ground_truth):\n",
    "    query = item[\"query\"]\n",
    "    relevant_ids = item[\"relevant_doc_ids\"]\n",
    "    \n",
    "    # Retrieve\n",
    "    results = hybrid_retrieve(query, k=5)\n",
    "    \n",
    "    # Skip if no results\n",
    "    if not results:\n",
    "        continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    try:\n",
    "        retrieved_5 = [r[\"payload\"][\"doc_id\"] for r in results[:5]]\n",
    "        retrieved_10 = [r[\"payload\"][\"doc_id\"] for r in results[:10]]\n",
    "        \n",
    "        recalls_5.append(int(any(rid in retrieved_5 for rid in relevant_ids)))\n",
    "        recalls_10.append(int(any(rid in retrieved_10 for rid in relevant_ids)))\n",
    "        \n",
    "        # MRR\n",
    "        mrr_score = 0.0\n",
    "        for i, r in enumerate(results, start=1):\n",
    "            if r[\"payload\"][\"doc_id\"] in relevant_ids:\n",
    "                mrr_score = 1.0 / i\n",
    "                break\n",
    "        mrrs.append(mrr_score)\n",
    "        \n",
    "        # Precision@5\n",
    "        relevant_count = sum(1 for rid in retrieved_5 if rid in relevant_ids)\n",
    "        precisions_5.append(relevant_count / 5.0)\n",
    "        \n",
    "    except (KeyError, TypeError) as e:\n",
    "        print(f\"Error on query {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Queries evaluated: {len(recalls_5)}/{len(ground_truth)}\")\n",
    "print(f\"Recall@5:      {np.mean(recalls_5):.4f}\")\n",
    "print(f\"Recall@10:     {np.mean(recalls_10):.4f}\")\n",
    "print(f\"MRR:           {np.mean(mrrs):.4f}\")\n",
    "print(f\"Precision@5:   {np.mean(precisions_5):.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91020d46-8fc7-4db1-b818-88c1207e1b8e",
   "metadata": {},
   "source": [
    "#### Optional: Compare Dense vs Sparse vs Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9caa2ba9-b1dd-4281-8d77-a824a1b5c1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ABLATION STUDY\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ABLATION STUDY\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e7076f-4034-4fa8-badc-9da4567f21e1",
   "metadata": {},
   "source": [
    "##### Dense only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b362490c-dabf-48fc-876b-43d3691998e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dense Only:\n",
      "  Recall@5: 0.8000\n",
      "  MRR:      0.6250\n"
     ]
    }
   ],
   "source": [
    "dense_recalls = []\n",
    "dense_mrrs = []\n",
    "for item in ground_truth:\n",
    "    results_dense = dense_retrieve(item[\"query\"], k=5)\n",
    "    # Convert to same format as RRF results\n",
    "    results_formatted = [\n",
    "        {\"payload\": hit.payload, \"score\": hit.score}\n",
    "        for hit in results_dense\n",
    "    ]\n",
    "    dense_recalls.append(recall_at_k(results_formatted, item[\"relevant_doc_ids\"], k=5))\n",
    "    dense_mrrs.append(mrr(results_formatted, item[\"relevant_doc_ids\"]))\n",
    "\n",
    "print(f\"\\nDense Only:\")\n",
    "print(f\"  Recall@5: {np.mean(dense_recalls):.4f}\")\n",
    "print(f\"  MRR:      {np.mean(dense_mrrs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef17b4-5a0d-4a64-8418-2c3013d79100",
   "metadata": {},
   "source": [
    "##### Sparse only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f9b62e55-973f-477f-ab87-07db05cb70c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'doc_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      4\u001b[39m results_sparse = sparse_retrieve(item[\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m], k=\u001b[32m5\u001b[39m)\n\u001b[32m      5\u001b[39m results_formatted = [\n\u001b[32m      6\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mpayload\u001b[39m\u001b[33m\"\u001b[39m: hit.payload, \u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m: hit.score}\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hit \u001b[38;5;129;01min\u001b[39;00m results_sparse\n\u001b[32m      8\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m sparse_recalls.append(\u001b[43mrecall_at_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_formatted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrelevant_doc_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     10\u001b[39m sparse_mrrs.append(mrr(results_formatted, item[\u001b[33m\"\u001b[39m\u001b[33mrelevant_doc_ids\u001b[39m\u001b[33m\"\u001b[39m]))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mrecall_at_k\u001b[39m\u001b[34m(results, relevant_ids, k)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecall_at_k\u001b[39m(results: List[Dict], relevant_ids: List[\u001b[38;5;28mstr\u001b[39m], k: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Binary recall: 1 if any relevant doc in top-k, else 0\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     retrieved = [\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpayload\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdoc_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results[:k]]\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28many\u001b[39m(rid \u001b[38;5;129;01min\u001b[39;00m retrieved \u001b[38;5;28;01mfor\u001b[39;00m rid \u001b[38;5;129;01min\u001b[39;00m relevant_ids))\n",
      "\u001b[31mKeyError\u001b[39m: 'doc_id'"
     ]
    }
   ],
   "source": [
    "sparse_recalls = []\n",
    "sparse_mrrs = []\n",
    "for item in ground_truth:\n",
    "    results_sparse = sparse_retrieve(item[\"query\"], k=5)\n",
    "    results_formatted = [\n",
    "        {\"payload\": hit.payload, \"score\": hit.score}\n",
    "        for hit in results_sparse\n",
    "    ]\n",
    "    sparse_recalls.append(recall_at_k(results_formatted, item[\"relevant_doc_ids\"], k=5))\n",
    "    sparse_mrrs.append(mrr(results_formatted, item[\"relevant_doc_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9110cf4f-4194-4b1d-b44a-d740edbbf29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparse Only:\n",
      "  Recall@5: nan\n",
      "  MRR:      nan\n",
      "\n",
      "Hybrid (RRF):\n",
      "  Recall@5: 0.6000\n",
      "  MRR:      0.3667\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSparse Only:\")\n",
    "print(f\"  Recall@5: {np.mean(sparse_recalls):.4f}\")\n",
    "print(f\"  MRR:      {np.mean(sparse_mrrs):.4f}\")\n",
    "\n",
    "print(f\"\\nHybrid (RRF):\")\n",
    "print(f\"  Recall@5: {np.mean(recalls_5):.4f}\")\n",
    "print(f\"  MRR:      {np.mean(mrrs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58147bf0-e48e-43d6-9956-a92488e95508",
   "metadata": {},
   "source": [
    "# Some checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "590cf27f-831d-43e8-b49a-39d4a4f38961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hybrid_collection: 99 documents\n",
      "google-io-transcripts: 99 documents\n",
      "sparse_collection: 99 documents\n"
     ]
    }
   ],
   "source": [
    "# Compare collection sizes\n",
    "collections = q_client.get_collections()\n",
    "for coll in collections.collections:\n",
    "    info = q_client.get_collection(coll.name)\n",
    "    print(f\"{coll.name}: {info.points_count} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d9d07be1-52a7-42b3-80ba-38467f434c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FAIR COMPARISON (same collection, same data)\n",
      "============================================================\n",
      "\n",
      "Dense Only (hybrid_collection):\n",
      "  Recall@5: 0.8000\n",
      "  MRR:      0.6393\n",
      "\n",
      "Hybrid (sparse prefetch → dense rerank):\n",
      "  Recall@5: 0.6000\n",
      "  MRR:      0.3667\n",
      "\n",
      "Difference:\n",
      "  Recall@5: +0.2000\n",
      "  MRR:      +0.2726\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Dense-only retrieval using the hybrid_collection\n",
    "def dense_only_retrieve(query: str, k: int = 10):\n",
    "    \"\"\"Retrieve using ONLY dense vectors from hybrid_collection\"\"\"\n",
    "    dense_vector = embed_query(query)\n",
    "    \n",
    "    response = q_client.query_points(\n",
    "        collection_name=\"hybrid_collection\",  # Same collection as hybrid!\n",
    "        query=dense_vector,\n",
    "        using=\"dense\",\n",
    "        limit=k,\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for point in response.points:\n",
    "        results.append({\n",
    "            \"id\": point.id,\n",
    "            \"score\": point.score,\n",
    "            \"payload\": point.payload\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate dense-only on hybrid_collection\n",
    "dense_recalls = []\n",
    "dense_mrrs = []\n",
    "\n",
    "for item in ground_truth:\n",
    "    results = dense_only_retrieve(item[\"query\"], k=10)\n",
    "    \n",
    "    if not results:\n",
    "        continue\n",
    "    \n",
    "    retrieved_5 = [r[\"payload\"][\"doc_id\"] for r in results[:5]]\n",
    "    \n",
    "    dense_recalls.append(int(any(rid in retrieved_5 for rid in item[\"relevant_doc_ids\"])))\n",
    "    \n",
    "    mrr_score = 0.0\n",
    "    for i, r in enumerate(results, start=1):\n",
    "        if r[\"payload\"][\"doc_id\"] in item[\"relevant_doc_ids\"]:\n",
    "            mrr_score = 1.0 / i\n",
    "            break\n",
    "    dense_mrrs.append(mrr_score)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FAIR COMPARISON (same collection, same data)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDense Only (hybrid_collection):\")\n",
    "print(f\"  Recall@5: {np.mean(dense_recalls):.4f}\")\n",
    "print(f\"  MRR:      {np.mean(dense_mrrs):.4f}\")\n",
    "\n",
    "print(f\"\\nHybrid (sparse prefetch → dense rerank):\")\n",
    "print(f\"  Recall@5: 0.6000\")\n",
    "print(f\"  MRR:      0.3667\")\n",
    "\n",
    "print(f\"\\nDifference:\")\n",
    "print(f\"  Recall@5: {np.mean(dense_recalls) - 0.6000:+.4f}\")\n",
    "print(f\"  MRR:      {np.mean(dense_mrrs) - 0.3667:+.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3cde44-7914-4606-9e77-e56dae776973",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "\n",
    "Excellent! Now we have a clear answer: The hybrid search is hurting performance on this dataset. Dense-only is significantly better (80% vs 60% recall).\n",
    "This is happening because the sparse prefetch is filtering out relevant documents before the dense reranking can find them.\n",
    "Let's diagnose and fix this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fee1c4f2-042b-417b-b6fe-59413160078a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What were the main topics and components discussed in Google's AI Stack for Developers session at Go...\n",
      "\n",
      "Relevant doc IDs: ['4TE-KFXvhAk__chunk_000']\n",
      "\n",
      "=== DENSE-ONLY TOP 10 ===\n",
      "1. ✓ 4TE-KFXvhAk__chunk_000 (score: 0.6397)\n",
      "2. ✗ 4TE-KFXvhAk__chunk_017 (score: 0.5994)\n",
      "3. ✗ 4TE-KFXvhAk__chunk_019 (score: 0.5812)\n",
      "4. ✗ 4TE-KFXvhAk__chunk_001 (score: 0.5470)\n",
      "5. ✗ gHHjDRDNUNU__chunk_007 (score: 0.5117)\n",
      "6. ✗ 4TE-KFXvhAk__chunk_006 (score: 0.4782)\n",
      "7. ✗ 4TE-KFXvhAk__chunk_003 (score: 0.4651)\n",
      "8. ✗ Uh-7YX8tkxI__chunk_000 (score: 0.4489)\n",
      "9. ✗ 4TE-KFXvhAk__chunk_007 (score: 0.4397)\n",
      "10. ✗ o7Bv4r08FBM__chunk_000 (score: 0.4363)\n",
      "\n",
      "=== SPARSE TOP 50 ===\n",
      "❌ NO RELEVANT DOCS IN SPARSE TOP-50!\n",
      "\n",
      "=== HYBRID TOP 10 ===\n",
      "1. ✗ 4TE-KFXvhAk__chunk_017 (score: 0.5994)\n",
      "2. ✗ 4TE-KFXvhAk__chunk_006 (score: 0.4782)\n",
      "3. ✗ 4TE-KFXvhAk__chunk_003 (score: 0.4651)\n",
      "4. ✗ Uh-7YX8tkxI__chunk_000 (score: 0.4489)\n",
      "5. ✗ 4TE-KFXvhAk__chunk_007 (score: 0.4397)\n",
      "6. ✗ gHHjDRDNUNU__chunk_017 (score: 0.4155)\n",
      "7. ✗ gHHjDRDNUNU__chunk_006 (score: 0.3974)\n",
      "8. ✗ Uh-7YX8tkxI__chunk_010 (score: 0.3955)\n",
      "9. ✗ 4TE-KFXvhAk__chunk_018 (score: 0.3917)\n",
      "10. ✗ gHHjDRDNUNU__chunk_003 (score: 0.3784)\n"
     ]
    }
   ],
   "source": [
    "# Analyze where hybrid fails vs dense succeeds\n",
    "query = ground_truth[0][\"query\"]\n",
    "relevant_ids = ground_truth[0][\"relevant_doc_ids\"]\n",
    "\n",
    "print(f\"Query: {query[:100]}...\\n\")\n",
    "print(f\"Relevant doc IDs: {relevant_ids}\\n\")\n",
    "\n",
    "# Dense-only results\n",
    "dense_results = dense_only_retrieve(query, k=10)\n",
    "print(\"=== DENSE-ONLY TOP 10 ===\")\n",
    "for i, r in enumerate(dense_results[:10], 1):\n",
    "    is_relevant = \"✓\" if r[\"payload\"][\"doc_id\"] in relevant_ids else \"✗\"\n",
    "    print(f\"{i}. {is_relevant} {r['payload']['doc_id']} (score: {r['score']:.4f})\")\n",
    "\n",
    "# Sparse-only results from hybrid_collection\n",
    "sparse_vector = bm25_query(query)\n",
    "sparse_results = q_client.query_points(\n",
    "    collection_name=\"hybrid_collection\",  # Use hybrid_collection\n",
    "    query=sparse_vector,\n",
    "    using=\"text\",\n",
    "    limit=50,\n",
    ")\n",
    "print(\"\\n=== SPARSE TOP 50 ===\")\n",
    "relevant_in_sparse = []\n",
    "for i, hit in enumerate(sparse_results.points[:50], 1):\n",
    "    is_relevant = \"✓\" if hit.payload[\"doc_id\"] in relevant_ids else \"✗\"\n",
    "    if is_relevant == \"✓\":\n",
    "        relevant_in_sparse.append(i)\n",
    "        print(f\"{i}. {is_relevant} {hit.payload['doc_id']} (score: {hit.score:.4f})\")\n",
    "\n",
    "if not relevant_in_sparse:\n",
    "    print(\"❌ NO RELEVANT DOCS IN SPARSE TOP-50!\")\n",
    "else:\n",
    "    print(f\"\\n✓ Relevant docs found at positions: {relevant_in_sparse}\")\n",
    "\n",
    "# Hybrid results\n",
    "hybrid_results = hybrid_retrieve(query, k=10)\n",
    "print(\"\\n=== HYBRID TOP 10 ===\")\n",
    "for i, r in enumerate(hybrid_results[:10], 1):\n",
    "    is_relevant = \"✓\" if r[\"payload\"][\"doc_id\"] in relevant_ids else \"✗\"\n",
    "    print(f\"{i}. {is_relevant} {r['payload']['doc_id']} (score: {r['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b0a9c-a9f3-4822-8fab-8cf249c19a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
